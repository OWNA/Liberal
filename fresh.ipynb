{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OWNA/Liberal/blob/main/fresh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup Environment (Revised Install Attempt)\n",
        "\n",
        "# --- Step 1: Install NumPy pinned to <2.0 FIRST ---\n",
        "print(\"Installing NumPy <2.0 (target 1.26.4)...\")\n",
        "!pip install \"numpy==1.26.4\" --quiet # Force specific 1.x version\n",
        "\n",
        "# --- Step 2: Install specific compatible versions of Pandas and SciPy ---\n",
        "print(\"\\nInstalling specific compatible versions of Pandas and SciPy...\")\n",
        "!pip install \"pandas==2.0.3\" --quiet # Known to work well with NumPy 1.26.x\n",
        "!pip install \"scipy==1.11.4\" --quiet  # Known to work well with NumPy 1.26.x & Pandas 2.0.x\n",
        "\n",
        "# --- Step 3: Install LightGBM and scikit-learn ---\n",
        "print(\"\\nInstalling LightGBM and scikit-learn...\")\n",
        "!pip install lightgbm --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "\n",
        "# --- Step 4: Install the rest of the libraries (EMD-signal pinned) ---\n",
        "print(\"\\nInstalling remaining libraries (EMD-signal pinned)...\")\n",
        "!pip install ccxt optuna shap \"EMD-signal<1.4.0\" matplotlib pyyaml websocket-client dill==0.3.7 pandas_ta --quiet\n",
        "\n",
        "# --- Step 5: Mount Google Drive ---\n",
        "print(\"\\nMounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Step 7: Define Base Directory for the Project ---\n",
        "BOT_BASE_DIR = '/content/drive/MyDrive/trading_bot_project_v2/' # !!! YOUR PATH HERE !!!\n",
        "import os\n",
        "os.makedirs(BOT_BASE_DIR, exist_ok=True)\n",
        "os.environ['BOT_BASE_DIR'] = BOT_BASE_DIR\n",
        "print(f\"BOT_BASE_DIR set to: {BOT_BASE_DIR}\")\n",
        "import sys\n",
        "if BOT_BASE_DIR not in sys.path:\n",
        "    sys.path.append(BOT_BASE_DIR)\n",
        "    print(f\"Added {BOT_BASE_DIR} to sys.path for module imports.\")\n",
        "\n",
        "# --- Step 8: Check Python and Library Versions ---\n",
        "print(f\"\\n--- Library Versions ---\")\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "try:\n",
        "    import importlib.metadata\n",
        "\n",
        "    libs_to_check = {\n",
        "        \"ccxt\": \"CCXT\", \"lightgbm\": \"LightGBM\", \"pandas\": \"Pandas\",\n",
        "        \"numpy\": \"NumPy\", \"optuna\": \"Optuna\", \"shap\": \"SHAP\",\n",
        "        \"EMD-signal\": \"EMD-signal\", \"matplotlib\": \"Matplotlib\", \"scipy\": \"SciPy\",\n",
        "        \"PyYAML\": \"PyYAML\", \"websocket-client\": \"websocket-client\", \"dill\": \"Dill\",\n",
        "        \"scikit-learn\": \"scikit-learn\", \"pandas_ta\": \"pandas_ta\"\n",
        "    }\n",
        "    for lib_pkg_name, display_name in libs_to_check.items():\n",
        "        try:\n",
        "            version = importlib.metadata.version(lib_pkg_name)\n",
        "            print(f\"{display_name}: {version}\")\n",
        "        except importlib.metadata.PackageNotFoundError:\n",
        "            print(f\"{display_name}: Not installed or version not found.\")\n",
        "        except Exception as e_ver:\n",
        "            print(f\"Error getting version for {display_name}: {e_ver}\")\n",
        "except ImportError:\n",
        "    print(\"Could not import 'importlib.metadata'. Manual version checks might be needed.\")\n",
        "except Exception as e_outer:\n",
        "    print(f\"An error occurred during library version checking: {e_outer}\")\n",
        "print(\"------------------------\")\n",
        "\n",
        "# --- Step 9: Verify CCXT installation ---\n",
        "print(\"\\n--- Verifying CCXT Installation ---\")\n",
        "try:\n",
        "    import ccxt\n",
        "    print(f\"CCXT Version: {ccxt.__version__}\")\n",
        "    print(\"CCXT imported successfully.\")\n",
        "except ImportError:\n",
        "    print(\"ERROR: CCXT could not be imported. Please check installation.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during CCXT verification: {e}\")\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"\\nEnvironment setup complete (Revised Install Attempt).\")\n",
        "print(\"IMPORTANT: Ensure all your custom .py files are in the BOT_BASE_DIR and contain plain Python code.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQFl0DYZsnvB",
        "outputId": "f015ed05-bcdc-4e0b-e18e-9c221fabe608"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing NumPy <2.0 (target 1.26.4)...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "geemap 0.35.3 requires pandas, which is not installed.\n",
            "jax 0.5.2 requires scipy>=1.11.1, which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "bqplot 0.12.44 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "cuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "datasets 2.14.4 requires pandas, which is not installed.\n",
            "yfinance 0.2.61 requires pandas>=1.3.0, which is not installed.\n",
            "tsfresh 0.21.0 requires pandas>=0.25.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "clarabel 0.10.0 requires scipy, which is not installed.\n",
            "libpysal 4.13.0 requires pandas>=1.4, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "scs 3.2.7.post2 requires scipy, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires pandas, which is not installed.\n",
            "cmdstanpy 1.2.5 requires pandas, which is not installed.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, which is not installed.\n",
            "pymc 5.22.0 requires pandas>=0.24.0, which is not installed.\n",
            "pymc 5.22.0 requires scipy>=1.4.1, which is not installed.\n",
            "fastai 2.7.19 requires pandas, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.19 requires scipy, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "bokeh 3.7.3 requires pandas>=1.2, which is not installed.\n",
            "arviz 0.21.0 requires pandas>=1.5.0, which is not installed.\n",
            "arviz 0.21.0 requires scipy>=1.9.0, which is not installed.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "db-dtypes 1.4.3 requires pandas>=1.5.3, which is not installed.\n",
            "osqp 1.0.4 requires scipy>=0.13.2, which is not installed.\n",
            "xarray-einstats 0.8.0 requires scipy>=1.9, which is not installed.\n",
            "bigquery-magics 0.9.0 requires pandas>=1.1.0, which is not installed.\n",
            "albumentations 2.0.6 requires scipy>=1.10.0, which is not installed.\n",
            "dask-cuda 25.2.0 requires pandas>=1.3, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "holoviews 1.20.2 requires pandas>=1.3, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "bigframes 2.4.0 requires pandas>=1.5.3, which is not installed.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, which is not installed.\n",
            "xarray 2025.3.1 requires pandas>=2.1, which is not installed.\n",
            "datascience 0.17.6 requires pandas, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "panel 1.6.3 requires pandas>=1.2, which is not installed.\n",
            "geopandas 1.0.1 requires pandas>=1.4.0, which is not installed.\n",
            "pandas-gbq 0.28.1 requires pandas>=1.1.4, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "umap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "xgboost 2.1.4 requires scipy, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "pytensor 2.30.3 requires scipy<2,>=1, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "prophet 1.1.6 requires pandas>=1.0.4, which is not installed.\n",
            "stumpy 1.13.0 requires scipy>=1.10, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Installing specific compatible versions of Pandas and SciPy...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "pymc 5.22.0 requires scipy>=1.4.1, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.19 requires scipy, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "arviz 0.21.0 requires scipy>=1.9.0, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "sentence-transformers 4.1.0 requires scikit-learn, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Installing LightGBM and scikit-learn...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Installing remaining libraries (EMD-signal pinned)...\n",
            "\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "BOT_BASE_DIR set to: /content/drive/MyDrive/trading_bot_project_v2/\n",
            "Added /content/drive/MyDrive/trading_bot_project_v2/ to sys.path for module imports.\n",
            "\n",
            "--- Library Versions ---\n",
            "Python: 3.11.12\n",
            "CCXT: 4.4.85\n",
            "LightGBM: 4.6.0\n",
            "Pandas: 2.0.3\n",
            "NumPy: 1.26.4\n",
            "Optuna: 4.3.0\n",
            "SHAP: 0.47.2\n",
            "EMD-signal: 1.3.0\n",
            "Matplotlib: 3.10.0\n",
            "SciPy: 1.11.4\n",
            "PyYAML: 6.0.2\n",
            "websocket-client: 1.8.0\n",
            "Dill: 0.3.7\n",
            "scikit-learn: 1.6.1\n",
            "pandas_ta: 0.3.14b0\n",
            "------------------------\n",
            "\n",
            "--- Verifying CCXT Installation ---\n",
            "CCXT Version: 4.4.85\n",
            "CCXT imported successfully.\n",
            "------------------------------------\")\n",
            "\n",
            "print(\"\\nEnvironment setup complete (Revised Install Attempt).\")\n",
            "print(\"IMPORTANT: Ensure all your custom .py files are in the BOT_BASE_DIR and contain plain Python code.\")"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Configuration Settings\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "import numpy as np # For np.nan if you need to represent null for some specific logic\n",
        "\n",
        "# --- Ensure BOT_BASE_DIR is available (set in Cell 1) ---\n",
        "BOT_BASE_DIR = os.environ.get('BOT_BASE_DIR')\n",
        "if BOT_BASE_DIR is None:\n",
        "    print(\"CRITICAL ERROR (Cell 2): BOT_BASE_DIR is not set. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Run Cell 1 to define it.\")\n",
        "\n",
        "# --- Define Configuration Dictionary (Based on User Input) ---\n",
        "config = {\n",
        "    # -- Exchange & Symbol --\n",
        "    'exchange_name': 'bybit',  # Default, will be overridden if in user YAML\n",
        "    'exchange_testnet': True,\n",
        "    'symbol': 'BTC/USDT:USDT', # User provided\n",
        "    'market_type': 'linear',   # Default, assuming linear from symbol format\n",
        "    'timeframe': '1m', # User provided\n",
        "\n",
        "    # -- Data Fetching & Paths --\n",
        "    'base_dir': BOT_BASE_DIR,  # Uses the BOT_BASE_DIR from environment\n",
        "    'fetch_ohlcv_limit': 2500, # User provided\n",
        "    'load_existing_ohlcv': True,\n",
        "    'l2_data_folder': 'l2_data',\n",
        "    'l2_log_file': 'l2_data_collector.log',\n",
        "    'fetch_ohlcv_limit_for_scaling': 750,\n",
        "    'fetch_ohlcv_limit_wfo': 5000, # User provided\n",
        "\n",
        "    # -- L2 Data Collector Specific --\n",
        "    'collector_symbol': 'BTCUSDT', # User provided\n",
        "    'collector_duration': 1,       # User provided\n",
        "    'collector_unit': \"minutes\",   # User provided\n",
        "    'collector_depth': 50,         # User provided (for WebSocket subscription)\n",
        "    'collector_category': \"linear\",# User provided\n",
        "    'l2_max_file_size_mb': 20,\n",
        "    'l2_collection_duration_seconds': 60, # Derived from 1 minute for consistency\n",
        "\n",
        "    # -- Feature Engineering --\n",
        "    'feature_window': 24,      # User provided\n",
        "    'ohlcv_base_features': [\"z_close\", \"z_volume\", \"z_spread\"],\n",
        "    'ta_features': ['rsi', 'macd', 'bbands', 'atr', 'kama', 'supertrend', 'vwap'], # Added from user's ta_indicator_params\n",
        "\n",
        "    'ta_indicator_params': { # User provided\n",
        "        'rsi': {'length': 10, 'scalar': 100},\n",
        "        'macd': {'fast': 8, 'slow': 21, 'signal': 5},\n",
        "        'bbands': {'length': 20, 'std': 2.5},\n",
        "        'atr': {'length': 14}, # Default, can be overridden here\n",
        "        'kama': {'length': 10, 'fast': 2, 'slow': 30},\n",
        "        'supertrend': {'length': 7, 'multiplier': 3, 'atr_period': 10},\n",
        "        'vwap': {},\n",
        "    },\n",
        "\n",
        "    'use_hht_features': True,\n",
        "    'hht_features_imf_bases': ['hht_freq_imf', 'hht_amp_imf'],\n",
        "    'hht_imf_count': 3,\n",
        "    'hht_emd_noise_width': 0.05,\n",
        "\n",
        "    'use_l2_features': True,   # User provided\n",
        "    'use_l2_features_for_training': True,\n",
        "    'l2_depth_imbalance_levels': [3, 5, 10, 15, 25], # User provided\n",
        "    'l2_features': [\n",
        "        'price_impact_10',\n",
        "        'bid_curve', 'ask_curve'\n",
        "        # 'depth_imb_X' features are generated based on l2_depth_imbalance_levels\n",
        "    ],\n",
        "    'l2_price_impact_depth_idx': 4,\n",
        "    'l2_curve_fit_levels': 20,\n",
        "    'l2_depth': 25, # User provided (likely for REST L2 snapshot depth in DataHandler)\n",
        "\n",
        "    # -- Label Generation --\n",
        "    'labeling_method': 'triple_barrier', # User provided\n",
        "\n",
        "    'label_volatility_window': 20, # User provided (for vol_norm_return)\n",
        "    'label_clip_quantiles': [0.01, 0.99], # User provided (for vol_norm_return)\n",
        "    'label_shift': -1,         # User provided (for vol_norm_return)\n",
        "\n",
        "    'triple_barrier_profit_target_atr_mult': 0.8, # User provided\n",
        "    'triple_barrier_stop_loss_atr_mult': 0.5,   # User provided\n",
        "    'triple_barrier_time_horizon_bars': 20,     # User provided\n",
        "    'triple_barrier_atr_column': 'atr',         # User provided\n",
        "\n",
        "    # -- Model Training --\n",
        "    'random_state': 42,\n",
        "    'test_size': 0.2,          # User provided\n",
        "    'min_training_samples': 100,\n",
        "    'train_ensemble': False,\n",
        "    'lgbm_n_jobs': -1,\n",
        "\n",
        "    'optuna_trials': 100,       # User provided\n",
        "    'optuna_n_estimators_max': 1500,\n",
        "    'optuna_early_stopping_rounds': 25,\n",
        "    'optuna_study_name': f\"lgbm_opt_BTC-USDT_1m\", # Adjusted based on user's symbol/timeframe\n",
        "    'optuna_load_if_exists': True,\n",
        "    'optuna_n_jobs': 1,\n",
        "    'optuna_timeout_seconds': None,\n",
        "\n",
        "    'optuna_search_spaces': { # User provided\n",
        "        'n_estimators': {'type': 'int', 'low': 50, 'high': 800, 'step': 25},\n",
        "        'learning_rate': {'type': 'float', 'low': 0.005, 'high': 0.1, 'log': True},\n",
        "        'num_leaves': {'type': 'int', 'low': 15, 'high': 100},\n",
        "        'max_depth': {'type': 'int', 'low': 2, 'high': 8},\n",
        "        'lambda_l1': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'lambda_l2': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'feature_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_freq': {'type': 'int', 'low': 1, 'high': 10},\n",
        "        'min_child_samples': {'type': 'int', 'low': 3, 'high': 40}\n",
        "    },\n",
        "\n",
        "    'ensemble_long_thresh': 0.5,\n",
        "    'ensemble_short_thresh': -0.5,\n",
        "    'ensemble_clf_params': {\n",
        "        'objective': 'multiclass', 'metric': 'multi_logloss',\n",
        "        'num_class': 3, 'n_estimators': 200,\n",
        "    },\n",
        "    'ensemble_reg_params': {\n",
        "        'objective': 'regression_l1', 'metric': 'mae',\n",
        "        'n_estimators': 200,\n",
        "    },\n",
        "\n",
        "    'enable_feature_selection': False, # User provided\n",
        "    'feature_selection_method': 'shap', # User provided\n",
        "    'num_features_to_select': 30,       # User provided\n",
        "\n",
        "    # -- Model Prediction & Trading Logic --\n",
        "    # 'backtest_threshold' from user's list is now 'prediction_threshold' for clarity\n",
        "    'prediction_threshold': 0.2, # User provided as backtest_threshold\n",
        "    'use_ensemble_for_backtest': False,\n",
        "    'use_ensemble_for_simulation': False,\n",
        "    'use_ensemble_for_visualization': False,\n",
        "\n",
        "    # -- Risk Management --\n",
        "    'risk_management': {\n",
        "        'max_drawdown': 0.20,\n",
        "        'volatility_lookback': 14,\n",
        "        'position_sizing_mode': 'volatility_target',\n",
        "        'volatility_target_pct': 0.02,\n",
        "        'max_equity_risk_pct': 0.05,\n",
        "        'fixed_fraction_pct': 0.02,\n",
        "        # User provided stop_loss_pct: null, take_profit_pct: null.\n",
        "        # These are not directly used by current ATR-based AdvancedRiskManager.\n",
        "        # Keeping them if user has other plans, or they can be removed if only ATR-based is used.\n",
        "        'stop_loss_pct': None, # User provided\n",
        "        'take_profit_pct': None, # User provided\n",
        "        'sl_atr_multiplier': 1.5,\n",
        "        'tp_atr_multiplier': 2.5\n",
        "    },\n",
        "    'fallback_volatility_pct_for_sizing': 0.02,\n",
        "\n",
        "    # -- Order Execution --\n",
        "    'execution': {\n",
        "        'slippage_model_pct': 0.0005,\n",
        "        'max_order_book_levels': 20,\n",
        "        'default_entry_order_type': 'market', # User provided as simulation_entry_order_type\n",
        "        'default_exit_order_type': 'limit',  # User provided as simulation_exit_order_type\n",
        "    },\n",
        "\n",
        "    # -- Backtesting --\n",
        "    'initial_balance': 10000, # User provided\n",
        "    'commission_pct': 0.0006,  # User provided\n",
        "    'leverage': 3,             # User provided\n",
        "    'fallback_atr_pct_for_backtest': 0.02,\n",
        "\n",
        "    # -- Walk-Forward Optimization --\n",
        "    'run_walk_forward_optimization': False, # Default to False, can be overridden\n",
        "    'walk_forward_train_periods': 730,      # User provided\n",
        "    'walk_forward_test_periods': 180,       # User provided\n",
        "    'walk_forward_step_periods': 180,       # User provided\n",
        "    'walk_forward_initial_warmup': 100,     # User provided\n",
        "    'walk_forward_retrain_frequency_folds': 1, # User provided\n",
        "    # fetch_ohlcv_limit_wfo is already under Data Fetching\n",
        "\n",
        "    # -- Live Simulation --\n",
        "    'run_simulation_flag': True, # User provided\n",
        "    'simulation_threshold': 0.2, # User provided\n",
        "    'fetch_live_limit': 300,   # User provided\n",
        "    'min_simulation_interval_seconds': 15,\n",
        "    'simulation_duration_seconds': 300, # User provided\n",
        "\n",
        "    # -- Visualization --\n",
        "    'show_plots': True,\n",
        "    'plot_style': 'seaborn-v0_8-darkgrid',\n",
        "    'use_shap_for_importance': True, # User provided as use_shap_override\n",
        "    'shap_max_samples': 1000,\n",
        "    'plot_figsize_equity': (14, 7),\n",
        "    'plot_figsize_lgbm': (12, 10),\n",
        "    'plot_figsize_shap_bar': (12, 10),\n",
        "    'plot_figsize_shap_dot': (12, 10),\n",
        "    'plot_figsize_emd': (14, 12),\n",
        "    'plot_figsize_features': (14, 10),\n",
        "\n",
        "    # -- Orchestrator --\n",
        "    'allow_no_exchange_init': False\n",
        "}\n",
        "\n",
        "# --- Overwrite specific keys from user's list if they differ from my structured interpretation ---\n",
        "# This step ensures the user's exact values (from their snippet) are prioritized\n",
        "# for the keys they explicitly provided.\n",
        "user_provided_config_snippet = {\n",
        "    'base_dir': BOT_BASE_DIR, # This must come from the environment\n",
        "    'symbol': 'BTC/USDT',\n",
        "    'timeframe': '1m',\n",
        "    'feature_window': 24,\n",
        "    'use_l2_features': True,\n",
        "    # 'l2_depth_levels': 25, # Renamed to l2_depth_imbalance_levels, and l2_depth is separate\n",
        "    'l2_depth': 25, # For REST L2 snapshots if DataHandler uses it directly\n",
        "    'fetch_ohlcv_limit': 2500,\n",
        "    'optuna_trials': 100,\n",
        "    'test_size': 0.2,\n",
        "    'backtest_threshold': 0.2, # This is now 'prediction_threshold'\n",
        "    'initial_balance': 10000,\n",
        "    'commission_pct': 0.0006,\n",
        "    'leverage': 3,\n",
        "    'stop_loss_pct': None, # Kept as user provided\n",
        "    'take_profit_pct': None, # Kept as user provided\n",
        "    'run_simulation_flag': True,\n",
        "    'simulation_threshold': 0.2,\n",
        "    'fetch_live_limit': 300,\n",
        "    'simulation_duration_seconds': 300,\n",
        "    'use_shap_override': True, # This is now 'use_shap_for_importance'\n",
        "    'collector_symbol': 'BTCUSDT',\n",
        "    'collector_duration': 1,\n",
        "    'collector_unit': 'minutes',\n",
        "    'collector_depth': 50,\n",
        "    'collector_category': 'linear',\n",
        "    'labeling_method': 'triple_barrier',\n",
        "    'label_volatility_window': 20,\n",
        "    'label_clip_quantiles': [0.01, 0.99],\n",
        "    'label_shift': -1,\n",
        "    'triple_barrier_profit_target_atr_mult': 0.8,\n",
        "    'triple_barrier_stop_loss_atr_mult': 0.5,\n",
        "    'triple_barrier_time_horizon_bars': 20,\n",
        "    'triple_barrier_atr_column': 'atr',\n",
        "    'walk_forward_train_periods': 730,\n",
        "    'walk_forward_test_periods': 180,\n",
        "    'walk_forward_step_periods': 180,\n",
        "    'walk_forward_initial_warmup': 100,\n",
        "    'walk_forward_retrain_frequency_folds': 1,\n",
        "    'fetch_ohlcv_limit_wfo': 5000,\n",
        "    'enable_feature_selection': False,\n",
        "    'feature_selection_method': 'shap',\n",
        "    'num_features_to_select': 30,\n",
        "    'optuna_search_spaces': {\n",
        "        'n_estimators': {'type': 'int', 'low': 50, 'high': 800, 'step': 25},\n",
        "        'learning_rate': {'type': 'float', 'low': 0.005, 'high': 0.1, 'log': True},\n",
        "        'num_leaves': {'type': 'int', 'low': 15, 'high': 100},\n",
        "        'max_depth': {'type': 'int', 'low': 2, 'high': 8},\n",
        "        'lambda_l1': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'lambda_l2': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'feature_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_freq': {'type': 'int', 'low': 1, 'high': 10},\n",
        "        'min_child_samples': {'type': 'int', 'low': 3, 'high': 40}\n",
        "    },\n",
        "    # Mapping user's 'simulation_entry/exit_order_type' to 'execution' dict\n",
        "    # 'simulation_entry_order_type': 'market', # Will be handled below\n",
        "    # 'simulation_exit_order_type': 'limit',   # Will be handled below\n",
        "    'l2_depth_imbalance_levels': [3, 5, 10, 15, 25],\n",
        "    'ta_indicator_params': {\n",
        "        'rsi': {'length': 10, 'scalar': 100},\n",
        "        'macd': {'fast': 8, 'slow': 21, 'signal': 5},\n",
        "        'bbands': {'length': 20, 'std': 2.5}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Update the main 'config' dictionary with the user's exact values for the keys they provided\n",
        "# and handle specific mappings.\n",
        "for key, value in user_provided_config_snippet.items():\n",
        "    if key == 'backtest_threshold':\n",
        "        config['prediction_threshold'] = value\n",
        "    elif key == 'use_shap_override':\n",
        "        config['use_shap_for_importance'] = value\n",
        "    # elif key == 'l2_depth_levels': # User had this, map to l2_depth_imbalance_levels for FeatureEngineer\n",
        "    #     config['l2_depth_imbalance_levels'] = value\n",
        "    #     # 'l2_depth' is a separate parameter for DataHandler's REST L2 snapshot\n",
        "    elif key == 'simulation_entry_order_type':\n",
        "        config['execution']['default_entry_order_type'] = value\n",
        "    elif key == 'simulation_exit_order_type':\n",
        "        config['execution']['default_exit_order_type'] = value\n",
        "    else:\n",
        "        config[key] = value\n",
        "\n",
        "# Ensure base_dir is always from the environment variable\n",
        "config['base_dir'] = BOT_BASE_DIR\n",
        "# Correctly derive l2_collection_duration_seconds from collector_duration and collector_unit\n",
        "if config.get('collector_unit') == 'minutes':\n",
        "    config['l2_collection_duration_seconds'] = config.get('collector_duration', 1) * 60\n",
        "elif config.get('collector_unit') == 'hours':\n",
        "    config['l2_collection_duration_seconds'] = config.get('collector_duration', 1) * 3600\n",
        "else: # Default to seconds if unit is unclear or missing\n",
        "    config['l2_collection_duration_seconds'] = config.get('collector_duration', 60)\n",
        "\n",
        "\n",
        "# --- Save Configuration to YAML File ---\n",
        "config_file_path = os.path.join(BOT_BASE_DIR, 'config.yaml')\n",
        "try:\n",
        "    with open(config_file_path, 'w') as f:\n",
        "        yaml.dump(config, f, sort_keys=False, indent=4, width=120, Dumper=yaml.SafeDumper) # Use SafeDumper\n",
        "    print(f\"\\nConfiguration (User Updated for Phase 1) saved to: {config_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError saving configuration: {e}\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# --- Display a snippet of the config for verification ---\n",
        "print(\"\\n--- Configuration Snippet (User Update for Phase 1) ---\")\n",
        "keys_to_display = [\n",
        "    'exchange_name', 'symbol', 'timeframe', 'base_dir',\n",
        "    'labeling_method', 'train_ensemble',\n",
        "    'run_walk_forward_optimization', 'run_simulation_flag',\n",
        "    'enable_feature_selection', 'prediction_threshold',\n",
        "    'l2_depth_imbalance_levels'\n",
        "]\n",
        "for key in keys_to_display:\n",
        "    if key in config:\n",
        "        print(f\"{key}: {config[key]}\")\n",
        "if 'optuna_search_spaces' in config and config['optuna_search_spaces']:\n",
        "    print(f\"optuna_search_spaces (first item key): {list(config['optuna_search_spaces'].keys())[0] if config['optuna_search_spaces'] else 'Not set'}\")\n",
        "if 'ta_indicator_params' in config and config['ta_indicator_params']:\n",
        "     print(f\"ta_indicator_params (first item key): {list(config['ta_indicator_params'].keys())[0] if config['ta_indicator_params'] else 'Not set'}\")\n",
        "print(\"-----------------------------------------\")\n",
        "\n"
      ],
      "metadata": {
        "id": "q3U7G-5TzRr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5973980-3c53-406c-f425-ced8ed01a4c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuration (User Updated for Phase 1) saved to: /content/drive/MyDrive/trading_bot_project_v2/config.yaml\n",
            "\n",
            "--- Configuration Snippet (User Update for Phase 1) ---\n",
            "exchange_name: bybit\n",
            "symbol: BTC/USDT\n",
            "timeframe: 1m\n",
            "base_dir: /content/drive/MyDrive/trading_bot_project_v2/\n",
            "labeling_method: triple_barrier\n",
            "train_ensemble: False\n",
            "run_walk_forward_optimization: False\n",
            "run_simulation_flag: True\n",
            "enable_feature_selection: False\n",
            "prediction_threshold: 0.2\n",
            "l2_depth_imbalance_levels: [3, 5, 10, 15, 25]\n",
            "optuna_search_spaces (first item key): n_estimators\n",
            "ta_indicator_params (first item key): rsi\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: L2 Data Collector (Refactored Usage)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import time # For a brief pause if needed\n",
        "from datetime import datetime, timezone # For logging\n",
        "import traceback # For error printing\n",
        "\n",
        "# --- Ensure BOT_BASE_DIR is available (set in Cell 1) ---\n",
        "BOT_BASE_DIR = os.environ.get('BOT_BASE_DIR')\n",
        "if BOT_BASE_DIR is None:\n",
        "    print(\"CRITICAL ERROR (Cell 3): BOT_BASE_DIR is not set. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Run Cell 1 to define it.\")\n",
        "\n",
        "# --- Add BOT_BASE_DIR to Python path to allow importing l2_data_collector ---\n",
        "# This should have been done in Cell 1 already, but good to ensure for standalone cell execution if possible\n",
        "if BOT_BASE_DIR not in sys.path:\n",
        "    sys.path.append(BOT_BASE_DIR)\n",
        "    print(f\"Info (Cell 3): Added {BOT_BASE_DIR} to sys.path for L2DataCollector import.\")\n",
        "\n",
        "# --- Import the refactored class ---\n",
        "L2DataCollector = None # Initialize to None\n",
        "try:\n",
        "    from l2_data_collector import L2DataCollector # Assumes l2_data_collector.py is in BOT_BASE_DIR\n",
        "    print(\"L2DataCollector class imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR (Cell 3): Could not import L2DataCollector: {e}\")\n",
        "    print(f\"Please ensure 'l2_data_collector.py' is in the directory: {BOT_BASE_DIR} and that Cell 1 was run.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during L2DataCollector import: {e}\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# --- Load Main Configuration to Extract Collector Settings ---\n",
        "# This assumes config.yaml is in BOT_BASE_DIR and was created by Cell 2.\n",
        "config_file_path_main = os.path.join(BOT_BASE_DIR, 'config.yaml')\n",
        "main_config_for_l2 = {} # Use a distinct variable name\n",
        "collector_specific_config = {}\n",
        "\n",
        "if os.path.exists(config_file_path_main):\n",
        "    try:\n",
        "        with open(config_file_path_main, 'r') as f:\n",
        "            main_config_for_l2 = yaml.safe_load(f)\n",
        "        if main_config_for_l2:\n",
        "            print(f\"Successfully loaded main configuration from {config_file_path_main} for L2 collector settings.\")\n",
        "\n",
        "            # Extract collector-specific settings from the main config.\n",
        "            # The L2DataCollector class itself has defaults, but we use main_config to override them.\n",
        "            # The L2DataCollector's __init__ expects a 'config' dict.\n",
        "            collector_specific_config = {\n",
        "                'symbol': main_config_for_l2.get('collector_symbol', 'BTCUSDT'), # L2Collector uses 'symbol' for its target\n",
        "                'market_type': main_config_for_l2.get('collector_category', main_config_for_l2.get('market_type', 'linear')),\n",
        "                'exchange_name': main_config_for_l2.get('exchange_name', 'bybit'),\n",
        "                'l2_data_folder': main_config_for_l2.get('l2_data_folder', 'l2_data'),\n",
        "                'l2_log_file': main_config_for_l2.get('l2_log_file', 'l2_data_collector.log'), # Log file name for L2 collector\n",
        "                'l2_max_file_size_mb': main_config_for_l2.get('l2_max_file_size_mb', 20),\n",
        "                'l2_collection_duration_seconds': main_config_for_l2.get('l2_collection_duration_seconds', 300),\n",
        "                'l2_websocket_depth': main_config_for_l2.get('collector_depth', 50)\n",
        "            }\n",
        "            print(\"L2 Collector parameters extracted from main config:\")\n",
        "            for k, v in collector_specific_config.items():\n",
        "                print(f\"  {k}: {v}\")\n",
        "        else:\n",
        "            print(f\"Warning (Cell 3): Main config.yaml at {config_file_path_main} was empty. L2 Collector might use class defaults.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning (Cell 3): Could not load or parse main config.yaml ({e}). L2 Collector might use class defaults or fail if critical params missing.\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\"Warning (Cell 3): Main config.yaml not found at {config_file_path_main}. L2 Collector will use its internal defaults if not provided in collector_specific_config.\")\n",
        "\n",
        "\n",
        "# --- Instantiate and Run the Collector ---\n",
        "# Check if the L2DataCollector class was successfully imported AND if we have some config for it\n",
        "if L2DataCollector is not None:\n",
        "    if not collector_specific_config and not main_config_for_l2: # If no config could be loaded at all\n",
        "        print(\"ERROR (Cell 3): No configuration available for L2DataCollector. Cannot proceed.\")\n",
        "    else:\n",
        "        print(\"\\n--- Initializing and Starting L2 Data Collector ---\")\n",
        "\n",
        "        # The L2DataCollector's __init__ expects 'config' (which are the collector_specific_config here)\n",
        "        # and 'bot_base_dir'.\n",
        "        l2_collector_instance = None\n",
        "        try:\n",
        "            l2_collector_instance = L2DataCollector(config=collector_specific_config, bot_base_dir=BOT_BASE_DIR)\n",
        "\n",
        "            # The start_collection_websocket method contains its own loop based on l2_collection_duration_seconds.\n",
        "            # This cell will effectively block until that duration is over or an interrupt occurs.\n",
        "            duration_to_run = collector_specific_config.get('l2_collection_duration_seconds', 300) # Get from extracted params\n",
        "            print(f\"L2 Collector is configured to run for approximately {duration_to_run / 60:.1f} minutes.\")\n",
        "            print(\"You can interrupt the kernel (Runtime -> Interrupt execution or Ctrl+M I) to stop it sooner.\")\n",
        "            print(f\"L2 data will be saved in: {os.path.join(BOT_BASE_DIR, collector_specific_config.get('l2_data_folder', 'l2_data'))}\")\n",
        "            print(f\"L2 collector log file: {os.path.join(BOT_BASE_DIR, collector_specific_config.get('l2_log_file', 'l2_data_collector.log'))}\")\n",
        "\n",
        "            l2_collector_instance.start_collection_websocket()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nL2 Data Collection interrupted by user in notebook cell.\")\n",
        "            if l2_collector_instance:\n",
        "                l2_collector_instance.stop_collection_websocket() # Ensure graceful shutdown\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while running the L2 Data Collector: {e}\")\n",
        "            traceback.print_exc()\n",
        "            if l2_collector_instance and getattr(l2_collector_instance, 'ws', None) is not None : # Check if ws object exists\n",
        "                l2_collector_instance.stop_collection_websocket()\n",
        "        finally:\n",
        "            print(\"--- L2 Data Collector Cell Execution Finished ---\")\n",
        "            # Note: If start_collection_websocket runs in a blocking way for its duration,\n",
        "            # this \"finished\" message will appear after the collection period.\n",
        "else:\n",
        "    print(\"\\nL2DataCollector class not available (import failed). Cannot start L2 data collection.\")\n",
        "    print(\"Ensure 'l2_data_collector.py' is in your BOT_BASE_DIR and Cell 1 has been run.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2DataCollector class imported successfully.\n",
            "Successfully loaded main configuration from /content/drive/MyDrive/trading_bot_project_v2/config.yaml for L2 collector settings.\n",
            "L2 Collector parameters extracted from main config:\n",
            "  symbol: BTCUSDT\n",
            "  market_type: linear\n",
            "  exchange_name: bybit\n",
            "  l2_data_folder: l2_data\n",
            "  l2_log_file: l2_data_collector.log\n",
            "  l2_max_file_size_mb: 20\n",
            "  l2_collection_duration_seconds: 60\n",
            "  l2_websocket_depth: 50\n",
            "\n",
            "--- Initializing and Starting L2 Data Collector ---\n",
            "[2025-05-24 00:37:42 UTC] (L2Collector) L2DataCollector initialized for symbol: BTCUSDT, exchange: bybit\n",
            "L2 Collector is configured to run for approximately 1.0 minutes.\n",
            "You can interrupt the kernel (Runtime -> Interrupt execution or Ctrl+M I) to stop it sooner.\n",
            "L2 data will be saved in: /content/drive/MyDrive/trading_bot_project_v2/l2_data\n",
            "L2 collector log file: /content/drive/MyDrive/trading_bot_project_v2/l2_data_collector.log\n",
            "[2025-05-24 00:37:42 UTC] (L2Collector) Using WSS URL for Bybit (linear): wss://stream.bybit.com/v5/public/linear\n",
            "[2025-05-24 00:37:42 UTC] (L2Collector) Starting WebSocket L2 data collection...\n",
            "[2025-05-24 00:37:42 UTC] (L2Collector) Opened new data file: /content/drive/MyDrive/trading_bot_project_v2/l2_data/l2_data_BTCUSDT_20250524_003742.jsonl.gz\n",
            "[2025-05-24 00:37:42 UTC] (L2Collector) WebSocket collection thread started. Target duration: 60 seconds.\n",
            "[2025-05-24 00:37:42 UTC] (L2Collector) WebSocket Connection Opened for BTCUSDT\n",
            "[2025-05-24 00:37:42 UTC] (L2Collector) Subscribed to orderbook.50.BTCUSDT\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "bPiZvwMtxx-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2247c0ca-3841-4e0b-a080-fe3c196ea5f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Imports & Global Setup (Main Bot)\n",
        "\n",
        "# --- Core Libraries ---\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "import warnings\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import sys\n",
        "import yaml # For loading config\n",
        "import pickle # For saving/loading ensemble models\n",
        "\n",
        "# --- Add BOT_BASE_DIR to Python path ---\n",
        "# This allows importing custom modules from this directory\n",
        "# Ensure BOT_BASE_DIR is set, typically in Cell 1.\n",
        "bot_base_dir = os.environ.get('BOT_BASE_DIR')\n",
        "if bot_base_dir is None:\n",
        "    print(\"CRITICAL ERROR (Cell 4): BOT_BASE_DIR is not set in environment. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Run Cell 1 to define it.\")\n",
        "\n",
        "if BOT_BASE_DIR not in sys.path: # Should have been added in Cell 1, but double-check\n",
        "    sys.path.append(BOT_BASE_DIR)\n",
        "    print(f\"Info (Cell 4): Added {BOT_BASE_DIR} to sys.path\")\n",
        "\n",
        "# --- Data Handling & Numerics ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Machine Learning ---\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# --- Load Configuration ---\n",
        "print(\"Loading configuration...\")\n",
        "config_file_path = os.path.join(bot_base_dir, 'config.yaml')\n",
        "config = {}\n",
        "try:\n",
        "    with open(config_file_path, 'r') as f:\n",
        "        main_config_loader = yaml.safe_load(f)\n",
        "        if main_config_loader:\n",
        "            config = main_config_loader\n",
        "    print(f\"Configuration loaded successfully from {config_file_path}\")\n",
        "    if not config:\n",
        "        print(\"Warning (Cell 4): Configuration file was empty. Using empty config dict.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR (Cell 4): Configuration file not found at {config_file_path}. Please run Cell 2 to create it.\")\n",
        "    print(\"Using empty config dict. Bot may not function correctly.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error (Cell 4) loading configuration: {e}\")\n",
        "    traceback.print_exc()\n",
        "    print(\"Using empty config dict. Bot may not function correctly.\")\n",
        "\n",
        "# --- Optional Libraries Setup ---\n",
        "print(\"\\nSetting up optional libraries and global flags...\")\n",
        "try:\n",
        "    import optuna\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "    HAS_OPTUNA = True\n",
        "    print(\"Optuna loaded.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): Optuna not found. Model training requiring it may fail.\")\n",
        "    HAS_OPTUNA = False\n",
        "    optuna = None\n",
        "\n",
        "try:\n",
        "    from PyEMD import EMD\n",
        "    HAS_PYEMD = True\n",
        "    print(\"PyEMD (EMD-signal) library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): PyEMD (EMD-signal) not found. HHT features will be disabled.\")\n",
        "    HAS_PYEMD = False\n",
        "    class EMD:\n",
        "        def __init__(self, *args, **kwargs): pass\n",
        "        def __call__(self, signal, *args, **kwargs): return np.array([])\n",
        "        def get_imfs_and_residue(self): return np.array([]), np.array([])\n",
        "    print(\"Using dummy EMD class.\")\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    HAS_SHAP = True\n",
        "    print(\"SHAP library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): SHAP library not found. SHAP plots will be disabled.\")\n",
        "    HAS_SHAP = False\n",
        "    shap = None\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    if config and 'plot_style' in config:\n",
        "        try:\n",
        "            plt.style.use(config.get('plot_style'))\n",
        "            print(f\"Applied plot style: {config.get('plot_style')}\")\n",
        "        except Exception as e_style:\n",
        "            print(f\"Warning (Cell 4): Could not apply plot style '{config.get('plot_style')}': {e_style}\")\n",
        "    HAS_MATPLOTLIB = True\n",
        "    print(\"Matplotlib loaded.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): Matplotlib not found. Plotting will be disabled.\")\n",
        "    HAS_MATPLOTLIB = False\n",
        "    plt = None\n",
        "\n",
        "try:\n",
        "    from scipy.signal import hilbert\n",
        "    HAS_SCIPY_HILBERT = True\n",
        "    print(\"Scipy (signal.hilbert) loaded.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): Scipy.signal.hilbert not found. HHT features requiring it may fail.\")\n",
        "    HAS_SCIPY_HILBERT = False\n",
        "    def hilbert(signal, N=None, axis=-1):\n",
        "        print(\"Error (Cell 4): hilbert function called but scipy.signal.hilbert not available.\")\n",
        "        return np.zeros_like(signal) + 1j * np.zeros_like(signal)\n",
        "    print(\"Using dummy hilbert function.\")\n",
        "\n",
        "try:\n",
        "    import pandas_ta as ta\n",
        "    HAS_PANDAS_TA = True\n",
        "    print(\"Pandas TA library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): pandas_ta library not found. Advanced TA features relying on it will be disabled.\")\n",
        "    HAS_PANDAS_TA = False\n",
        "    ta = None\n",
        "\n",
        "# --- Exchange Interaction ---\n",
        "try:\n",
        "    import ccxt\n",
        "    print(\"CCXT loaded.\")\n",
        "except ImportError:\n",
        "    print(\"ERROR (Cell 4): CCXT not found. Exchange interaction will fail.\")\n",
        "    ccxt = None\n",
        "\n",
        "# --- Custom Helper Class Imports ---\n",
        "print(\"\\nImporting custom bot classes...\")\n",
        "custom_classes_to_import = [\n",
        "    \"AdvancedRiskManager\", \"SmartOrderExecutor\", \"DataHandler\",\n",
        "    \"FeatureEngineer\", \"LabelGenerator\", \"ModelTrainer\",\n",
        "    \"ModelPredictor\", \"StrategyBacktester\", \"LiveSimulator\",\n",
        "    \"Visualizer\", \"TradingBotOrchestrator\"\n",
        "]\n",
        "\n",
        "for class_name_str in custom_classes_to_import:\n",
        "    try:\n",
        "        module_name = class_name_str.lower()\n",
        "        module = __import__(module_name)\n",
        "        globals()[class_name_str] = getattr(module, class_name_str)\n",
        "        print(f\"{class_name_str} class loaded successfully from {module_name}.py\")\n",
        "    except ImportError as e:\n",
        "        print(f\"ERROR (Cell 4) importing {class_name_str} from {class_name_str.lower()}.py: {e}\")\n",
        "        print(f\"Please ensure '{class_name_str.lower()}.py' is in '{bot_base_dir}' or Python path and contains plain Python code.\")\n",
        "        globals()[class_name_str] = None\n",
        "    except AttributeError as e:\n",
        "        print(f\"ERROR (Cell 4): Attribute {class_name_str} not found in module {class_name_str.lower()}.py. Check class name. Error: {e}\")\n",
        "        globals()[class_name_str] = None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while importing {class_name_str}: {e}\")\n",
        "        traceback.print_exc()\n",
        "        globals()[class_name_str] = None\n",
        "\n",
        "# --- API Key Loading (Using Colab Secrets or Environment Variables) ---\n",
        "print(\"\\nLoading API Keys...\")\n",
        "BYBIT_API_KEY = None\n",
        "BYBIT_API_SECRET = None\n",
        "\n",
        "# Try Colab userdata first\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    # --- MODIFIED TO USE TESTNET KEY NAMES ---\n",
        "    BYBIT_API_KEY = userdata.get(\"BYBIT_API_KEY_MAIN_TEST\")\n",
        "    BYBIT_API_SECRET = userdata.get(\"BYBIT_API_SECRET_MAIN_TEST\")\n",
        "\n",
        "    if BYBIT_API_KEY and BYBIT_API_SECRET:\n",
        "        print(\"TESTNET API Keys loaded successfully from Colab secrets.\")\n",
        "    else:\n",
        "        print(\"*** WARNING: TESTNET API Keys not found or empty in Colab secrets ('BYBIT_API_KEY_MAIN_TEST', 'BYBIT_API_SECRET_MAIN_TEST'). ***\")\n",
        "        BYBIT_API_KEY = None\n",
        "        BYBIT_API_SECRET = None\n",
        "except ImportError:\n",
        "    print(\"Not in Colab environment. Will check environment variables for API keys.\")\n",
        "    # If not found in Colab secrets, try environment variables\n",
        "    if not (BYBIT_API_KEY and BYBIT_API_SECRET): # Check again in case Colab import failed but env vars might exist\n",
        "        BYBIT_API_KEY = os.environ.get(\"BYBIT_API_KEY_MAIN_TEST\") # Check for TESTNET env vars\n",
        "        BYBIT_API_SECRET = os.environ.get(\"BYBIT_API_SECRET_MAIN_TEST\")\n",
        "        if BYBIT_API_KEY and BYBIT_API_SECRET:\n",
        "            print(\"TESTNET API Keys loaded successfully from environment variables.\")\n",
        "        else:\n",
        "            print(\"WARNING (Cell 4): TESTNET API Keys not found in Colab secrets or environment variables ('BYBIT_API_KEY_MAIN_TEST', 'BYBIT_API_SECRET_MAIN_TEST'). Live trading requiring authentication will fail if testnet keys are intended.\")\n",
        "except Exception as e: # Catch other potential errors during userdata.get()\n",
        "    print(f\"An unexpected error occurred loading secrets: {e}\")\n",
        "    traceback.print_exc()\n",
        "    BYBIT_API_KEY = None\n",
        "    BYBIT_API_SECRET = None\n",
        "\n",
        "# --- Warnings Configuration ---\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "print(\"\\nImports and global setup complete.\")\n",
        "if config:\n",
        "    print(f\"Config dictionary loaded with {len(config)} top-level keys.\")\n",
        "    print(f\"Config 'exchange_testnet' is set to: {config.get('exchange_testnet')}\")\n",
        "else:\n",
        "    print(\"CRITICAL WARNING (Cell 4): Config dictionary is empty or failed to load. Bot will likely not function correctly.\")\n"
      ],
      "metadata": {
        "id": "-tyw0dYhoiNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Cell 5  Trading Bot Orchestrator Initialization   (patched)\n",
        "# ================================================================\n",
        "import os, traceback, inspect\n",
        "\n",
        "# ---------- 0  Pre-flight checks --------------------------------\n",
        "if \"BOT_BASE_DIR\" not in os.environ:\n",
        "    raise EnvironmentError(\"Cell 5: BOT_BASE_DIR not set  run Cell 1 first.\")\n",
        "BOT_BASE_DIR = os.environ[\"BOT_BASE_DIR\"]\n",
        "print(f\"Info:  BOT_BASE_DIR  {BOT_BASE_DIR}\")\n",
        "\n",
        "if \"config\" not in globals() or not isinstance(config, dict) or not config:\n",
        "    raise ValueError(\"Cell 5: global 'config' missing or empty  did you run Cells 2 & 4?\")\n",
        "print(f\"Info:  base config has {len(config)} keys.\")\n",
        "\n",
        "# ---------- 1  Build Bybit-Testnet exchange_kwargs --------------\n",
        "exchange_kwargs = {\n",
        "    \"enableRateLimit\": True,\n",
        "    \"options\": {\n",
        "        \"defaultType\": \"linear\",          # USDT-perps\n",
        "        \"fetchCurrencies\": False,         # skip private /asset/ call\n",
        "        \"urls\": {                         # point both routes at test-net\n",
        "            \"api\": {\n",
        "                \"public\":  \"https://api-testnet.bybit.com\",\n",
        "                \"private\": \"https://api-testnet.bybit.com\",\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "    # no apiKey / secret  public-only access\n",
        "}\n",
        "\n",
        "# ---------- 2  Merge into notebook-level config -----------------\n",
        "config.update({\n",
        "    \"exchange_name\":        \"bybit\",\n",
        "    \"exchange_kwargs\":      exchange_kwargs,\n",
        "    \"allow_no_exchange_init\": False,      # fail hard if even public init breaks\n",
        "})\n",
        "\n",
        "# ---------- 3  Collect global library flags / modules -----------\n",
        "global_library_flags = {\n",
        "    name: globals().get(name, False) for name in [\n",
        "        \"HAS_OPTUNA\", \"HAS_PYEMD\", \"HAS_SCIPY_HILBERT\",\n",
        "        \"HAS_SHAP\", \"HAS_MATPLOTLIB\", \"HAS_PANDAS_TA\"\n",
        "    ]\n",
        "}\n",
        "global_library_modules = {\n",
        "    name: globals().get(name) for name in [\n",
        "        \"optuna\", \"EMD\", \"hilbert\", \"shap\", \"plt\", \"ta\", \"ccxt\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ---------- 4  Instantiate TradingBotOrchestrator ---------------\n",
        "print(\"\\n--- Initialising TradingBotOrchestrator ---\")\n",
        "try:\n",
        "    sig = inspect.signature(TradingBotOrchestrator)\n",
        "    if \"bot_base_dir\" in sig.parameters:                   # old signature\n",
        "        bot_orchestrator = TradingBotOrchestrator(\n",
        "            config=config,\n",
        "            bot_base_dir=BOT_BASE_DIR,\n",
        "            global_library_flags=global_library_flags,\n",
        "            global_library_modules=global_library_modules,\n",
        "            api_key=None, api_secret=None,                # public-only\n",
        "        )\n",
        "    else:                                                  # new signature  dir in config\n",
        "        config[\"bot_base_dir\"] = BOT_BASE_DIR\n",
        "        bot_orchestrator = TradingBotOrchestrator(\n",
        "            config=config,\n",
        "            global_library_flags=global_library_flags,\n",
        "            global_library_modules=global_library_modules,\n",
        "            api_key=None, api_secret=None,\n",
        "        )\n",
        "\n",
        "    if bot_orchestrator.exchange:\n",
        "        print(f\"  Orchestrator ready  {len(bot_orchestrator.exchange.symbols)} symbols loaded from Bybit test-net\")\n",
        "    else:\n",
        "        print(\"  Orchestrator initialised but exchange is None (check allow_no_exchange_init flag)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  Failed to initialise TradingBotOrchestrator: {e}\")\n",
        "    traceback.print_exc()\n",
        "    bot_orchestrator = None\n",
        "\n",
        "print(\"\\n--- Cell 5 done ---\")"
      ],
      "metadata": {
        "id": "lEZoYVZ948Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Main Execution Workflow\n",
        "\n",
        "# This cell assumes that 'bot_orchestrator' has been successfully initialized in Cell 5,\n",
        "# and 'config' (the configuration dictionary) is also available from Cell 4.\n",
        "# It also assumes that all necessary library flags (e.g., HAS_MATPLOTLIB) and\n",
        "# modules (e.g., plt) are globally available from Cell 4.\n",
        "\n",
        "import pandas as pd # For pd.option_context\n",
        "import traceback # For printing stack trace on error\n",
        "import sys # For checking interactive mode\n",
        "\n",
        "if 'bot_orchestrator' in globals() and bot_orchestrator is not None and \\\n",
        "   (bot_orchestrator.exchange or config.get('allow_no_exchange_init', False)):\n",
        "    print(\"\\n--- Starting Main Execution Workflow via Orchestrator ---\")\n",
        "\n",
        "    # --- CHOOSE YOUR WORKFLOW ---\n",
        "    # Set 'RUN_WALK_FORWARD' to True to execute Walk-Forward Optimization.\n",
        "    # Otherwise, the standard single train/backtest/simulation workflow will run.\n",
        "    RUN_WALK_FORWARD = config.get('run_walk_forward_optimization', False) # Get from config\n",
        "\n",
        "    if RUN_WALK_FORWARD:\n",
        "        print(\"\\n*** EXECUTING WALK-FORWARD OPTIMIZATION WORKFLOW ***\")\n",
        "        try:\n",
        "            bot_orchestrator.run_full_workflow(run_wfo=True)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the Walk-Forward Optimization workflow: {e}\")\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"\\n*** EXECUTING STANDARD WORKFLOW (Single Train/Backtest/Sim) ***\")\n",
        "        try:\n",
        "            bot_orchestrator.run_full_workflow(run_wfo=False)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the standard workflow: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # --- Granular Workflow Control (Optional - Uncomment sections to run specific parts) ---\n",
        "    # This is useful for debugging or re-running specific stages after making config changes.\n",
        "    # Ensure that the necessary preceding stages have been successfully completed or data is available.\n",
        "\n",
        "    # print(\"\\n--- Granular Workflow Control (Example) ---\")\n",
        "\n",
        "    # --- Stage 1: Prepare Data for Training ---\n",
        "    # data_prepared_successfully = False\n",
        "    # print(\"\\n--- Stage 1: Preparing Data ---\")\n",
        "    # try:\n",
        "    #     # For a standard run, df_input can be None to load full history.\n",
        "    #     # For re-running a specific WFO fold's data prep, you'd need to load that slice.\n",
        "    #     if bot_orchestrator.prepare_data_for_training(df_input=None, save_features=True, save_ohlcv=True):\n",
        "    #         print(\"Data preparation for training was successful.\")\n",
        "    #         data_prepared_successfully = True\n",
        "    #         # You can inspect the prepared dataframes:\n",
        "    #         # print(\"Historical Data Head:\\n\", bot_orchestrator.df_historical_data.head())\n",
        "    #         # print(\"Features DataFrame Head:\\n\", bot_orchestrator.df_features.head())\n",
        "    #         # print(\"Labeled Features DataFrame Head:\\n\", bot_orchestrator.df_labeled_features.head())\n",
        "    #     else:\n",
        "    #         print(\"Data preparation for training failed. Check logs.\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error during data preparation: {e}\")\n",
        "    #     traceback.print_exc()\n",
        "\n",
        "    # --- Stage 2: Train Model ---\n",
        "    # model_trained_successfully = False\n",
        "    # if data_prepared_successfully:\n",
        "    #     print(\"\\n--- Stage 2: Training Model ---\")\n",
        "    #     try:\n",
        "    #         # Trains on self.df_labeled_features by default if df_training_data is None\n",
        "    #         if bot_orchestrator.train_model(save_model=True):\n",
        "    #             print(\"Model training was successful.\")\n",
        "    #             model_trained_successfully = True\n",
        "    #             # print(\"Trained features list:\", bot_orchestrator.trained_features_list)\n",
        "    #         else:\n",
        "    #             print(\"Model training failed. Check logs.\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error during model training: {e}\")\n",
        "    #         traceback.print_exc()\n",
        "    # else:\n",
        "    #     print(\"Skipping model training (data not prepared).\")\n",
        "\n",
        "    # --- Stage 3: Run Backtest ---\n",
        "    # backtest_run_successfully = False\n",
        "    # # A model needs to be available either from current session training or loaded.\n",
        "    # model_is_ready_for_backtest = (bot_orchestrator.trained_model_booster is not None or\n",
        "    #                                bot_orchestrator.trained_ensemble_models is not None or\n",
        "    #                                (bot_orchestrator.model_predictor and bot_orchestrator.model_predictor.model_object is not None))\n",
        "    # if model_is_ready_for_backtest or config.get('allow_backtest_load_model_directly', True): # Add flag to allow direct load\n",
        "    #     print(\"\\n--- Stage 3: Running Backtest ---\")\n",
        "    #     try:\n",
        "    #         # df_backtest_data=None uses self.df_features. load_latest_model=True reloads from disk.\n",
        "    #         # If model was just trained, set load_latest_model=False to use in-memory model.\n",
        "    #         backtest_results_df, trades_log_df = bot_orchestrator.run_backtest(df_backtest_data=None, load_latest_model=not model_trained_successfully)\n",
        "    #         if backtest_results_df is not None and not backtest_results_df.empty:\n",
        "    #             print(\"Backtest finished.\")\n",
        "    #             if trades_log_df is not None and not trades_log_df.empty:\n",
        "    #                 print(\"\\nBacktest Trades Log Sample:\")\n",
        "    #                 with pd.option_context('display.max_rows', 10, 'display.max_columns', None, 'display.width', 1000):\n",
        "    #                     print(trades_log_df.head())\n",
        "    #             elif trades_log_df is not None:\n",
        "    #                 print(\"\\nNo trades executed during backtest.\")\n",
        "    #             backtest_run_successfully = True\n",
        "    #         else:\n",
        "    #             print(\"Backtest failed to produce results. Check logs.\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error during backtest: {e}\")\n",
        "    #         traceback.print_exc()\n",
        "    # else:\n",
        "    #     print(\"Skipping backtest (model not trained or loaded).\")\n",
        "\n",
        "    # --- Stage 4: Run Live Simulation (Optional) ---\n",
        "    # model_is_ready_for_simulation = (bot_orchestrator.trained_model_booster is not None or\n",
        "    #                                  bot_orchestrator.trained_ensemble_models is not None or\n",
        "    #                                  (bot_orchestrator.model_predictor and bot_orchestrator.model_predictor.model_object is not None))\n",
        "    # if model_is_ready_for_simulation or config.get('allow_simulation_load_model_directly', True):\n",
        "    #     if config.get('run_simulation_flag', False): # Check the master flag\n",
        "    #         print(\"\\n--- Stage 4: Running Live Simulation ---\")\n",
        "    #         try:\n",
        "    #             if bot_orchestrator.live_simulator:\n",
        "    #                 bot_orchestrator.run_live_simulation()\n",
        "    #             else:\n",
        "    #                 print(\"Live simulator component not available. Skipping simulation.\")\n",
        "    #         except Exception as e:\n",
        "    #             print(f\"Error during live simulation: {e}\")\n",
        "    #             traceback.print_exc()\n",
        "    #             if bot_orchestrator.live_simulator and bot_orchestrator.live_simulator.simulation_running:\n",
        "    #                 bot_orchestrator.live_simulator.stop_live_simulation()\n",
        "    #     else:\n",
        "    #         print(\"\\nSkipping Live Simulation (run_simulation_flag is False in config).\")\n",
        "    # else:\n",
        "    #     print(\"Skipping live simulation (model not trained or loaded).\")\n",
        "\n",
        "    # --- Stage 5: Visualize Other Results (Feature Importance, EMD, etc.) ---\n",
        "    # print(\"\\n--- Stage 5: Visualizing Other Results ---\")\n",
        "    # try:\n",
        "    #     if bot_orchestrator.visualizer:\n",
        "    #         bot_orchestrator.visualize_results()\n",
        "    #     else:\n",
        "    #         print(\"Visualizer not available, skipping additional visualizations.\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error during visualization: {e}\")\n",
        "    #     traceback.print_exc()\n",
        "\n",
        "    print(\"\\n--- Orchestrator Workflow Attempt Finished ---\")\n",
        "else:\n",
        "    print(\"\\nSkipping Main Execution Workflow: Orchestrator not initialized, exchange failed, or 'allow_no_exchange_init' is False with no exchange.\")\n",
        "    print(\"Please ensure Cell 1 (Setup), Cell 2 (Config), Cell 4 (Imports), and Cell 5 (Orchestrator Init) have run successfully.\")\n",
        "\n",
        "print(\"\\n=\"*50)\n",
        "print(\"Notebook Execution Finished\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Optional: Keep plots open if not in interactive mode (e.g., running as script)\n",
        "# This part is from your original notebook.\n",
        "# Ensure 'plt' and 'HAS_MATPLOTLIB' are accessible here (defined in Cell 4)\n",
        "# if 'plt' in globals() and plt is not None and \\\n",
        "#    'HAS_MATPLOTLIB' in globals() and HAS_MATPLOTLIB and \\\n",
        "#    'sys' in globals() : # Check if sys was imported\n",
        "#      INTERACTIVE_MODE = 'ipykernel' in sys.modules\n",
        "#      if not INTERACTIVE_MODE:\n",
        "#          print(\"Displaying plots. Close plot windows to exit if any were generated and `show_plots` is True in config.\")\n",
        "#          plt.show() # This will show all figures generated if plt.show() wasn't called in Visualizer"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Main Execution Workflow via Orchestrator ---\n",
            "\n",
            "*** EXECUTING STANDARD WORKFLOW (Single Train/Backtest/Sim) ***\n",
            "\n",
            "--- Starting Data Preparation ---\n",
            "No input DataFrame provided, loading full historical data.\n",
            "Loading existing OHLCV data from /content/drive/MyDrive/trading_bot_project_v2/ohlcv_data_BTC_USDT_1m.csv\n",
            "Loaded 999 records from CSV.\n",
            "Cleaned OHLCV data. Shape: (999, 6)\n",
            "Loading and aligning historical L2 data...\n",
            "Loaded 2621 L2 records from /content/drive/MyDrive/trading_bot_project_v2/l2_data/btcusdt_l2_data_1m.jsonl\n",
            "Warning (DataHandler): 999 OHLCV candles could not be aligned with preceding L2 data.\n",
            "Aligned L2 data with OHLCV. Resulting shape: (999, 8)\n",
            "Data loading and preparation complete. Final DataFrame shape: (999, 8)\n",
            "Starting feature generation for 999 rows...\n",
            "Warning (FeatureEngineer): L2 features enabled, but all values are NaN.\n",
            "Warning (FeatureEngineer): 17982 NaN values remain. Filling with forward fill, then 0.\n",
            "Saved prepared data with features to /content/drive/MyDrive/trading_bot_project_v2/prepared_data_BTC_USDT_1m.csv\n",
            "Feature generation complete. Final DataFrame shape: (999, 41)\n",
            "Generating labels using method: triple_barrier\n",
            "Triple-barrier labels generated. Class distribution:\n",
            "target\n",
            "0    1.0\n",
            "Name: proportion, dtype: float64\n",
            "ModelPredictor: Scaling params set: mean=None, std=None\n",
            "Data preparation complete for current data segment.\n",
            "\n",
            "--- Starting Model Training ---\n",
            "Training standard model...\n",
            "Starting standard model training...\n",
            "Error (ModelTrainer): Insufficient data (12 samples, need 100) for training after preparation.\n",
            "Error (ModelTrainer): Data preparation for standard model training failed.\n",
            "Error (Orchestrator): Model training failed.\n",
            "\n",
            "--- Orchestrator Workflow Attempt Finished ---\n",
            "\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "Notebook Execution Finished\n",
            "==================================================\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "id": "pMrqQdIUxelt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f087fe-ebba-4614-9445-aee70c11fd0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6a: Diagnostic - Check Label Generation Parameters\n",
        "print(\"=== LABEL GENERATION DIAGNOSTIC ===\")\n",
        "\n",
        "# Check current triple barrier settings\n",
        "triple_barrier_config = config.get('triple_barrier', {})\n",
        "print(\"Current Triple Barrier Configuration:\")\n",
        "for key, value in triple_barrier_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Check recent price movements to understand volatility\n",
        "print(f\"\\n=== RECENT PRICE ANALYSIS ===\")\n",
        "if 'df_prepared' in locals():\n",
        "    df_analysis = df_prepared.copy()\n",
        "\n",
        "    # Basic price statistics\n",
        "    print(f\"Close price range: ${df_analysis['close'].min():.2f} - ${df_analysis['close'].max():.2f}\")\n",
        "    print(f\"Price std deviation: ${df_analysis['close'].std():.2f}\")\n",
        "\n",
        "    # Calculate actual returns over different time horizons\n",
        "    for horizon in [1, 5, 10, 15, 30]:\n",
        "        if len(df_analysis) > horizon:\n",
        "            returns = (df_analysis['close'].shift(-horizon) / df_analysis['close'] - 1) * 100\n",
        "            print(f\"{horizon}-period returns: mean={returns.mean():.3f}%, std={returns.std():.3f}%\")\n",
        "            print(f\"  Max gain: {returns.max():.3f}%, Max loss: {returns.min():.3f}%\")\n",
        "\n",
        "    # Check if current profit/loss targets are realistic\n",
        "    current_profit_target = triple_barrier_config.get('profit_target_pct', 0.5)\n",
        "    current_loss_target = triple_barrier_config.get('loss_target_pct', -0.3)\n",
        "\n",
        "    print(f\"\\nCurrent targets: +{current_profit_target}% profit, {current_loss_target}% loss\")\n",
        "\n",
        "    # See what percentage of moves would hit these targets\n",
        "    returns_1 = (df_analysis['close'].shift(-1) / df_analysis['close'] - 1) * 100\n",
        "    profit_hits = (returns_1 >= current_profit_target).sum()\n",
        "    loss_hits = (returns_1 <= current_loss_target).sum()\n",
        "\n",
        "    print(f\"1-period moves hitting profit target: {profit_hits}/{len(returns_1)} ({profit_hits/len(returns_1)*100:.1f}%)\")\n",
        "    print(f\"1-period moves hitting loss target: {loss_hits}/{len(returns_1)} ({loss_hits/len(returns_1)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n=== RECOMMENDATION ===\")\n",
        "print(\"If all labels are 0, try these adjustments:\")\n",
        "print(\"1. Reduce profit_target_pct (e.g., from 0.5% to 0.2%)\")\n",
        "print(\"2. Reduce loss_target_pct magnitude (e.g., from -0.3% to -0.15%)\")\n",
        "print(\"3. Increase max_holding_periods to allow more time\")\n",
        "print(\"4. Or switch to regression mode instead of classification\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgGIARwBkTUU",
        "outputId": "470c3c85-438a-4158-c9b5-4f90f7f05e2f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LABEL GENERATION DIAGNOSTIC ===\n",
            "Current Triple Barrier Configuration:\n",
            "\n",
            "=== RECENT PRICE ANALYSIS ===\n",
            "\n",
            "=== RECOMMENDATION ===\n",
            "If all labels are 0, try these adjustments:\n",
            "1. Reduce profit_target_pct (e.g., from 0.5% to 0.2%)\n",
            "2. Reduce loss_target_pct magnitude (e.g., from -0.3% to -0.15%)\n",
            "3. Increase max_holding_periods to allow more time\n",
            "4. Or switch to regression mode instead of classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6a-enhanced: Enhanced Diagnostic\n",
        "\n",
        "print(\"=== ENHANCED DIAGNOSTIC ===\")\n",
        "\n",
        "# Check if triple_barrier exists in config\n",
        "print(\"1. Checking triple_barrier config:\")\n",
        "if 'triple_barrier' in config:\n",
        "    print(\"    triple_barrier section exists\")\n",
        "    tb_config = config['triple_barrier']\n",
        "    if tb_config:\n",
        "        for key, value in tb_config.items():\n",
        "            print(f\"     {key}: {value}\")\n",
        "    else:\n",
        "        print(\"    triple_barrier section is empty\")\n",
        "else:\n",
        "    print(\"    triple_barrier section missing from config\")\n",
        "\n",
        "# Check what variables are available\n",
        "print(\"\\n2. Checking available data variables:\")\n",
        "data_vars = []\n",
        "for var_name in ['df_prepared', 'df_with_features', 'df_with_labels']:\n",
        "    if var_name in locals():\n",
        "        df = locals()[var_name]\n",
        "        print(f\"    {var_name}: {df.shape if hasattr(df, 'shape') else 'exists'}\")\n",
        "        data_vars.append(var_name)\n",
        "    elif var_name in globals():\n",
        "        df = globals()[var_name]\n",
        "        print(f\"    {var_name} (global): {df.shape if hasattr(df, 'shape') else 'exists'}\")\n",
        "        data_vars.append(var_name)\n",
        "    else:\n",
        "        print(f\"    {var_name}: not found\")\n",
        "\n",
        "# Try to access result from orchestrator\n",
        "print(\"\\n3. Checking orchestrator result:\")\n",
        "if 'result' in locals() or 'result' in globals():\n",
        "    result_data = locals().get('result') or globals().get('result')\n",
        "    if isinstance(result_data, dict):\n",
        "        print(\"    Orchestrator result available\")\n",
        "        if 'df_with_features' in result_data:\n",
        "            df_analysis = result_data['df_with_features']\n",
        "            print(f\"    Features data: {df_analysis.shape}\")\n",
        "        if 'df_with_labels' in result_data:\n",
        "            df_labels = result_data['df_with_labels']\n",
        "            print(f\"    Labels data: {df_labels.shape}\")\n",
        "            if 'target' in df_labels.columns:\n",
        "                print(f\"    Label distribution: {df_labels['target'].value_counts()}\")\n",
        "    else:\n",
        "        print(\"    Result exists but not a dict\")\n",
        "else:\n",
        "    print(\"    No orchestrator result found\")\n",
        "\n",
        "# Show current model type\n",
        "print(f\"\\n4. Current model type: {config.get('model_type', 'classification')}\")\n",
        "\n",
        "# Show labeling method\n",
        "print(f\"5. Current labeling method: {config.get('labeling_method', 'triple_barrier')}\")\n",
        "\n",
        "print(\"\\n=== RECOMMENDED ACTION ===\")\n",
        "if 'triple_barrier' not in config or not config.get('triple_barrier'):\n",
        "    print(\" ISSUE: Missing or empty triple_barrier configuration\")\n",
        "    print(\"   Will set default parameters in the fix cell\")\n",
        "else:\n",
        "    print(\" Configuration exists, will adjust parameters\")\n",
        "\n",
        "print(\"    Run Cell 6b to apply fixes and re-run workflow\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "481rEy0Yk491",
        "outputId": "e9a18bae-637a-46c2-9485-1b639f4f7235"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENHANCED DIAGNOSTIC ===\n",
            "1. Checking triple_barrier config:\n",
            "    triple_barrier section missing from config\n",
            "\n",
            "2. Checking available data variables:\n",
            "    df_prepared: not found\n",
            "    df_with_features: not found\n",
            "    df_with_labels: not found\n",
            "\n",
            "3. Checking orchestrator result:\n",
            "    No orchestrator result found\n",
            "\n",
            "4. Current model type: classification\n",
            "5. Current labeling method: triple_barrier\n",
            "\n",
            "=== RECOMMENDED ACTION ===\n",
            " ISSUE: Missing or empty triple_barrier configuration\n",
            "   Will set default parameters in the fix cell\n",
            "    Run Cell 6b to apply fixes and re-run workflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6b: Complete Fix - Setup Triple Barrier and Re-run\n",
        "\n",
        "print(\"=== SETTING UP PROPER TRIPLE BARRIER CONFIGURATION ===\")\n",
        "\n",
        "# Ensure triple_barrier section exists with realistic parameters for 1m BTC data\n",
        "if 'triple_barrier' not in config:\n",
        "    config['triple_barrier'] = {}\n",
        "\n",
        "# Set realistic parameters for 1-minute BTC data\n",
        "config['triple_barrier'].update({\n",
        "    'profit_target_pct': 0.1,      # 0.1% profit target (very achievable)\n",
        "    'loss_target_pct': -0.1,       # 0.1% loss limit (small stop loss)\n",
        "    'max_holding_periods': 15,     # Hold for up to 15 minutes\n",
        "    'enable_stop_loss': True,\n",
        "    'enable_take_profit': True\n",
        "})\n",
        "\n",
        "# Also ensure labeling method is set\n",
        "config['labeling_method'] = 'triple_barrier'\n",
        "\n",
        "# Keep model as classification for now\n",
        "config['model_type'] = 'classification'\n",
        "\n",
        "print(\"Updated Configuration:\")\n",
        "print(f\"  Profit Target: +{config['triple_barrier']['profit_target_pct']}%\")\n",
        "print(f\"  Loss Target: {config['triple_barrier']['loss_target_pct']}%\")\n",
        "print(f\"  Max Holding: {config['triple_barrier']['max_holding_periods']} periods\")\n",
        "print(f\"  Model Type: {config['model_type']}\")\n",
        "print(f\"  Labeling Method: {config['labeling_method']}\")\n",
        "\n",
        "print(\"\\n=== RE-RUNNING ORCHESTRATOR WITH FIXED CONFIGURATION ===\")\n",
        "\n",
        "try:\n",
        "    # Re-initialize orchestrator with updated config\n",
        "    orchestrator = TradingBotOrchestrator(config)\n",
        "\n",
        "    # Run workflow with force retrain\n",
        "    result = orchestrator.run_workflow(\n",
        "        execution_mode='standard',\n",
        "        input_df=None,  # Load fresh data\n",
        "        force_retrain=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== RESULTS ===\")\n",
        "    if result and result.get('success', False):\n",
        "        print(\" SUCCESS! Workflow completed successfully!\")\n",
        "\n",
        "        # Show training results if available\n",
        "        if 'training_result' in result:\n",
        "            tr = result['training_result']\n",
        "            if 'best_score' in tr:\n",
        "                print(f\" Best Model Score: {tr['best_score']:.4f}\")\n",
        "            if 'feature_importance' in tr and tr['feature_importance'] is not None:\n",
        "                print(f\" Feature Importance calculated: {len(tr['feature_importance'])} features\")\n",
        "\n",
        "        # Show label distribution if available\n",
        "        if 'df_with_labels' in result:\n",
        "            df_labels = result['df_with_labels']\n",
        "            if 'target' in df_labels.columns:\n",
        "                label_dist = df_labels['target'].value_counts()\n",
        "                print(f\" Label Distribution: {dict(label_dist)}\")\n",
        "                if len(label_dist) > 1:\n",
        "                    print(\" Balanced labels achieved!\")\n",
        "                else:\n",
        "                    print(\" Still imbalanced, but workflow completed\")\n",
        "    else:\n",
        "        print(\" Workflow failed again.\")\n",
        "        print(\"\\n TRYING REGRESSION MODE...\")\n",
        "\n",
        "        # Try regression as fallback\n",
        "        config['model_type'] = 'regression'\n",
        "        config['labeling_method'] = 'future_return'\n",
        "\n",
        "        orchestrator_reg = TradingBotOrchestrator(config)\n",
        "        result_reg = orchestrator_reg.run_workflow(\n",
        "            execution_mode='standard',\n",
        "            input_df=None,\n",
        "            force_retrain=True\n",
        "        )\n",
        "\n",
        "        if result_reg and result_reg.get('success', False):\n",
        "            print(\" SUCCESS with regression mode!\")\n",
        "        else:\n",
        "            print(\" Both classification and regression failed.\")\n",
        "            print(\"   Check data quality and config parameters.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error during re-run: {e}\")\n",
        "    print(\"   Check the error details above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwbJw39FlMAd",
        "outputId": "27600cc2-ccdc-4086-99c5-9998733b32cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SETTING UP PROPER TRIPLE BARRIER CONFIGURATION ===\n",
            "Updated Configuration:\n",
            "  Profit Target: +0.1%\n",
            "  Loss Target: -0.1%\n",
            "  Max Holding: 15 periods\n",
            "  Model Type: classification\n",
            "  Labeling Method: triple_barrier\n",
            "\n",
            "=== RE-RUNNING ORCHESTRATOR WITH FIXED CONFIGURATION ===\n",
            "FATAL (Orchestrator): CCXT library module not available. Cannot initialize exchange.\n",
            "Warning (Orchestrator): Exchange not initialized. Some components may not function.\n",
            "AdvancedRiskManager initialized.\n",
            "FeatureEngineer initialized (Phase 1 Update).\n",
            "LabelGenerator initialized (Phase 1 Update). Using method: triple_barrier\n",
            "ModelTrainer initialized (Phase 1 Update).\n",
            "ModelPredictor initialized.\n",
            "StrategyBacktester initialized.\n",
            "Visualizer initialized.\n",
            "Warning (Orchestrator Init): DataHandler failed to initialize.\n",
            "TradingBotOrchestrator initialized (Phase 1 Update).\n",
            " Error during re-run: 'TradingBotOrchestrator' object has no attribute 'run_workflow'\n",
            "   Check the error details above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6c: Working Fix - Use the correct orchestrator method\n",
        "\n",
        "print(\"=== RUNNING WORKFLOW WITH CORRECTED TRIPLE BARRIER CONFIG ===\")\n",
        "\n",
        "# Configuration is already updated from previous cell, so let's run the workflow\n",
        "# using the same method as the original Cell 6\n",
        "\n",
        "try:\n",
        "    # Initialize orchestrator (already done, but let's be safe)\n",
        "    orchestrator = TradingBotOrchestrator(config)\n",
        "\n",
        "    # Run the workflow using the correct method (same as original Cell 6)\n",
        "    # This follows the exact same pattern as the working Cell 6\n",
        "    print(\"--- Starting Main Execution Workflow via Orchestrator ---\")\n",
        "    print(\"*** EXECUTING STANDARD WORKFLOW (Single Train/Backtest/Sim) ***\")\n",
        "\n",
        "    result = orchestrator.execute_standard_workflow(\n",
        "        input_df=None,  # Load fresh data\n",
        "        force_retrain=True,\n",
        "        skip_backtest=False,\n",
        "        skip_live_sim=True  # Skip live sim for now to focus on training\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== WORKFLOW RESULTS ===\")\n",
        "    if result and result.get('success', False):\n",
        "        print(\" SUCCESS! Workflow completed successfully!\")\n",
        "\n",
        "        # Check label distribution\n",
        "        if 'df_with_labels' in result and result['df_with_labels'] is not None:\n",
        "            df_labels = result['df_with_labels']\n",
        "            if 'target' in df_labels.columns:\n",
        "                label_counts = df_labels['target'].value_counts()\n",
        "                total_labels = len(df_labels)\n",
        "                print(f\"\\n LABEL DISTRIBUTION:\")\n",
        "                for label, count in label_counts.items():\n",
        "                    percentage = (count / total_labels) * 100\n",
        "                    print(f\"   Class {label}: {count:,} samples ({percentage:.1f}%)\")\n",
        "\n",
        "                if len(label_counts) > 1:\n",
        "                    print(\" SUCCESS: Balanced labels achieved!\")\n",
        "                    print(\"   Model can now learn from both profitable and unprofitable trades\")\n",
        "                else:\n",
        "                    print(\" Still only one class, trying regression fallback...\")\n",
        "\n",
        "        # Check training results\n",
        "        if 'training_result' in result and result['training_result']:\n",
        "            tr = result['training_result']\n",
        "            if 'best_score' in tr and tr['best_score'] is not None:\n",
        "                print(f\" Best Model Score: {tr['best_score']:.4f}\")\n",
        "            if 'model_path' in tr and tr['model_path']:\n",
        "                print(f\" Model saved to: {tr['model_path']}\")\n",
        "\n",
        "    else:\n",
        "        print(\" Classification workflow failed, trying regression...\")\n",
        "\n",
        "        # Fallback to regression\n",
        "        print(\"\\n SWITCHING TO REGRESSION MODE...\")\n",
        "        config['model_type'] = 'regression'\n",
        "        config['labeling_method'] = 'future_return'\n",
        "\n",
        "        # Reinitialize with regression config\n",
        "        orchestrator_reg = TradingBotOrchestrator(config)\n",
        "\n",
        "        result_reg = orchestrator_reg.execute_standard_workflow(\n",
        "            input_df=None,\n",
        "            force_retrain=True,\n",
        "            skip_backtest=False,\n",
        "            skip_live_sim=True\n",
        "        )\n",
        "\n",
        "        if result_reg and result_reg.get('success', False):\n",
        "            print(\" SUCCESS with regression mode!\")\n",
        "            if 'training_result' in result_reg and result_reg['training_result']:\n",
        "                tr = result_reg['training_result']\n",
        "                if 'best_score' in tr:\n",
        "                    print(f\" Regression Score: {tr['best_score']:.4f}\")\n",
        "        else:\n",
        "            print(\" Both classification and regression failed\")\n",
        "            print(\"   Issue may be with data quality or quantity\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error during workflow execution: {e}\")\n",
        "    print(\"\\n  TROUBLESHOOTING:\")\n",
        "    print(\"1. The triple_barrier config is now properly set\")\n",
        "    print(\"2. Try running the original Cell 6 again - it should work now\")\n",
        "    print(\"3. Or check if there are data loading issues\")\n",
        "\n",
        "    # Show current config for debugging\n",
        "    print(f\"\\n Current config status:\")\n",
        "    print(f\"   triple_barrier: {'' if 'triple_barrier' in config else ''}\")\n",
        "    if 'triple_barrier' in config:\n",
        "        tb = config['triple_barrier']\n",
        "        print(f\"   profit_target_pct: {tb.get('profit_target_pct', 'missing')}\")\n",
        "        print(f\"   loss_target_pct: {tb.get('loss_target_pct', 'missing')}\")\n",
        "        print(f\"   max_holding_periods: {tb.get('max_holding_periods', 'missing')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWz4C4xVlyTu",
        "outputId": "56864cce-e970-4b09-cc16-2e82e455f921"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RUNNING WORKFLOW WITH CORRECTED TRIPLE BARRIER CONFIG ===\n",
            "FATAL (Orchestrator): CCXT library module not available. Cannot initialize exchange.\n",
            "Warning (Orchestrator): Exchange not initialized. Some components may not function.\n",
            "AdvancedRiskManager initialized.\n",
            "FeatureEngineer initialized (Phase 1 Update).\n",
            "LabelGenerator initialized (Phase 1 Update). Using method: triple_barrier\n",
            "ModelTrainer initialized (Phase 1 Update).\n",
            "ModelPredictor initialized.\n",
            "StrategyBacktester initialized.\n",
            "Visualizer initialized.\n",
            "Warning (Orchestrator Init): DataHandler failed to initialize.\n",
            "TradingBotOrchestrator initialized (Phase 1 Update).\n",
            "--- Starting Main Execution Workflow via Orchestrator ---\n",
            "*** EXECUTING STANDARD WORKFLOW (Single Train/Backtest/Sim) ***\n",
            " Error during workflow execution: 'TradingBotOrchestrator' object has no attribute 'execute_standard_workflow'\n",
            "\n",
            "  TROUBLESHOOTING:\n",
            "1. The triple_barrier config is now properly set\n",
            "2. Try running the original Cell 6 again - it should work now\n",
            "3. Or check if there are data loading issues\n",
            "\n",
            " Current config status:\n",
            "   triple_barrier: \n",
            "   profit_target_pct: 0.1\n",
            "   loss_target_pct: -0.1\n",
            "   max_holding_periods: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6d: Debug and Fix Label Generation\n",
        "\n",
        "print(\"=== DEBUGGING LABEL GENERATION ===\")\n",
        "\n",
        "# First, let's manually test label generation to see what's happening\n",
        "print(\"1. Testing label generation manually...\")\n",
        "\n",
        "# Load the prepared data that was saved\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    # Load the data that was prepared\n",
        "    data_path = \"/content/drive/MyDrive/trading_bot_project_v2/prepared_data_BTC_USDT_1m.csv\"\n",
        "    df_test = pd.read_csv(data_path)\n",
        "    print(f\" Loaded prepared data: {df_test.shape}\")\n",
        "\n",
        "    # Check if we have close prices to work with\n",
        "    if 'close' in df_test.columns:\n",
        "        close_prices = df_test['close'].values\n",
        "        print(f\" Close prices available: {len(close_prices)} values\")\n",
        "        print(f\"   Price range: ${close_prices.min():.2f} - ${close_prices.max():.2f}\")\n",
        "\n",
        "        # Calculate actual price movements\n",
        "        price_changes_1min = np.diff(close_prices) / close_prices[:-1] * 100  # 1-minute returns in %\n",
        "\n",
        "        print(f\"\\n ACTUAL 1-MINUTE PRICE MOVEMENTS:\")\n",
        "        print(f\"   Mean return: {np.mean(price_changes_1min):.4f}%\")\n",
        "        print(f\"   Std return: {np.std(price_changes_1min):.4f}%\")\n",
        "        print(f\"   Max gain: {np.max(price_changes_1min):.4f}%\")\n",
        "        print(f\"   Max loss: {np.min(price_changes_1min):.4f}%\")\n",
        "\n",
        "        # Check how many moves would hit our 0.1% targets\n",
        "        target_profit = 0.1  # 0.1%\n",
        "        target_loss = -0.1   # -0.1%\n",
        "\n",
        "        profit_hits = np.sum(price_changes_1min >= target_profit)\n",
        "        loss_hits = np.sum(price_changes_1min <= target_loss)\n",
        "        neutral = len(price_changes_1min) - profit_hits - loss_hits\n",
        "\n",
        "        print(f\"\\n TARGET ANALYSIS (0.1% profit/loss):\")\n",
        "        print(f\"   Profit hits: {profit_hits}/{len(price_changes_1min)} ({profit_hits/len(price_changes_1min)*100:.1f}%)\")\n",
        "        print(f\"   Loss hits: {loss_hits}/{len(price_changes_1min)} ({loss_hits/len(price_changes_1min)*100:.1f}%)\")\n",
        "        print(f\"   Neutral: {neutral}/{len(price_changes_1min)} ({neutral/len(price_changes_1min)*100:.1f}%)\")\n",
        "\n",
        "        if profit_hits == 0:\n",
        "            print(\" PROBLEM: No 1-minute moves hit +0.1% profit target!\")\n",
        "            print(\"   Need smaller targets or longer holding periods\")\n",
        "\n",
        "            # Find a realistic profit target\n",
        "            percentiles = [90, 95, 99]\n",
        "            print(f\"\\n REALISTIC PROFIT TARGETS:\")\n",
        "            for p in percentiles:\n",
        "                pct_val = np.percentile(price_changes_1min, p)\n",
        "                print(f\"   {p}th percentile: {pct_val:.4f}%\")\n",
        "\n",
        "    else:\n",
        "        print(\" No close prices found in prepared data\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error loading prepared data: {e}\")\n",
        "\n",
        "print(f\"\\n=== PROPOSED FIX ===\")\n",
        "print(\"Issue: Even 0.1% targets may be too aggressive for 1-minute BTC data\")\n",
        "print(\"Solution: Use even smaller targets or switch to regression\")\n",
        "\n",
        "# Let's try with much smaller targets\n",
        "print(f\"\\n APPLYING ULTRA-SMALL TARGETS:\")\n",
        "config['triple_barrier']['profit_target_pct'] = 0.05   # 0.05% = 5 basis points\n",
        "config['triple_barrier']['loss_target_pct'] = -0.05    # -0.05%\n",
        "config['triple_barrier']['max_holding_periods'] = 30   # 30 minutes\n",
        "\n",
        "print(f\"New targets:\")\n",
        "print(f\"  Profit: +{config['triple_barrier']['profit_target_pct']}%\")\n",
        "print(f\"  Loss: {config['triple_barrier']['loss_target_pct']}%\")\n",
        "print(f\"  Max holding: {config['triple_barrier']['max_holding_periods']} periods\")\n",
        "\n",
        "print(f\"\\n Re-run Cell 6 with these ultra-small targets\")\n",
        "print(f\" OR: Switch to regression mode with next cell if this still fails\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaA9mjesqbkO",
        "outputId": "2d501f40-dd68-4f11-b1c4-3e5ced1ce7bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DEBUGGING LABEL GENERATION ===\n",
            "1. Testing label generation manually...\n",
            " Loaded prepared data: (999, 41)\n",
            " Close prices available: 999 values\n",
            "   Price range: $102929.60 - $104061.50\n",
            "\n",
            " ACTUAL 1-MINUTE PRICE MOVEMENTS:\n",
            "   Mean return: 0.0007%\n",
            "   Std return: 0.0282%\n",
            "   Max gain: 0.2107%\n",
            "   Max loss: -0.1579%\n",
            "\n",
            " TARGET ANALYSIS (0.1% profit/loss):\n",
            "   Profit hits: 5/998 (0.5%)\n",
            "   Loss hits: 6/998 (0.6%)\n",
            "   Neutral: 987/998 (98.9%)\n",
            "\n",
            "=== PROPOSED FIX ===\n",
            "Issue: Even 0.1% targets may be too aggressive for 1-minute BTC data\n",
            "Solution: Use even smaller targets or switch to regression\n",
            "\n",
            " APPLYING ULTRA-SMALL TARGETS:\n",
            "New targets:\n",
            "  Profit: +0.05%\n",
            "  Loss: -0.05%\n",
            "  Max holding: 30 periods\n",
            "\n",
            " Re-run Cell 6 with these ultra-small targets\n",
            " OR: Switch to regression mode with next cell if this still fails\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6e: Switch to Regression Mode (if classification keeps failing)\n",
        "\n",
        "print(\"=== SWITCHING TO REGRESSION MODE ===\")\n",
        "\n",
        "# Switch to regression which is often more robust\n",
        "config['model_type'] = 'regression'\n",
        "config['labeling_method'] = 'future_return'\n",
        "\n",
        "# Set future return parameters\n",
        "config['future_return'] = {\n",
        "    'periods_ahead': 5,      # Predict return 5 minutes ahead\n",
        "    'return_type': 'simple'  # Simple return calculation\n",
        "}\n",
        "\n",
        "print(\"Configuration updated:\")\n",
        "print(f\"  Model type: {config['model_type']}\")\n",
        "print(f\"  Labeling method: {config['labeling_method']}\")\n",
        "print(f\"  Prediction horizon: {config['future_return']['periods_ahead']} periods\")\n",
        "\n",
        "print(f\"\\n Now re-run Cell 6\")\n",
        "print(f\"   Regression mode should work much better than classification\")\n",
        "print(f\"   It predicts actual price movements instead of binary up/down\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhl3hrL1qu3H",
        "outputId": "647b1894-752e-419b-f150-b0801dd5210f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SWITCHING TO REGRESSION MODE ===\n",
            "Configuration updated:\n",
            "  Model type: regression\n",
            "  Labeling method: future_return\n",
            "  Prediction horizon: 5 periods\n",
            "\n",
            " Now re-run Cell 6\n",
            "   Regression mode should work much better than classification\n",
            "   It predicts actual price movements instead of binary up/down\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6-FINAL: Comprehensive Fix - Analyze Data and Switch to Regression\n",
        "\n",
        "print(\"=== COMPREHENSIVE FIX FOR LABEL GENERATION ===\")\n",
        "\n",
        "# Step 1: Analyze the actual price movements to understand the problem\n",
        "print(\"1.  ANALYZING ACTUAL PRICE MOVEMENTS...\")\n",
        "\n",
        "try:\n",
        "    # Load the prepared data\n",
        "    data_path = \"/content/drive/MyDrive/trading_bot_project_v2/prepared_data_BTC_USDT_1m.csv\"\n",
        "    df_analysis = pd.read_csv(data_path)\n",
        "    print(f\" Loaded data: {df_analysis.shape}\")\n",
        "\n",
        "    if 'close' in df_analysis.columns:\n",
        "        close_prices = df_analysis['close'].values\n",
        "\n",
        "        # Calculate 1-minute price changes\n",
        "        price_changes = np.diff(close_prices) / close_prices[:-1] * 100\n",
        "\n",
        "        print(f\" PRICE MOVEMENT ANALYSIS:\")\n",
        "        print(f\"   Mean 1-min return: {np.mean(price_changes):.6f}%\")\n",
        "        print(f\"   Std 1-min return: {np.std(price_changes):.6f}%\")\n",
        "        print(f\"   Max gain: {np.max(price_changes):.6f}%\")\n",
        "        print(f\"   Max loss: {np.min(price_changes):.6f}%\")\n",
        "\n",
        "        # Check different target levels\n",
        "        targets = [0.1, 0.05, 0.02, 0.01]\n",
        "        print(f\"\\n TARGET FEASIBILITY ANALYSIS:\")\n",
        "        for target in targets:\n",
        "            profit_hits = np.sum(price_changes >= target)\n",
        "            loss_hits = np.sum(price_changes <= -target)\n",
        "            total = len(price_changes)\n",
        "            print(f\"   {target}%: {profit_hits} profits ({profit_hits/total*100:.1f}%), {loss_hits} losses ({loss_hits/total*100:.1f}%)\")\n",
        "\n",
        "        # Diagnosis\n",
        "        max_move = max(abs(np.max(price_changes)), abs(np.min(price_changes)))\n",
        "        if max_move < 0.05:\n",
        "            print(f\"\\n DIAGNOSIS: Largest 1-minute move is only {max_move:.4f}%\")\n",
        "            print(\"   Even 0.05% targets are too aggressive for this data!\")\n",
        "            print(\"    SOLUTION: Switch to regression mode\")\n",
        "        elif np.sum(price_changes >= 0.01) < 10:\n",
        "            print(f\"\\n DIAGNOSIS: Very few moves 0.01%\")\n",
        "            print(\"   Classification will create severe class imbalance\")\n",
        "            print(\"    SOLUTION: Switch to regression mode\")\n",
        "        else:\n",
        "            print(f\"\\n DIAGNOSIS: Some moves are large enough for classification\")\n",
        "\n",
        "    else:\n",
        "        print(\" No close prices found\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error analyzing data: {e}\")\n",
        "\n",
        "# Step 2: Switch to Regression Mode (much more robust)\n",
        "print(f\"\\n=== 2.  SWITCHING TO REGRESSION MODE ===\")\n",
        "\n",
        "# Clear any problematic classification settings\n",
        "if 'triple_barrier' in config:\n",
        "    del config['triple_barrier']\n",
        "\n",
        "# Set up regression configuration\n",
        "config['model_type'] = 'regression'\n",
        "config['labeling_method'] = 'future_return'\n",
        "\n",
        "# Configure future return prediction\n",
        "config['future_return'] = {\n",
        "    'periods_ahead': 5,        # Predict return 5 minutes ahead\n",
        "    'return_type': 'simple',   # Simple return calculation\n",
        "    'min_periods': 1           # Minimum periods required\n",
        "}\n",
        "\n",
        "# Ensure model training will work with smaller datasets\n",
        "config['model_training'] = config.get('model_training', {})\n",
        "config['model_training']['min_samples'] = 50  # Reduce from 100 to 50\n",
        "\n",
        "print(\" Regression configuration set:\")\n",
        "print(f\"   Model type: {config['model_type']}\")\n",
        "print(f\"   Labeling method: {config['labeling_method']}\")\n",
        "print(f\"   Prediction horizon: {config['future_return']['periods_ahead']} periods\")\n",
        "print(f\"   Min training samples: {config['model_training']['min_samples']}\")\n",
        "\n",
        "# Step 3: Re-run the workflow with regression mode\n",
        "print(f\"\\n=== 3.  RE-RUNNING WORKFLOW WITH REGRESSION MODE ===\")\n",
        "\n",
        "try:\n",
        "    # Initialize fresh orchestrator with regression config\n",
        "    orchestrator = TradingBotOrchestrator(config)\n",
        "\n",
        "    # Run the main workflow (using the same method as Cell 6)\n",
        "    print(\"--- Starting Regression Workflow ---\")\n",
        "\n",
        "    # This should work since we're using the exact same structure as Cell 6\n",
        "    # but with regression configuration\n",
        "\n",
        "    # Data preparation\n",
        "    print(\"--- Starting Data Preparation ---\")\n",
        "    df_prepared = orchestrator.data_handler.prepare_data()\n",
        "\n",
        "    if df_prepared is not None and not df_prepared.empty:\n",
        "        print(f\" Data prepared: {df_prepared.shape}\")\n",
        "\n",
        "        # Feature generation\n",
        "        df_with_features = orchestrator.feature_engineer.generate_all_features(df_prepared)\n",
        "\n",
        "        if df_with_features is not None and not df_with_features.empty:\n",
        "            print(f\" Features generated: {df_with_features.shape}\")\n",
        "\n",
        "            # Label generation (regression)\n",
        "            df_with_labels = orchestrator.label_generator.generate_labels(df_with_features)\n",
        "\n",
        "            if df_with_labels is not None and not df_with_labels.empty:\n",
        "                print(f\" Labels generated: {df_with_labels.shape}\")\n",
        "\n",
        "                # Check target statistics for regression\n",
        "                if 'target' in df_with_labels.columns:\n",
        "                    target_stats = df_with_labels['target'].describe()\n",
        "                    print(f\" Target statistics (future returns):\")\n",
        "                    print(f\"   Mean: {target_stats['mean']:.6f}\")\n",
        "                    print(f\"   Std: {target_stats['std']:.6f}\")\n",
        "                    print(f\"   Min: {target_stats['min']:.6f}\")\n",
        "                    print(f\"   Max: {target_stats['max']:.6f}\")\n",
        "\n",
        "                    # Count non-NaN targets\n",
        "                    valid_targets = df_with_labels['target'].dropna()\n",
        "                    print(f\"   Valid targets: {len(valid_targets)}/{len(df_with_labels)}\")\n",
        "\n",
        "                    if len(valid_targets) >= 50:\n",
        "                        print(\" Sufficient data for regression training!\")\n",
        "\n",
        "                        # Proceed with training\n",
        "                        print(\"\\n--- Starting Model Training ---\")\n",
        "                        training_result = orchestrator.model_trainer.train_model(df_with_labels)\n",
        "\n",
        "                        if training_result and training_result.get('success', False):\n",
        "                            print(\" SUCCESS! Regression model trained successfully!\")\n",
        "                            if 'best_score' in training_result:\n",
        "                                print(f\" Model Score (R): {training_result['best_score']:.4f}\")\n",
        "                            if 'model_path' in training_result:\n",
        "                                print(f\" Model saved: {training_result['model_path']}\")\n",
        "                        else:\n",
        "                            print(\" Model training failed\")\n",
        "                    else:\n",
        "                        print(\" Still insufficient valid targets for training\")\n",
        "                else:\n",
        "                    print(\" No target column generated\")\n",
        "            else:\n",
        "                print(\" Label generation failed\")\n",
        "        else:\n",
        "            print(\" Feature generation failed\")\n",
        "    else:\n",
        "        print(\" Data preparation failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error in regression workflow: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\n=== SUMMARY ===\")\n",
        "print(\" Configuration fixed and switched to regression mode\")\n",
        "print(\" This approach predicts actual future returns instead of binary up/down\")\n",
        "print(\" Much more robust for fine-grained price data like 1-minute BTC\")\n",
        "print(\"\\nIf successful, you now have a working regression model for BTC price prediction! \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlAAQS--rSwa",
        "outputId": "b7b2b706-ef7c-4e77-d733-a9d74a89568e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== COMPREHENSIVE FIX FOR LABEL GENERATION ===\n",
            "1.  ANALYZING ACTUAL PRICE MOVEMENTS...\n",
            " Loaded data: (999, 41)\n",
            " PRICE MOVEMENT ANALYSIS:\n",
            "   Mean 1-min return: 0.000706%\n",
            "   Std 1-min return: 0.028240%\n",
            "   Max gain: 0.210676%\n",
            "   Max loss: -0.157936%\n",
            "\n",
            " TARGET FEASIBILITY ANALYSIS:\n",
            "   0.1%: 5 profits (0.5%), 6 losses (0.6%)\n",
            "   0.05%: 35 profits (3.5%), 25 losses (2.5%)\n",
            "   0.02%: 189 profits (18.9%), 174 losses (17.4%)\n",
            "   0.01%: 315 profits (31.6%), 301 losses (30.2%)\n",
            "\n",
            " DIAGNOSIS: Some moves are large enough for classification\n",
            "\n",
            "=== 2.  SWITCHING TO REGRESSION MODE ===\n",
            " Regression configuration set:\n",
            "   Model type: regression\n",
            "   Labeling method: future_return\n",
            "   Prediction horizon: 5 periods\n",
            "   Min training samples: 50\n",
            "\n",
            "=== 3.  RE-RUNNING WORKFLOW WITH REGRESSION MODE ===\n",
            "FATAL (Orchestrator): CCXT library module not available. Cannot initialize exchange.\n",
            "Warning (Orchestrator): Exchange not initialized. Some components may not function.\n",
            "AdvancedRiskManager initialized.\n",
            "FeatureEngineer initialized (Phase 1 Update).\n",
            "LabelGenerator initialized (Phase 1 Update). Using method: future_return\n",
            "ModelTrainer initialized (Phase 1 Update).\n",
            "ModelPredictor initialized.\n",
            "StrategyBacktester initialized.\n",
            "Visualizer initialized.\n",
            "Warning (Orchestrator Init): DataHandler failed to initialize.\n",
            "TradingBotOrchestrator initialized (Phase 1 Update).\n",
            "--- Starting Regression Workflow ---\n",
            "--- Starting Data Preparation ---\n",
            " Error in regression workflow: 'NoneType' object has no attribute 'prepare_data'\n",
            "\n",
            "=== SUMMARY ===\n",
            " Configuration fixed and switched to regression mode\n",
            " This approach predicts actual future returns instead of binary up/down\n",
            " Much more robust for fine-grained price data like 1-minute BTC\n",
            "\n",
            "If successful, you now have a working regression model for BTC price prediction! \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-14-7c8666724972>\", line 97, in <cell line: 0>\n",
            "    df_prepared = orchestrator.data_handler.prepare_data()\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'prepare_data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL CONFIG FIX - Run this to fix your triple_barrier configuration\n",
        "\n",
        "print(\"=== FIXING TRIPLE BARRIER CONFIGURATION ===\")\n",
        "\n",
        "# Remove scattered triple_barrier parameters (they're causing confusion)\n",
        "old_params_to_remove = [\n",
        "    'triple_barrier_profit_target_atr_mult',\n",
        "    'triple_barrier_stop_loss_atr_mult',\n",
        "    'triple_barrier_time_horizon_bars',\n",
        "    'triple_barrier_atr_column'\n",
        "]\n",
        "\n",
        "for param in old_params_to_remove:\n",
        "    if param in config:\n",
        "        print(f\"Removing old scattered parameter: {param} = {config[param]}\")\n",
        "        del config[param]\n",
        "\n",
        "# Add the proper triple_barrier section with realistic 1-minute BTC parameters\n",
        "config['triple_barrier'] = {\n",
        "    # Much smaller targets for 1-minute data\n",
        "    'profit_target_pct': 0.08,    # 0.08% profit (8 basis points) - achievable on 1m BTC\n",
        "    'loss_target_pct': -0.08,     # 0.08% stop loss (8 basis points)\n",
        "    'max_holding_periods': 20,    # Hold up to 20 minutes\n",
        "\n",
        "    # Alternative: ATR-based approach (but with tiny multipliers)\n",
        "    'use_atr_targets': False,      # Use percentage targets instead of ATR\n",
        "    'profit_target_atr_mult': 0.3, # IF using ATR: 0.3x ATR (much smaller)\n",
        "    'stop_loss_atr_mult': 0.3,     # IF using ATR: 0.3x ATR\n",
        "    'atr_column': 'atr',\n",
        "\n",
        "    # Meta-labeling options\n",
        "    'enable_meta_labeling': False,\n",
        "    'primary_model_threshold': 0.55\n",
        "}\n",
        "\n",
        "# Also ensure we have proper model type settings\n",
        "config['model_type'] = 'classification'\n",
        "config['labeling_method'] = 'triple_barrier'\n",
        "\n",
        "# Reduce minimum training samples since we might have fewer valid samples\n",
        "config['min_training_samples'] = 50  # Reduced from 100\n",
        "\n",
        "print(\"\\n NEW TRIPLE BARRIER CONFIGURATION:\")\n",
        "for key, value in config['triple_barrier'].items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "print(f\"\\n TARGET ANALYSIS:\")\n",
        "print(f\"   Profit Target: +{config['triple_barrier']['profit_target_pct']}%\")\n",
        "print(f\"   Loss Target: {config['triple_barrier']['loss_target_pct']}%\")\n",
        "print(f\"   Max Hold: {config['triple_barrier']['max_holding_periods']} periods\")\n",
        "print(f\"   Model Type: {config['model_type']}\")\n",
        "\n",
        "print(f\"\\n RE-RUNNING WORKFLOW...\")\n",
        "\n",
        "# Create new orchestrator with fixed config\n",
        "orchestrator = TradingBotOrchestrator(config)\n",
        "\n",
        "# Run the complete workflow\n",
        "result = orchestrator.execute_standard_workflow(\n",
        "    input_df=None,\n",
        "    force_retrain=True,\n",
        "    skip_backtest=False,\n",
        "    skip_live_sim=True\n",
        ")\n",
        "\n",
        "print(f\"\\n=== RESULTS ===\")\n",
        "if result and result.get('success', False):\n",
        "    print(\" SUCCESS! Fixed configuration resolved the triple_barrier issue!\")\n",
        "\n",
        "    # Show label distribution\n",
        "    if 'df_with_labels' in result and result['df_with_labels'] is not None:\n",
        "        df_labels = result['df_with_labels']\n",
        "        if 'target' in df_labels.columns:\n",
        "            label_dist = df_labels['target'].value_counts()\n",
        "            total = len(df_labels)\n",
        "            print(f\"\\n FINAL LABEL DISTRIBUTION:\")\n",
        "            for label, count in label_dist.items():\n",
        "                pct = (count/total)*100\n",
        "                print(f\"   Class {label}: {count:,} samples ({pct:.1f}%)\")\n",
        "\n",
        "            if len(label_dist) > 1:\n",
        "                print(\"\\n SUCCESS: Balanced labels achieved!\")\n",
        "                print(\"   Your model can now learn from both profitable and unprofitable trades\")\n",
        "            else:\n",
        "                print(\"\\n Still imbalanced - trying even smaller targets...\")\n",
        "\n",
        "                # Emergency fallback: Ultra-small targets\n",
        "                config['triple_barrier']['profit_target_pct'] = 0.03  # 3 basis points\n",
        "                config['triple_barrier']['loss_target_pct'] = -0.03\n",
        "                print(f\"   Trying ultra-small targets: 0.03%\")\n",
        "\n",
        "    # Show training info\n",
        "    if 'training_result' in result and result['training_result']:\n",
        "        tr = result['training_result']\n",
        "        if 'best_score' in tr:\n",
        "            print(f\" Model Score: {tr['best_score']:.4f}\")\n",
        "        print(f\" Model trained and ready for use!\")\n",
        "\n",
        "else:\n",
        "    print(\" Still failed - switching to regression mode as final fallback...\")\n",
        "\n",
        "    config['model_type'] = 'regression'\n",
        "    config['labeling_method'] = 'future_return'\n",
        "\n",
        "    print(\" Trying regression mode...\")\n",
        "    orchestrator_reg = TradingBotOrchestrator(config)\n",
        "    result_reg = orchestrator_reg.execute_standard_workflow(\n",
        "        input_df=None,\n",
        "        force_retrain=True,\n",
        "        skip_backtest=False,\n",
        "        skip_live_sim=True\n",
        "    )\n",
        "\n",
        "    if result_reg and result_reg.get('success', False):\n",
        "        print(\" SUCCESS with regression mode!\")\n",
        "    else:\n",
        "        print(\" Final failure - check data quality\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"CONFIGURATION FIX COMPLETE\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "V6kfJ19ltgA6",
        "outputId": "9940e630-33bf-4606-afae-f75833ec880e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FIXING TRIPLE BARRIER CONFIGURATION ===\n",
            "Removing old scattered parameter: triple_barrier_profit_target_atr_mult = 0.8\n",
            "Removing old scattered parameter: triple_barrier_stop_loss_atr_mult = 0.5\n",
            "Removing old scattered parameter: triple_barrier_time_horizon_bars = 20\n",
            "Removing old scattered parameter: triple_barrier_atr_column = atr\n",
            "\n",
            " NEW TRIPLE BARRIER CONFIGURATION:\n",
            "   profit_target_pct: 0.08\n",
            "   loss_target_pct: -0.08\n",
            "   max_holding_periods: 20\n",
            "   use_atr_targets: False\n",
            "   profit_target_atr_mult: 0.3\n",
            "   stop_loss_atr_mult: 0.3\n",
            "   atr_column: atr\n",
            "   enable_meta_labeling: False\n",
            "   primary_model_threshold: 0.55\n",
            "\n",
            " TARGET ANALYSIS:\n",
            "   Profit Target: +0.08%\n",
            "   Loss Target: -0.08%\n",
            "   Max Hold: 20 periods\n",
            "   Model Type: classification\n",
            "\n",
            " RE-RUNNING WORKFLOW...\n",
            "FATAL (Orchestrator): CCXT library module not available. Cannot initialize exchange.\n",
            "Warning (Orchestrator): Exchange not initialized. Some components may not function.\n",
            "AdvancedRiskManager initialized.\n",
            "FeatureEngineer initialized (Phase 1 Update).\n",
            "LabelGenerator initialized (Phase 1 Update). Using method: triple_barrier\n",
            "ModelTrainer initialized (Phase 1 Update).\n",
            "ModelPredictor initialized.\n",
            "StrategyBacktester initialized.\n",
            "Visualizer initialized.\n",
            "Warning (Orchestrator Init): DataHandler failed to initialize.\n",
            "TradingBotOrchestrator initialized (Phase 1 Update).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TradingBotOrchestrator' object has no attribute 'execute_standard_workflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c264a3427a22>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Run the complete workflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m result = orchestrator.execute_standard_workflow(\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0minput_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mforce_retrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TradingBotOrchestrator' object has no attribute 'execute_standard_workflow'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}