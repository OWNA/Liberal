{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OWNA/Liberal/blob/main/liberal2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP8Fz9RjYYIz"
      },
      "source": [
        "## 1. Setup Environment\n",
        "\n",
        "This cell installs the required Python libraries and mounts Google Drive to persist data, models, and logs across sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFg3MFcqYYI1",
        "outputId": "fbc485fd-20f7-47fd-b467-69ebd80e9134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLibraries installation attempt finished.\n",
            "\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Using Google Drive base directory: /content/drive/MyDrive/trading_bot_project_v2\n",
            "env: BOT_BASE_DIR=/content/drive/MyDrive/trading_bot_project_v2\n"
          ]
        }
      ],
      "source": [
        "import os # Import os here for makedirs\n",
        "\n",
        "# Install necessary libraries\n",
        "# Using -q for quieter installation, remove it for verbose output\n",
        "print(\"Installing required libraries...\")\n",
        "# Main Bot Libraries\n",
        "!pip install ccxt lightgbm scikit-learn pandas matplotlib scipy numpy -q\n",
        "!pip install optuna shap EMD-signal PyYAML -q\n",
        "# L2 Collector Library\n",
        "!pip install websocket-client -q\n",
        "print(\"Libraries installation attempt finished.\")\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"\\nMounting Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True) # force_remount can be useful\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "    # Define a base directory on Google Drive (adjust as needed)\n",
        "    # NOTE: This path is also set in the config file below. Ensure they match or load from config.\n",
        "    DRIVE_BASE_DIR = '/content/drive/MyDrive/trading_bot_project_v2'\n",
        "    os.makedirs(DRIVE_BASE_DIR, exist_ok=True)\n",
        "    print(f\"Using Google Drive base directory: {DRIVE_BASE_DIR}\")\n",
        "except ImportError:\n",
        "    print(\"Google Drive mounting failed (not in Colab environment?). Using local directory.\")\n",
        "    DRIVE_BASE_DIR = \"./trading_bot_project_v2_local\" # Fallback local directory\n",
        "    os.makedirs(DRIVE_BASE_DIR, exist_ok=True)\n",
        "    print(f\"Using local base directory: {DRIVE_BASE_DIR}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Google Drive mounting: {e}\")\n",
        "    DRIVE_BASE_DIR = \"./trading_bot_project_v2_local\" # Fallback local directory\n",
        "    os.makedirs(DRIVE_BASE_DIR, exist_ok=True)\n",
        "    print(f\"Using local base directory due to error: {DRIVE_BASE_DIR}\")\n",
        "\n",
        "# Store the determined base directory as an environment variable for easy access\n",
        "%env BOT_BASE_DIR={DRIVE_BASE_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dek20ANrYYI2"
      },
      "source": [
        "## 2. Configuration Settings\n",
        "\n",
        "Define all hyperparameters, paths, and settings for the trading bot run in a YAML configuration block. This makes it easy to manage experiments. The configuration is saved to `config.yaml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avX1_UCvYYI2",
        "outputId": "4a77bbb5-6e90-47d8-cb25-e990128090de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving configuration to: /content/drive/MyDrive/trading_bot_project_v2/config.yaml\n",
            "Configuration saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Get the base directory determined in the previous cell\n",
        "# If the environment variable wasn't set, default to a local path\n",
        "bot_base_dir = os.environ.get('BOT_BASE_DIR', './trading_bot_project_v2_local')\n",
        "config_path = os.path.join(bot_base_dir, 'config.yaml')\n",
        "\n",
        "# Define configuration as a Python dictionary\n",
        "config = {\n",
        "    # --- General Settings ---\n",
        "    \"base_dir\": bot_base_dir, # Use the path determined during Drive mount\n",
        "    \"symbol\": \"BTC/USDT\",\n",
        "    \"timeframe\": \"1m\", # Changed back to 5m as 1m caused issues\n",
        "\n",
        "    # --- Feature Engineering ---\n",
        "    \"feature_window\": 24,\n",
        "    \"use_l2_features\": True, # *** Enable L2 features (primarily for simulation loop) ***\n",
        "    \"l2_depth_levels\": 25, # Number of levels to use for L2 calculations (e.g., OBI, depth ratio)\n",
        "\n",
        "    # --- Data Fetching ---\n",
        "    \"fetch_ohlcv_limit\": 2500, # Initial historical data fetch\n",
        "    \"l2_depth\": 25, # Depth for fetching L2 order book (used in simulation)\n",
        "\n",
        "    # --- Model Training ---\n",
        "    \"optuna_trials\": 100,\n",
        "    \"test_size\": 0.2, # Train/validation split for Optuna\n",
        "\n",
        "    # --- Backtesting ---\n",
        "    \"backtest_threshold\": 0.2, # Signal threshold (adjust based on results)\n",
        "    \"initial_balance\": 10000,\n",
        "    \"commission_pct\": 0.0006,\n",
        "    \"leverage\": 3,\n",
        "    \"stop_loss_pct\": None, # Optional: e.g., 0.02 for 2%\n",
        "    \"take_profit_pct\": None, # Optional: e.g., 0.04 for 4%\n",
        "\n",
        "    # --- Live Simulation ---\n",
        "    \"run_simulation_flag\": False, # Set to True to run simulation\n",
        "    \"simulation_threshold\": 0.2, # Signal threshold for simulation\n",
        "    \"fetch_live_limit\": 300, # Candles to fetch in sim loop\n",
        "    \"simulation_duration_seconds\": 300, # How long to run sim in example\n",
        "\n",
        "    # --- Analysis ---\n",
        "    \"use_shap_override\": True, # Attempt to use SHAP if available\n",
        "\n",
        "    # --- L2 Collector Specific Settings (Can be overridden in Collector Cell) ---\n",
        "    \"collector_symbol\": \"BTCUSDT\", # Note: CCXT uses '/', Bybit WS uses ''\n",
        "    \"collector_duration\": 5,\n",
        "    \"collector_unit\": \"minutes\",\n",
        "    \"collector_depth\": 50,\n",
        "    \"collector_category\": \"linear\"\n",
        "}\n",
        "\n",
        "# Save the configuration to a YAML file\n",
        "print(f\"Saving configuration to: {config_path}\")\n",
        "try:\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
        "    print(\"Configuration saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving configuration file: {e}\")\n",
        "\n",
        "# Optional: Print the configuration to verify\n",
        "# print(\"\\n--- Configuration ---\")\n",
        "# print(yaml.dump(config, default_flow_style=False))\n",
        "# print(\"---------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTEa0SO1YYI2"
      },
      "source": [
        "## 3. L2 Data Collector\n",
        "\n",
        "This cell runs the Bybit Level 2 order book data collector using WebSockets. It saves the data snapshots to a `.jsonl` file. You can run this cell independently to collect data for a specified duration.\n",
        "\n",
        "**Note:** The main trading bot (`CombinedTradingBot`) in this notebook primarily uses L2 features fetched via REST API during the *live simulation*. It does **not** automatically load or use the historical L2 data collected by this cell for backtesting or training unless you manually implement the logic to read the `.jsonl` file and merge it with OHLCV data *before* the `prepare_features` step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka5NIR7CYYI2",
        "outputId": "02382b78-24cd-44a9-b8d4-39929bc5e76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded collector settings from config.yaml\n",
            "--- Collector Configuration ---\n",
            "Symbol:   BTCUSDT\n",
            "Duration: 5 minutes\n",
            "Depth:    50\n",
            "Category: linear\n",
            "Output File: /content/drive/MyDrive/trading_bot_project_v2/l2_data/btcusdt_l2_data_5m.jsonl\n",
            "-----------------------------\n",
            "Opened output file for writing: /content/drive/MyDrive/trading_bot_project_v2/l2_data/btcusdt_l2_data_5m.jsonl\n",
            "Connecting to WebSocket: wss://stream.bybit.com/v5/public/linear\n",
            "WebSocket thread started.\n",
            "Collecting data for 5 minutes...\n",
            "Target end time: 2025-05-11T08:43:45.800398+00:00\n",
            "WebSocket Connection Opened\n",
            "Sent subscription request for: orderbook.50.BTCUSDT\n",
            "[2025-05-11T08:38:47+00:00] Received initial snapshot (Update ID: 97236613)\n",
            "Successfully subscribed to: \n",
            "Time remaining: 0:00:00 | Snapshots saved: 9079   \n",
            "Initiating shutdown...\n",
            "Signalling WebSocket connection to close...\n",
            "\n",
            "WebSocket Closed: Code=None, Msg=None\n",
            "Closing output file: /content/drive/MyDrive/trading_bot_project_v2/l2_data/btcusdt_l2_data_5m.jsonl (Snapshots saved: 9079)\n",
            "Data collection script finished. Total snapshots saved: 9079\n"
          ]
        }
      ],
      "source": [
        "# === Requirements ===\n",
        "# Make sure websocket-client is installed in your Colab environment (should be done in Cell 1)\n",
        "# !pip install websocket-client -q\n",
        "\n",
        "import websocket # Requires: pip install websocket-client\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "import traceback # Added for better error detail\n",
        "from datetime import datetime, timezone, timedelta\n",
        "# import signal # Signal handling might be less reliable in Colab, rely on KeyboardInterrupt\n",
        "import sys\n",
        "import os\n",
        "import yaml # To load base dir from config if needed\n",
        "\n",
        "# ==============================================================================\n",
        "#                           CONFIGURATION (Collector)\n",
        "# ==============================================================================\n",
        "# --- Set your desired parameters here before running the cell ---\n",
        "# --- These override the defaults set in the main config.yaml if needed ---\n",
        "\n",
        "# Load base directory from environment variable set in Cell 1\n",
        "COLLECTOR_BASE_DIR = os.environ.get('BOT_BASE_DIR', './trading_bot_project_v2_local')\n",
        "\n",
        "# Try loading collector settings from config.yaml, use defaults if not found\n",
        "config_file_path_collector = os.path.join(COLLECTOR_BASE_DIR, 'config.yaml')\n",
        "try:\n",
        "    with open(config_file_path_collector, 'r') as f:\n",
        "        main_config = yaml.safe_load(f)\n",
        "    COLLECTOR_SYMBOL = main_config.get('collector_symbol', \"BTCUSDT\")\n",
        "    COLLECTOR_DURATION = main_config.get('collector_duration', 5)\n",
        "    COLLECTOR_UNIT = main_config.get('collector_unit', \"minutes\")\n",
        "    COLLECTOR_DEPTH = main_config.get('collector_depth', 50)\n",
        "    COLLECTOR_CATEGORY = main_config.get('collector_category', \"linear\")\n",
        "    print(\"Loaded collector settings from config.yaml\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not load collector settings from config.yaml ({e}). Using defaults.\")\n",
        "    COLLECTOR_SYMBOL = \"BTCUSDT\"\n",
        "    COLLECTOR_DURATION = 5\n",
        "    COLLECTOR_UNIT = \"minutes\"\n",
        "    COLLECTOR_DEPTH = 50\n",
        "    COLLECTOR_CATEGORY = \"linear\"\n",
        "\n",
        "# Construct the output file path within a 'l2_data' subdirectory\n",
        "L2_DATA_SUBDIR = os.path.join(COLLECTOR_BASE_DIR, 'l2_data')\n",
        "FILENAME = f\"{COLLECTOR_SYMBOL.lower()}_l2_data_{COLLECTOR_DURATION}{COLLECTOR_UNIT[0]}.jsonl\" # e.g., btcusdt_l2_data_5m.jsonl\n",
        "COLLECTOR_OUTPUT_FILE = os.path.join(L2_DATA_SUBDIR, FILENAME)\n",
        "\n",
        "# ==============================================================================\n",
        "print(f\"--- Collector Configuration ---\")\n",
        "print(f\"Symbol:   {COLLECTOR_SYMBOL}\")\n",
        "print(f\"Duration: {COLLECTOR_DURATION} {COLLECTOR_UNIT}\")\n",
        "print(f\"Depth:    {COLLECTOR_DEPTH}\")\n",
        "print(f\"Category: {COLLECTOR_CATEGORY}\")\n",
        "print(f\"Output File: {COLLECTOR_OUTPUT_FILE}\")\n",
        "print(f\"-----------------------------\")\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# --- Global Variables (Collector Specific) ---\n",
        "collector_ws_connection = None\n",
        "collector_output_file_handle = None\n",
        "collector_snapshot_data = {\"bids\": {}, \"asks\": {}}\n",
        "collector_last_update_id = 0\n",
        "collector_ws_thread_object = None\n",
        "collector_lock = threading.Lock()\n",
        "collector_write_counter = 0\n",
        "collector_main_loop_running = True # Flag for main loop control\n",
        "\n",
        "# --- WebSocket Event Handlers (Collector Specific) ---\n",
        "\n",
        "def collector_on_message(ws, message):\n",
        "    \"\"\"Handles incoming WebSocket messages for the collector.\"\"\"\n",
        "    global collector_snapshot_data, collector_last_update_id, collector_output_file_handle, collector_lock, collector_write_counter\n",
        "    # No need to check main_loop_running here, rely on WS closure and thread join\n",
        "\n",
        "    try:\n",
        "        data = json.loads(message)\n",
        "        # Handle ping/pong\n",
        "        if \"op\" in data and data.get(\"op\") == \"ping\":\n",
        "            ws.send(json.dumps({\"op\": \"pong\", \"req_id\": data.get(\"req_id\")}))\n",
        "            return\n",
        "        # Handle subscription confirmation\n",
        "        if \"op\" in data and data.get(\"op\") == \"subscribe\":\n",
        "            if data.get(\"success\"):\n",
        "                print(f\"Successfully subscribed to: {data.get('ret_msg')}\")\n",
        "            else:\n",
        "                print(f\"Subscription failed: {data.get('ret_msg')}\")\n",
        "            return\n",
        "\n",
        "        # Process order book data\n",
        "        if \"topic\" in data and \"orderbook\" in data[\"topic\"]:\n",
        "            data_content = data.get(\"data\", {})\n",
        "            update_id = data_content.get(\"u\")\n",
        "            timestamp_ms = data.get(\"ts\") # Timestamp from Bybit message\n",
        "            if not update_id or not timestamp_ms:\n",
        "                # print(f\"Skipping message, missing update_id or timestamp_ms: {data}\") # Optional debug\n",
        "                return # Skip if essential data missing\n",
        "\n",
        "            with collector_lock: # Lock before accessing/modifying shared snapshot_data\n",
        "                if data.get(\"type\") == \"snapshot\":\n",
        "                    print(f\"[{datetime.now(timezone.utc).isoformat(timespec='seconds')}] Received initial snapshot (Update ID: {update_id})\")\n",
        "                    collector_snapshot_data[\"bids\"] = {price: vol for price, vol in data_content.get(\"b\", [])}\n",
        "                    collector_snapshot_data[\"asks\"] = {price: vol for price, vol in data_content.get(\"a\", [])}\n",
        "                    collector_last_update_id = update_id\n",
        "                    collector_save_snapshot_locked(timestamp_ms) # Call locked version\n",
        "\n",
        "                elif data.get(\"type\") == \"delta\":\n",
        "                    # Apply updates only if the update ID is newer than the last processed one\n",
        "                    if update_id > collector_last_update_id:\n",
        "                        # Apply bid updates\n",
        "                        for price, vol in data_content.get(\"b\", []):\n",
        "                            if float(vol) == 0: # Remove level\n",
        "                                collector_snapshot_data[\"bids\"].pop(price, None)\n",
        "                            else: # Add/Update level\n",
        "                                collector_snapshot_data[\"bids\"][price] = vol\n",
        "                        # Apply ask updates\n",
        "                        for price, vol in data_content.get(\"a\", []):\n",
        "                            if float(vol) == 0: # Remove level\n",
        "                                collector_snapshot_data[\"asks\"].pop(price, None)\n",
        "                            else: # Add/Update level\n",
        "                                collector_snapshot_data[\"asks\"][price] = vol\n",
        "                        collector_last_update_id = update_id\n",
        "                        collector_save_snapshot_locked(timestamp_ms) # Call locked version\n",
        "                    # else:\n",
        "                        # print(f\"Ignoring stale update (Last: {collector_last_update_id}, Current: {update_id})\") # Optional debug\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON: {message}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing message: {e}\\n{message}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "def collector_save_snapshot_locked(bybit_timestamp_ms):\n",
        "    \"\"\"Saves snapshot. Assumes collector_lock is already held.\"\"\"\n",
        "    global collector_snapshot_data, collector_output_file_handle, collector_write_counter, collector_last_update_id\n",
        "    if collector_output_file_handle and not collector_output_file_handle.closed:\n",
        "        try:\n",
        "            # Convert dicts back to sorted lists [price, volume] for saving\n",
        "            # Sort bids descending, asks ascending by price (numeric conversion needed)\n",
        "            bids_list = sorted([[p, v] for p, v in collector_snapshot_data[\"bids\"].items()], key=lambda x: float(x[0]), reverse=True)\n",
        "            asks_list = sorted([[p, v] for p, v in collector_snapshot_data[\"asks\"].items()], key=lambda x: float(x[0]))\n",
        "\n",
        "            snapshot_to_save = {\n",
        "                \"timestamp_ms\": bybit_timestamp_ms, # Use Bybit's timestamp\n",
        "                \"local_timestamp_iso\": datetime.now(timezone.utc).isoformat(), # Add local timestamp for reference\n",
        "                \"b\": bids_list,\n",
        "                \"a\": asks_list,\n",
        "                \"last_update_id\": collector_last_update_id\n",
        "            }\n",
        "\n",
        "            collector_output_file_handle.write(json.dumps(snapshot_to_save) + '\\n')\n",
        "            collector_output_file_handle.flush() # Ensure data is written promptly\n",
        "            collector_write_counter += 1\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError writing to file: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "def collector_on_error(ws, error):\n",
        "    \"\"\"Handles WebSocket errors for the collector.\"\"\"\n",
        "    print(f\"\\nWebSocket Error: {error}\")\n",
        "    global collector_main_loop_running\n",
        "    collector_main_loop_running = False # Signal main loop to stop\n",
        "\n",
        "def collector_on_close(ws, close_status_code, close_msg):\n",
        "    \"\"\"Handles WebSocket connection close for the collector.\"\"\"\n",
        "    print(f\"\\nWebSocket Closed: Code={close_status_code}, Msg={close_msg}\")\n",
        "    global collector_main_loop_running\n",
        "    collector_main_loop_running = False # Ensure main loop stops\n",
        "\n",
        "def collector_on_open(ws, symbol, depth):\n",
        "    \"\"\"Sends subscription message when connection opens for the collector.\"\"\"\n",
        "    print(\"WebSocket Connection Opened\")\n",
        "    # Construct the subscription topic string (remove slashes for WebSocket topic)\n",
        "    clean_symbol = symbol.replace(\"/\", \"\")\n",
        "    topic = f\"orderbook.{depth}.{clean_symbol}\"\n",
        "    subscribe_message = {\"op\": \"subscribe\", \"args\": [topic]}\n",
        "    try:\n",
        "        ws.send(json.dumps(subscribe_message))\n",
        "        print(f\"Sent subscription request for: {topic}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error sending subscribe message: {e}\")\n",
        "        ws.close() # Close WS if subscribe fails\n",
        "\n",
        "# --- Main Execution Logic (Collector) ---\n",
        "def run_websocket_collector(symbol, depth, category):\n",
        "    \"\"\"Starts the WebSocket connection for the collector.\"\"\"\n",
        "    global collector_ws_connection, collector_ws_thread_object\n",
        "    # Determine endpoint based on category\n",
        "    if category == 'spot':\n",
        "        ws_url = \"wss://stream.bybit.com/v5/public/spot\"\n",
        "    elif category == 'inverse':\n",
        "         ws_url = \"wss://stream.bybit.com/v5/public/inverse\"\n",
        "    else: # Default to linear\n",
        "        ws_url = \"wss://stream.bybit.com/v5/public/linear\"\n",
        "\n",
        "    print(f\"Connecting to WebSocket: {ws_url}\")\n",
        "    # Note: Pass symbol and depth to on_open using lambda\n",
        "    collector_ws_connection = websocket.WebSocketApp(ws_url,\n",
        "                                  on_open=lambda ws: collector_on_open(ws, symbol, depth),\n",
        "                                  on_message=collector_on_message,\n",
        "                                  on_error=collector_on_error,\n",
        "                                  on_close=collector_on_close)\n",
        "\n",
        "    # Run in a separate thread to allow main thread to handle timing/shutdown\n",
        "    # Use ping_interval and ping_timeout for keepalive\n",
        "    collector_ws_thread_object = threading.Thread(target=lambda: collector_ws_connection.run_forever(ping_interval=20, ping_timeout=10))\n",
        "    collector_ws_thread_object.daemon = True # Allow main thread to exit even if this thread is running\n",
        "    collector_ws_thread_object.start()\n",
        "    print(\"WebSocket thread started.\")\n",
        "\n",
        "# --- Colab Execution Block (Collector) ---\n",
        "duration_seconds = COLLECTOR_DURATION * 60 if COLLECTOR_UNIT == 'minutes' else COLLECTOR_DURATION * 3600\n",
        "start_time = time.time()\n",
        "end_time = start_time + duration_seconds\n",
        "\n",
        "# Reset global state variables for the collector before each run\n",
        "collector_ws_connection = None\n",
        "collector_output_file_handle = None\n",
        "collector_snapshot_data = {\"bids\": {}, \"asks\": {}}\n",
        "collector_last_update_id = 0\n",
        "collector_ws_thread_object = None\n",
        "collector_write_counter = 0\n",
        "collector_main_loop_running = True # Reset flag\n",
        "\n",
        "try:\n",
        "    # Ensure output directory exists\n",
        "    output_dir = os.path.dirname(COLLECTOR_OUTPUT_FILE)\n",
        "    if output_dir and not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "    # Open output file\n",
        "    collector_output_file_handle = open(COLLECTOR_OUTPUT_FILE, 'w')\n",
        "    print(f\"Opened output file for writing: {COLLECTOR_OUTPUT_FILE}\")\n",
        "\n",
        "    # Start WebSocket\n",
        "    run_websocket_collector(COLLECTOR_SYMBOL, COLLECTOR_DEPTH, COLLECTOR_CATEGORY)\n",
        "\n",
        "    # Main loop to check duration\n",
        "    print(f\"Collecting data for {COLLECTOR_DURATION} {COLLECTOR_UNIT}...\")\n",
        "    print(f\"Target end time: {datetime.fromtimestamp(end_time, timezone.utc).isoformat()}\")\n",
        "\n",
        "    while collector_main_loop_running and time.time() < end_time:\n",
        "        # Check if WebSocket thread is still alive\n",
        "        if collector_ws_thread_object and not collector_ws_thread_object.is_alive():\n",
        "            print(\"\\nWebSocket thread stopped unexpectedly.\")\n",
        "            collector_main_loop_running = False # Stop main loop\n",
        "            break\n",
        "\n",
        "        # Keep the main thread alive, WebSocket runs in background\n",
        "        time.sleep(1.0) # Check every second\n",
        "\n",
        "        # Optional: Print progress (only if loop is still supposed to be running)\n",
        "        if collector_main_loop_running:\n",
        "            remaining_time = max(0, int(end_time - time.time()))\n",
        "            print(f\"Time remaining: {timedelta(seconds=remaining_time)} | Snapshots saved: {collector_write_counter}   \", end='\\r')\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nKeyboard interrupt detected.\")\n",
        "    collector_main_loop_running = False # Signal shutdown\n",
        "\n",
        "finally:\n",
        "    print(\"\\nInitiating shutdown...\")\n",
        "\n",
        "    # 1. Signal WebSocket to close\n",
        "    if collector_ws_connection:\n",
        "        print(\"Signalling WebSocket connection to close...\")\n",
        "        try:\n",
        "            collector_ws_connection.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error signalling WebSocket close: {e}\")\n",
        "\n",
        "    # 2. Wait for WebSocket thread to finish\n",
        "    if collector_ws_thread_object and collector_ws_thread_object.is_alive():\n",
        "        print(\"Waiting for WebSocket thread to join...\")\n",
        "        collector_ws_thread_object.join(timeout=10.0) # Wait up to 10 seconds\n",
        "        if collector_ws_thread_object.is_alive():\n",
        "            print(\"Warning: WebSocket thread did not join within timeout.\")\n",
        "        else:\n",
        "            print(\"WebSocket thread joined.\")\n",
        "\n",
        "    # 3. Close the file handle (now that WS thread should be stopped)\n",
        "    # Use lock just in case, though WS thread should be dead\n",
        "    with collector_lock:\n",
        "        if collector_output_file_handle and not collector_output_file_handle.closed:\n",
        "            print(f\"Closing output file: {collector_output_file_handle.name} (Snapshots saved: {collector_write_counter})\")\n",
        "            try:\n",
        "                collector_output_file_handle.close()\n",
        "            except Exception as e:\n",
        "                print(f\"Error closing file handle: {e}\")\n",
        "        collector_output_file_handle = None # Ensure handle is cleared\n",
        "\n",
        "    print(f\"Data collection script finished. Total snapshots saved: {collector_write_counter}\")\n",
        "    # Final check for resource cleanup\n",
        "    if collector_output_file_handle:\n",
        "        print(\"Warning: Output file handle may not be None after shutdown.\")\n",
        "    if collector_ws_thread_object and collector_ws_thread_object.is_alive():\n",
        "        print(\"Warning: WebSocket thread may still be alive after shutdown.\")\n",
        "\n",
        "# --- End of Collector Execution Block ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yxyAsGVYYI3"
      },
      "source": [
        "## 4. Imports & Global Setup (Main Bot)\n",
        "\n",
        "Import libraries, load the configuration from `config.yaml`, set up API keys from Colab Secrets, and handle optional dependencies for the main trading bot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjEsfyCSYYI3",
        "outputId": "31c03daf-e642-4158-f54f-ba4cca07ed52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading configuration...\n",
            "Configuration loaded successfully.\n",
            "Optuna loaded.\n",
            "PyEMD (EMD-signal) library loaded successfully.\n",
            "SHAP library loaded successfully.\n",
            "Matplotlib loaded.\n",
            "Scipy (signal.hilbert) loaded.\n",
            "CCXT loaded.\n",
            "\n",
            "Loading API Keys from Colab Secrets...\n",
            "API Keys loaded successfully from Colab secrets.\n",
            "\n",
            "Imports and global setup complete.\n"
          ]
        }
      ],
      "source": [
        "# --- Core Libraries ---\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "import warnings\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import sys\n",
        "import yaml # For loading config\n",
        "\n",
        "# --- Data Handling & Numerics ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Machine Learning ---\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# --- Load Configuration ---\n",
        "print(\"Loading configuration...\")\n",
        "config_file_path = os.path.join(os.environ.get('BOT_BASE_DIR', './trading_bot_project_v2_local'), 'config.yaml')\n",
        "try:\n",
        "    with open(config_file_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    print(\"Configuration loaded successfully.\")\n",
        "    # You can access config values like config['symbol'], config['timeframe'] etc.\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Configuration file not found at {config_file_path}. Please run the config cell (Cell 2).\")\n",
        "    config = {} # Provide empty dict to avoid downstream errors\n",
        "except Exception as e:\n",
        "    print(f\"Error loading configuration: {e}\")\n",
        "    config = {}\n",
        "\n",
        "# --- Optional Libraries Setup ---\n",
        "\n",
        "# Optuna\n",
        "try:\n",
        "    import optuna\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "    print(\"Optuna loaded.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING: Optuna not found. Model training will fail.\")\n",
        "\n",
        "# PyEMD (EMD-signal)\n",
        "try:\n",
        "    from PyEMD import EMD\n",
        "    HAS_PYEMD = True\n",
        "    print(\"PyEMD (EMD-signal) library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Warning: PyEMD (EMD-signal) not found. HHT features disabled.\")\n",
        "    HAS_PYEMD = False\n",
        "    class EMD: # Dummy class\n",
        "        def __init__(self, *args, **kwargs): pass\n",
        "        def __call__(self, signal, *args, **kwargs): return np.empty((0, len(signal))) if isinstance(signal, np.ndarray) else None\n",
        "        def get_imfs_and_residue(self): return np.empty((0,0)), np.empty((0))\n",
        "\n",
        "# SHAP\n",
        "try:\n",
        "    import shap\n",
        "    HAS_SHAP = True\n",
        "    print(\"SHAP library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Warning: SHAP library not found. SHAP plots disabled.\")\n",
        "    HAS_SHAP = False\n",
        "\n",
        "# Matplotlib\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    print(\"Matplotlib loaded.\")\n",
        "except ImportError:\n",
        "    print(\"Warning: Matplotlib not found. Plotting disabled.\")\n",
        "    plt = None\n",
        "\n",
        "# Scipy (for Hilbert)\n",
        "try:\n",
        "    from scipy.signal import hilbert\n",
        "    print(\"Scipy (signal.hilbert) loaded.\")\n",
        "except ImportError:\n",
        "    print(\"ERROR: Scipy not found. Hilbert transform (needed for HHT) will fail.\")\n",
        "    # If HHT is critical, raise error. Otherwise, HAS_PYEMD check handles it.\n",
        "    # raise ImportError(\"Scipy is required but not installed.\")\n",
        "\n",
        "\n",
        "# --- Exchange Interaction ---\n",
        "try:\n",
        "    import ccxt\n",
        "    print(\"CCXT loaded.\")\n",
        "except ImportError:\n",
        "    print(\"ERROR: CCXT not found. Exchange interaction will fail.\")\n",
        "    raise ImportError(\"CCXT is required but not installed.\")\n",
        "\n",
        "\n",
        "# --- API Key Loading (Using Colab Secrets) ---\n",
        "print(\"\\nLoading API Keys from Colab Secrets...\")\n",
        "BYBIT_API_KEY = None\n",
        "BYBIT_API_SECRET = None\n",
        "try:\n",
        "    # Try importing the specific Colab module\n",
        "    from google.colab import userdata\n",
        "    # Fetch secrets using the correct names\n",
        "    BYBIT_API_KEY = userdata.get(\"BYBIT_API_KEY_MAIN\")\n",
        "    BYBIT_API_SECRET = userdata.get(\"BYBIT_API_SECRET_MAIN\")\n",
        "    if BYBIT_API_KEY and BYBIT_API_SECRET:\n",
        "        print(\"API Keys loaded successfully from Colab secrets.\")\n",
        "    else:\n",
        "        print(\"*** WARNING: API Keys not found in Colab secrets (or value is empty). ***\")\n",
        "        print(\"*** Check secret names ('BYBIT_API_KEY_MAIN', 'BYBIT_API_SECRET_MAIN') and values in Colab UI. ***\")\n",
        "        print(\"*** Proceeding with public access (may fail due to geo-blocking). ***\")\n",
        "        BYBIT_API_KEY = None # Ensure they are None if fetch failed\n",
        "        BYBIT_API_SECRET = None\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (won't load secrets)\n",
        "    print(\"*** WARNING: Not running in Colab environment. Cannot load Colab secrets. ***\")\n",
        "    print(\"*** Proceeding with public access (may fail due to geo-blocking). ***\")\n",
        "    BYBIT_API_KEY = None\n",
        "    BYBIT_API_SECRET = None\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred loading secrets: {e}\")\n",
        "    BYBIT_API_KEY = None\n",
        "    BYBIT_API_SECRET = None\n",
        "\n",
        "# --- Warnings Configuration ---\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "print(\"\\nImports and global setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhY1ZB1uYYI3"
      },
      "source": [
        "## 5. `CombinedTradingBot` Class Definition\n",
        "\n",
        "This cell defines the main class encapsulating the trading bot's logic, including data fetching, feature engineering (with PyEMD for HHT and L2 features), model training, prediction, backtesting, simulation, and visualization.\n",
        "\n",
        "*(Note: L2 features are calculated from snapshots fetched via REST API, primarily intended for the live simulation loop. Historical L2 data requires a separate data source and pre-processing, potentially using the `.jsonl` file generated by the L2 Collector cell.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-lZ4_GHYYI4"
      },
      "outputs": [],
      "source": [
        "# Ensure libraries loaded in the previous cell are available\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "import warnings\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import ccxt\n",
        "\n",
        "# These might be None if optional imports failed\n",
        "# import optuna\n",
        "# import shap\n",
        "# import matplotlib.pyplot as plt\n",
        "# from PyEMD import EMD # EMD might be the dummy class if import failed\n",
        "# from scipy.signal import hilbert\n",
        "\n",
        "class CombinedTradingBot:\n",
        "    \"\"\"\n",
        "    Trading bot using PyEMD for HHT features and L2 order book features,\n",
        "    driven by configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, api_key=None, api_secret=None):\n",
        "        \"\"\"\n",
        "        Initializes the bot instance using configuration dictionary.\n",
        "\n",
        "        Args:\n",
        "            config (dict): Dictionary containing configuration parameters.\n",
        "            api_key (str, optional): Bybit API Key.\n",
        "            api_secret (str, optional): Bybit API Secret.\n",
        "        \"\"\"\n",
        "        # Load parameters from config\n",
        "        self.config = config\n",
        "        self.symbol = config['symbol']\n",
        "        self.timeframe = config['timeframe']\n",
        "        self.base_dir = config['base_dir']\n",
        "        self.feature_window = config['feature_window']\n",
        "        self.l2_depth = config['l2_depth'] # Used for fetching\n",
        "        self.l2_depth_levels = config.get('l2_depth_levels', 5) # Used for calculation\n",
        "        self.api_key = api_key\n",
        "        self.api_secret = api_secret\n",
        "\n",
        "        # --- Define Feature Lists ---\n",
        "        self.ohlcv_base_features = [\"z_close\", \"z_volume\", \"z_spread\"]\n",
        "        self.hht_features = [\"hht_freq\", \"hht_accel\", \"hht_jerk\", \"hht_amplitude\"] if HAS_PYEMD else []\n",
        "        # Define the L2 features based on the previous example\n",
        "        self.l2_features = [\n",
        "            \"l2_obi\",\n",
        "            \"l2_depth_ratio_bid\",\n",
        "            \"l2_depth_ratio_ask\",\n",
        "            \"l2_weighted_mid_price\",\n",
        "            \"l2_micro_price\",\n",
        "            # Add derived L2 features like spread if needed, e.g., \"l2_spread\", \"l2_perc_spread\"\n",
        "            # Note: Core L2 features like best_bid, best_ask, mid_price are intermediate steps\n",
        "            # and might not be needed directly as model features unless desired.\n",
        "        ]\n",
        "        # Combine feature lists based on availability and config\n",
        "        self.feature_list = list(self.ohlcv_base_features)\n",
        "        if HAS_PYEMD: self.feature_list.extend(self.hht_features)\n",
        "        if self.config.get('use_l2_features', False): self.feature_list.extend(self.l2_features)\n",
        "\n",
        "        # --- File Paths (Derived from config base_dir) ---\n",
        "        os.makedirs(self.base_dir, exist_ok=True)\n",
        "        safe_symbol = self.symbol.replace('/', '_').replace(':', '')\n",
        "        self.model_path = os.path.join(self.base_dir, f\"lgbm_model_{safe_symbol}_{self.timeframe}.txt\")\n",
        "        self.features_json_path = os.path.join(self.base_dir, f\"model_features_{safe_symbol}_{self.timeframe}.json\")\n",
        "        self.ohlcv_data_path = os.path.join(self.base_dir, f\"ohlcv_data_{safe_symbol}_{self.timeframe}.csv\")\n",
        "        self.prepared_data_path = os.path.join(self.base_dir, f\"prepared_data_{safe_symbol}_{self.timeframe}.csv\")\n",
        "        # Path for the data collected by the L2 Collector cell\n",
        "        collector_symbol_safe = config.get('collector_symbol', 'BTCUSDT').lower()\n",
        "        collector_duration = config.get('collector_duration', 5)\n",
        "        collector_unit_char = config.get('collector_unit', 'minutes')[0]\n",
        "        l2_collector_filename = f\"{collector_symbol_safe}_l2_data_{collector_duration}{collector_unit_char}.jsonl\"\n",
        "        self.l2_raw_data_path = os.path.join(self.base_dir, 'l2_data', l2_collector_filename)\n",
        "        self.backtest_log_path = os.path.join(self.base_dir, f\"backtest_log_{safe_symbol}_{self.timeframe}.csv\")\n",
        "        self.simulation_log_path = os.path.join(self.base_dir, f\"simulation_log_{safe_symbol}_{self.timeframe}.csv\")\n",
        "        self.imf_plot_path = os.path.join(self.base_dir, f\"emd_imf_plot_{safe_symbol}_{self.timeframe}.png\")\n",
        "        self.feature_plot_path = os.path.join(self.base_dir, f\"feature_vs_price_plot_{safe_symbol}_{self.timeframe}.png\")\n",
        "        self.shap_bar_plot_path = os.path.join(self.base_dir, f\"shap_bar_plot_{safe_symbol}_{self.timeframe}.png\")\n",
        "        self.shap_dot_plot_path = os.path.join(self.base_dir, f\"shap_dot_plot_{safe_symbol}_{self.timeframe}.png\")\n",
        "        self.lgbm_importance_plot_path = os.path.join(self.base_dir, f\"lgbm_importance_plot_{safe_symbol}_{self.timeframe}.png\")\n",
        "        self.backtest_equity_plot_path = os.path.join(self.base_dir, f\"backtest_equity_curve_{safe_symbol}_{self.timeframe}.png\")\n",
        "        self.simulation_equity_plot_path = os.path.join(self.base_dir, f\"simulation_equity_curve_{safe_symbol}_{self.timeframe}.png\")\n",
        "\n",
        "        # --- Exchange Initialization ---\n",
        "        self.exchange = None # Initialize as None\n",
        "        try:\n",
        "            print(\"Initializing CCXT Bybit exchange...\")\n",
        "            exchange_config = {'enableRateLimit': True}\n",
        "            if self.api_key and self.api_secret:\n",
        "                exchange_config['apiKey'] = self.api_key\n",
        "                exchange_config['secret'] = self.api_secret\n",
        "                print(\"  -> Using provided API Key for authentication.\")\n",
        "            else:\n",
        "                print(\"  -> No API Key provided. Using public access.\")\n",
        "            self.exchange = ccxt.bybit(exchange_config)\n",
        "            print(\"  -> Loading markets...\")\n",
        "            self.exchange.load_markets() # Test connection\n",
        "            print(\"  -> Markets loaded successfully.\")\n",
        "            if not self.exchange.has.get('fetchOHLCV'): raise ccxt.ExchangeError(\"Exchange needs fetchOHLCV\")\n",
        "            # Check for L2 capability (needed for simulation)\n",
        "            if not self.exchange.has.get('fetchL2OrderBook'):\n",
        "                 print(f\"*** WARNING: {self.exchange.id} does not support fetchL2OrderBook via CCXT. L2 features in simulation will NOT work. ***\")\n",
        "            else:\n",
        "                 print(f\"  -> Exchange supports fetchL2OrderBook.\")\n",
        "            print(f\"CCXT exchange initialized: {self.exchange.id}\")\n",
        "        except ccxt.AuthenticationError as e: print(f\"FATAL: CCXT Auth Error: {e}. Check API keys.\"); self.exchange = None\n",
        "        except ccxt.ExchangeError as e:\n",
        "             print(f\"FATAL: CCXT Exchange Error initializing: {e}\")\n",
        "             if 'Forbidden' in str(e) and ('country' in str(e) or 'CloudFront' in str(e)): print(\"      Hint: Possible Geo-IP block. Try VPN/Proxy.\")\n",
        "             self.exchange = None\n",
        "        except Exception as e: print(f\"FATAL: Unexpected error initializing CCXT: {e}\"); traceback.print_exc(); self.exchange = None\n",
        "\n",
        "        # --- Bot State ---\n",
        "        self.target_mean = None; self.target_std = None\n",
        "        self.model = None; self.trained_features = []\n",
        "\n",
        "        # --- Live Simulation State ---\n",
        "        self.live_equity_history = []; self.live_position_history = []\n",
        "        self.live_timestamp_history = []; self.simulation_running = False\n",
        "        self.simulation_stop_event = threading.Event(); self.simulation_thread = None\n",
        "        self.current_live_position = {\"side\": None, \"entry_price\": 0, \"size\": 0, \"timestamp\": None}\n",
        "\n",
        "        print(f\"\\nBot Initialized:\")\n",
        "        print(f\"  - Base Directory: {self.base_dir}\")\n",
        "        print(f\"  - Symbol: {self.symbol}\")\n",
        "        print(f\"  - Timeframe: {self.timeframe}\")\n",
        "        print(f\"  - Use L2 Features: {self.config.get('use_l2_features', False)}\")\n",
        "        print(f\"  - L2 Raw Data Path: {self.l2_raw_data_path}\")\n",
        "        print(f\"  - PyEMD Available: {HAS_PYEMD}\")\n",
        "        print(f\"  - SHAP Available: {HAS_SHAP}\")\n",
        "        print(f\"  - Exchange Initialized: {'Yes' if self.exchange else 'No'}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    # ==========================================================================\n",
        "    # Data Fetching & Preparation\n",
        "    # ==========================================================================\n",
        "    def fetch_ohlcv(self, limit=None, since=None, max_retries=3, delay_seconds=5):\n",
        "        \"\"\"Fetches historical OHLCV data with retries.\"\"\"\n",
        "        if not self.exchange: print(\"Error: Exchange not initialized.\"); return pd.DataFrame()\n",
        "        # Use limit from config as default if not specified\n",
        "        limit = limit or self.config.get('fetch_ohlcv_limit', 1000)\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                if since is None:\n",
        "                    timeframe_duration_in_ms = self.exchange.parse_timeframe(self.timeframe) * 1000\n",
        "                    since = self.exchange.milliseconds() - (limit + 2) * timeframe_duration_in_ms\n",
        "                ohlcv = self.exchange.fetch_ohlcv(self.symbol, timeframe=self.timeframe, since=since, limit=limit)\n",
        "                if not ohlcv:\n",
        "                    print(f\"Warning: No OHLCV data returned (Attempt {attempt + 1}).\")\n",
        "                    if attempt < max_retries - 1: time.sleep(delay_seconds * (2**attempt)) # Exponential backoff\n",
        "                    continue\n",
        "                df = pd.DataFrame(ohlcv, columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
        "                df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", utc=True)\n",
        "                return df\n",
        "            except (ccxt.NetworkError, ccxt.ExchangeError, ccxt.RequestTimeout, ccxt.RateLimitExceeded) as e:\n",
        "                print(f\"Warning: CCXT error fetching OHLCV (Attempt {attempt + 1}/{max_retries}): {e}\")\n",
        "                if attempt < max_retries - 1: wait_time = delay_seconds * (2**attempt); print(f\"Retrying in {wait_time} seconds...\"); time.sleep(wait_time) # Exponential backoff\n",
        "                else: print(\"Error: Max retries reached for fetching OHLCV.\"); return pd.DataFrame()\n",
        "            except Exception as e: print(f\"Error: Unexpected error fetching OHLCV: {e}\"); traceback.print_exc(); return pd.DataFrame()\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def fetch_l2_order_book(self, limit=None, max_retries=3, delay_seconds=2):\n",
        "        \"\"\"Fetches a *snapshot* of the current Level 2 order book via REST API.\"\"\"\n",
        "        if not self.exchange: return None\n",
        "        if not self.exchange.has.get('fetchL2OrderBook'): return None\n",
        "        fetch_limit = limit if limit is not None else self.l2_depth\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                order_book = self.exchange.fetch_l2_order_book(self.symbol, limit=fetch_limit)\n",
        "                # Basic validation of the returned structure\n",
        "                if (order_book and isinstance(order_book, dict) and\n",
        "                    'bids' in order_book and isinstance(order_book['bids'], list) and order_book['bids'] and\n",
        "                    'asks' in order_book and isinstance(order_book['asks'], list) and order_book['asks'] and\n",
        "                    isinstance(order_book['bids'][0], list) and len(order_book['bids'][0]) == 2 and\n",
        "                    isinstance(order_book['asks'][0], list) and len(order_book['asks'][0]) == 2):\n",
        "                    # Add fetch timestamp for potential reference\n",
        "                    order_book['fetch_timestamp_ms'] = self.exchange.milliseconds()\n",
        "                    return order_book\n",
        "                else:\n",
        "                    print(f\"Warning: Invalid L2 data structure received (Attempt {attempt + 1}). Data: {order_book}\")\n",
        "                    if attempt < max_retries - 1: time.sleep(delay_seconds * (2**attempt)) # Exponential backoff\n",
        "            except (ccxt.NetworkError, ccxt.ExchangeError, ccxt.RequestTimeout, ccxt.RateLimitExceeded) as e:\n",
        "                print(f\"Warning: CCXT error fetching L2 snapshot (Attempt {attempt + 1}/{max_retries}): {e}\")\n",
        "                if attempt < max_retries - 1: wait_time = delay_seconds * (2**attempt); time.sleep(wait_time)\n",
        "                else: print(\"Error: Max retries reached for fetching L2 snapshot.\"); return None\n",
        "            except Exception as e: print(f\"Error: Unexpected error fetching L2 snapshot: {e}\"); traceback.print_exc(); return None\n",
        "        return None\n",
        "\n",
        "    def clean_ohlcv_data(self, df):\n",
        "        \"\"\"Cleans the raw OHLCV DataFrame.\"\"\"\n",
        "        if df is None or df.empty: return pd.DataFrame()\n",
        "        df = df.copy()\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['timestamp']): df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", utc=True, errors='coerce')\n",
        "        elif df['timestamp'].dt.tz is None: df['timestamp'] = df['timestamp'].dt.tz_localize('utc')\n",
        "        else: df['timestamp'] = df['timestamp'].dt.tz_convert('utc')\n",
        "        df.dropna(subset=['timestamp'], inplace=True)\n",
        "        df.drop_duplicates(subset=['timestamp'], keep='last', inplace=True)\n",
        "        df.sort_values('timestamp', inplace=True); df.reset_index(drop=True, inplace=True)\n",
        "        ohlcv_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "        for col in ohlcv_cols: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        df.dropna(subset=ohlcv_cols, inplace=True)\n",
        "        df = df[df['volume'] >= 0]\n",
        "        for col in [\"open\", \"high\", \"low\", \"close\"]: df = df[df[col] > 0]\n",
        "        df = df[df['high'] >= df['low']]\n",
        "        return df\n",
        "\n",
        "    # --- Feature Calculation Methods ---\n",
        "\n",
        "    def _calculate_zscore_features(self, df):\n",
        "        \"\"\"Calculates Z-score features.\"\"\"\n",
        "        df = df.copy()\n",
        "        window = self.feature_window\n",
        "        min_periods = max(1, window // 2)\n",
        "        for col in [\"close\", \"volume\"]:\n",
        "            mean = df[col].rolling(window=window, min_periods=min_periods).mean()\n",
        "            std = df[col].rolling(window=window, min_periods=min_periods).std()\n",
        "            std_safe = std.replace(0, np.nan)\n",
        "            df[f'z_{col}'] = (df[col] - mean) / std_safe\n",
        "        return df\n",
        "\n",
        "    def _calculate_spread_features(self, df):\n",
        "        \"\"\"Calculates spread features.\"\"\"\n",
        "        df = df.copy()\n",
        "        window = self.feature_window\n",
        "        min_periods = max(1, window // 2)\n",
        "        df['spread'] = df['high'] - df['low']\n",
        "        mean = df['spread'].rolling(window=window, min_periods=min_periods).mean()\n",
        "        std = df['spread'].rolling(window=window, min_periods=min_periods).std()\n",
        "        std_safe = std.replace(0, np.nan)\n",
        "        df['z_spread'] = (df['spread'] - mean) / std_safe\n",
        "        return df\n",
        "\n",
        "    def _calculate_hht_features(self, df):\n",
        "        \"\"\"Calculates HHT features using PyEMD.\"\"\"\n",
        "        if not HAS_PYEMD: return df\n",
        "        # Reduce verbosity for HHT calculation\n",
        "        # print(\"  Calculating HHT features (PyEMD)...\")\n",
        "        df = df.copy()\n",
        "        hht_cols = self.hht_features\n",
        "        min_required_len = self.feature_window + 5\n",
        "        if len(df) < min_required_len:\n",
        "            for col in hht_cols: df[col] = np.nan\n",
        "            return df\n",
        "        signal = df[\"close\"].values.astype(float)\n",
        "        for col in hht_cols: df[col] = np.nan\n",
        "        try:\n",
        "            emd = EMD() # Use PyEMD's EMD\n",
        "            imfs = emd(signal)\n",
        "            if imfs is None or not isinstance(imfs, np.ndarray) or imfs.shape[0] < 1: return df\n",
        "            imf1 = imfs[0]\n",
        "            analytic_signal = hilbert(imf1) # Use scipy.signal.hilbert\n",
        "            instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n",
        "            instantaneous_amplitude = np.abs(analytic_signal)\n",
        "            dt_series = df['timestamp'].diff().dt.total_seconds()\n",
        "            dt_median = dt_series.iloc[-self.feature_window:].median()\n",
        "            if pd.isna(dt_median) or dt_median <= 0: dt_median = dt_series.median()\n",
        "            dt = dt_median if (pd.notna(dt_median) and dt_median > 0) else 1.0\n",
        "            instantaneous_frequency = np.gradient(instantaneous_phase) / (2.0 * np.pi * dt)\n",
        "            instantaneous_accel = np.gradient(instantaneous_frequency) / dt\n",
        "            instantaneous_jerk = np.gradient(instantaneous_accel) / dt\n",
        "            if len(instantaneous_amplitude) == len(df): df[\"hht_amplitude\"] = instantaneous_amplitude\n",
        "            if len(instantaneous_frequency) == len(df): df[\"hht_freq\"] = instantaneous_frequency\n",
        "            if len(instantaneous_accel) == len(df): df[\"hht_accel\"] = instantaneous_accel\n",
        "            if len(instantaneous_jerk) == len(df): df[\"hht_jerk\"] = instantaneous_jerk\n",
        "        except Exception as e: print(f\"    Warning: Error during PyEMD/HHT calc: {e}\"); traceback.print_exc(limit=1)\n",
        "        return df\n",
        "\n",
        "    def _calculate_l2_features_from_snapshot(self, bids_snapshot, asks_snapshot):\n",
        "        \"\"\"\n",
        "        Calculates L2 features from a single snapshot of bids and asks.\n",
        "\n",
        "        Args:\n",
        "            bids_snapshot (list): List of [price, volume] for bids.\n",
        "            asks_snapshot (list): List of [price, volume] for asks.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing calculated L2 features (or NaNs on error).\n",
        "        \"\"\"\n",
        "        features = {col: np.nan for col in self.l2_features}\n",
        "        depth_levels = self.l2_depth_levels\n",
        "        try:\n",
        "            if not bids_snapshot or not asks_snapshot: return features\n",
        "            # Ensure snapshots are lists of lists/tuples with 2 elements\n",
        "            if not all(isinstance(level, (list, tuple)) and len(level) == 2 for level in bids_snapshot):\n",
        "                return features\n",
        "            if not all(isinstance(level, (list, tuple)) and len(level) == 2 for level in asks_snapshot):\n",
        "                return features\n",
        "\n",
        "            # Convert valid levels to numpy array for calculation\n",
        "            bids = np.array(bids_snapshot[:depth_levels], dtype=float)\n",
        "            asks = np.array(asks_snapshot[:depth_levels], dtype=float)\n",
        "\n",
        "            # Check if conversion resulted in valid arrays\n",
        "            if bids.shape[0] == 0 or asks.shape[0] == 0: return features\n",
        "            if bids.ndim != 2 or asks.ndim != 2 or bids.shape[1] != 2 or asks.shape[1] != 2: return features\n",
        "\n",
        "            bid_prices, bid_vols = bids[:, 0], bids[:, 1]\n",
        "            ask_prices, ask_vols = asks[:, 0], asks[:, 1]\n",
        "\n",
        "            best_bid, best_ask = bid_prices[0], ask_prices[0]\n",
        "            mid_price = (best_bid + best_ask) / 2\n",
        "            if mid_price == 0: return features # Avoid division by zero\n",
        "\n",
        "            # OBI (Order Book Imbalance)\n",
        "            total_bid_vol = bid_vols.sum(); total_ask_vol = ask_vols.sum()\n",
        "            total_vol = total_bid_vol + total_ask_vol\n",
        "            features['l2_obi'] = (total_bid_vol - total_ask_vol) / total_vol if total_vol > 0 else 0\n",
        "\n",
        "            # Depth Ratio (Top 1 level vs specified depth)\n",
        "            features['l2_depth_ratio_bid'] = bid_vols[0] / total_bid_vol if total_bid_vol > 0 else 0\n",
        "            features['l2_depth_ratio_ask'] = ask_vols[0] / total_ask_vol if total_ask_vol > 0 else 0\n",
        "\n",
        "            # Weighted Mid-Price\n",
        "            weighted_bid_sum = np.dot(bid_prices, bid_vols)\n",
        "            weighted_ask_sum = np.dot(ask_prices, ask_vols)\n",
        "            features['l2_weighted_mid_price'] = (weighted_bid_sum + weighted_ask_sum) / total_vol if total_vol > 0 else mid_price\n",
        "\n",
        "            # Micro-Price\n",
        "            top_level_vol = bid_vols[0] + ask_vols[0]\n",
        "            features['l2_micro_price'] = (best_bid * ask_vols[0] + best_ask * bid_vols[0]) / top_level_vol if top_level_vol > 0 else mid_price\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Warning: Error calculating L2 features from snapshot: {e}\")\n",
        "            # traceback.print_exc(limit=1) # Uncomment for more detail\n",
        "            pass # Keep features as NaN on error\n",
        "        return features\n",
        "\n",
        "    def _calculate_l2_features(self, df):\n",
        "        \"\"\"\n",
        "        Adds L2 feature columns (initially NaN) to the DataFrame.\n",
        "        Actual calculation requires pre-aligned L2 data ('l2_bids', 'l2_asks' columns)\n",
        "        or is handled separately (e.g., in the simulation loop).\n",
        "\n",
        "        *** IMPORTANT NOTE for Historical Data: ***\n",
        "        This function, when called by `prepare_features` on historical data,\n",
        "        DOES NOT fetch historical L2 snapshots. It only adds the columns.\n",
        "        You need a separate process to populate 'l2_bids'/'l2_asks' for historical data.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame containing OHLCV data.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with L2 feature columns added (likely NaN for historical runs).\n",
        "        \"\"\"\n",
        "        # Reduce verbosity\n",
        "        # print(\"  Adding L2 feature columns (requires pre-aligned L2 data or live fetch)...\")\n",
        "        df = df.copy()\n",
        "\n",
        "        # Add L2 feature columns, initializing with NaN\n",
        "        for col in self.l2_features:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "\n",
        "        # --- Calculation Logic (Example - Not Run Here for Historical) --- #\n",
        "        # If L2 data was pre-merged into df (e.g., df['l2_bids'], df['l2_asks'] columns):\n",
        "        # You could apply the '_calculate_l2_features_from_snapshot' function row-wise here.\n",
        "        # Example (Conceptual):\n",
        "        # if 'l2_bids' in df.columns and 'l2_asks' in df.columns:\n",
        "        #     print(\"  Attempting L2 calculation from pre-merged 'l2_bids'/'l2_asks' columns...\")\n",
        "        #     l2_results = df.apply(lambda row: self._calculate_l2_features_from_snapshot(row['l2_bids'], row['l2_asks']), axis=1)\n",
        "        #     l2_features_df = pd.DataFrame(l2_results.tolist(), index=df.index)\n",
        "        #     # Update only the L2 columns, preserving other data\n",
        "        #     for col in self.l2_features:\n",
        "        #         if col in l2_features_df.columns:\n",
        "        #             df[col] = l2_features_df[col]\n",
        "        # else:\n",
        "        #      print(\"    -> 'l2_bids'/'l2_asks' columns not found. L2 features remain NaN.\")\n",
        "\n",
        "        # print(\"    -> L2 feature columns added/ensured present.\") # Reduce verbosity\n",
        "        return df\n",
        "\n",
        "    def prepare_features(self, df_ohlcv, save=True):\n",
        "        \"\"\"Cleans OHLCV data and calculates all features based on config.\"\"\"\n",
        "        if df_ohlcv is None or df_ohlcv.empty: return pd.DataFrame()\n",
        "        print(f\"\\nPreparing features for {len(df_ohlcv)} OHLCV rows...\")\n",
        "        df = self.clean_ohlcv_data(df_ohlcv)\n",
        "        if df.empty: return df\n",
        "\n",
        "        # Calculate standard features\n",
        "        df = self._calculate_zscore_features(df)\n",
        "        df = self._calculate_spread_features(df)\n",
        "        if HAS_PYEMD: df = self._calculate_hht_features(df)\n",
        "\n",
        "        # Add L2 feature columns if enabled (will be NaN unless data pre-merged)\n",
        "        use_l2 = self.config.get('use_l2_features', False)\n",
        "        if use_l2:\n",
        "            df = self._calculate_l2_features(df)\n",
        "            # *** Reminder: For historical data, actual L2 data needs to be fetched/merged ***\n",
        "            # *** before this point for the L2 features to be non-NaN. ***\n",
        "\n",
        "        # Define features that MUST NOT be NaN for a row to be valid for base training/backtesting\n",
        "        required_non_nan_features = list(self.ohlcv_base_features)\n",
        "        if HAS_PYEMD:\n",
        "            required_non_nan_features.extend(self.hht_features)\n",
        "        # Filter to only those actually present in the df\n",
        "        required_non_nan_features = [f for f in required_non_nan_features if f in df.columns]\n",
        "\n",
        "        initial_len = len(df)\n",
        "        # Drop rows only if essential non-L2 features are NaN\n",
        "        df.dropna(subset=required_non_nan_features, inplace=True)\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        rows_dropped = initial_len - len(df)\n",
        "        if rows_dropped > 0:\n",
        "            print(f\"  Dropped {rows_dropped} rows with NaN in essential features: {required_non_nan_features}\")\n",
        "\n",
        "        if df.empty:\n",
        "             print(\"Error: DataFrame empty after essential feature NaN drop.\")\n",
        "             # This shouldn't happen if OHLCV/HHT features are calculated correctly\n",
        "             return df\n",
        "\n",
        "        # Save prepared data\n",
        "        if save:\n",
        "            try: df.to_csv(self.prepared_data_path, index=False); print(f\"Prepared data saved: {self.prepared_data_path}\")\n",
        "            except Exception as e: print(f\"Warning: Error saving prepared data: {e}\")\n",
        "\n",
        "        print(f\"Feature preparation complete. Final shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def generate_labels(self, df, scale=True):\n",
        "        \"\"\"Generates the target variable (future return).\"\"\"\n",
        "        if df is None or df.empty or 'close' not in df.columns: return pd.DataFrame()\n",
        "        df = df.copy()\n",
        "        df[\"future_return\"] = df[\"close\"].pct_change().shift(-1)\n",
        "        self.target_mean = df[\"future_return\"].mean()\n",
        "        self.target_std = df[\"future_return\"].std()\n",
        "        df.dropna(subset=[\"future_return\"], inplace=True)\n",
        "        if df.empty: return df\n",
        "        if scale:\n",
        "            if self.target_std is not None and self.target_std != 0: df[\"target\"] = (df[\"future_return\"] - self.target_mean) / self.target_std\n",
        "            else: df[\"target\"] = df[\"future_return\"]\n",
        "        else: df[\"target\"] = df[\"future_return\"]\n",
        "        df.dropna(subset=[\"target\"], inplace=True)\n",
        "        return df\n",
        "\n",
        "    # ==========================================================================\n",
        "    # Model Training & Prediction\n",
        "    # ==========================================================================\n",
        "    def train_model(self, df=None, save=True):\n",
        "        \"\"\"Trains model using Optuna, based on config.\"\"\"\n",
        "        if 'optuna' not in sys.modules: print(\"Error: Optuna required.\"); return None\n",
        "        print(\"\\n--- Starting Model Training ---\")\n",
        "        train_df = None\n",
        "        if df is None:\n",
        "            try: train_df = pd.read_csv(self.prepared_data_path, parse_dates=['timestamp']); print(f\"Loaded data ({len(train_df)} rows).\")\n",
        "            except Exception as e: print(f\"Error loading data: {e}\"); return None\n",
        "        else: train_df = df.copy()\n",
        "        if train_df is None or train_df.empty: print(\"Error: No data.\"); return None\n",
        "        if \"target\" not in train_df.columns:\n",
        "            train_df = self.generate_labels(train_df, scale=True)\n",
        "            if train_df.empty or \"target\" not in train_df.columns: print(\"Error: Label gen failed.\"); return None\n",
        "        # Determine features to use based on config and availability in data\n",
        "        potential_features = list(self.ohlcv_base_features)\n",
        "        if HAS_PYEMD: potential_features.extend(self.hht_features)\n",
        "        if self.config.get('use_l2_features', False):\n",
        "             potential_features.extend(self.l2_features)\n",
        "\n",
        "        # Filter potential features to only those present in the dataframe AND not all NaN\n",
        "        self.trained_features = []\n",
        "        l2_all_nan = False\n",
        "        l2_cols_in_train = [f for f in self.l2_features if f in train_df.columns]\n",
        "        if self.config.get('use_l2_features', False) and l2_cols_in_train:\n",
        "            if train_df[l2_cols_in_train].isnull().all().all():\n",
        "                 l2_all_nan = True\n",
        "                 print(\"*** WARNING: L2 features are enabled but appear to be all NaN in training data. ***\")\n",
        "                 print(\"*** Ensure historical L2 data pre-processing step is working correctly if needed for training. ***\")\n",
        "                 print(\"*** Training will proceed WITHOUT effectively using L2 features. ***\")\n",
        "\n",
        "        for f in potential_features:\n",
        "             if f in train_df.columns and not train_df[f].isnull().all():\n",
        "                 # Exclude L2 features if they are flagged as all NaN\n",
        "                 is_l2_feature = f in self.l2_features\n",
        "                 if not (is_l2_feature and l2_all_nan):\n",
        "                     self.trained_features.append(f)\n",
        "\n",
        "        # Drop rows with NaNs in the final selected features or target\n",
        "        final_feature_target_list = self.trained_features + ['target']\n",
        "        if train_df[final_feature_target_list].isnull().any().any():\n",
        "             print(f\"Warning: NaNs detected in final selected features/target {final_feature_target_list} before training. Dropping rows.\");\n",
        "             initial_len = len(train_df)\n",
        "             train_df.dropna(subset=final_feature_target_list, inplace=True)\n",
        "             print(f\"Dropped {initial_len - len(train_df)} rows.\")\n",
        "\n",
        "        if not self.trained_features: print(\"Error: No valid features remain after checking data and NaNs.\"); return None\n",
        "        if train_df.empty: print(\"Error: DataFrame empty after NaN checks.\"); return None\n",
        "\n",
        "        print(f\"Training with {len(self.trained_features)} features: {self.trained_features}\")\n",
        "        X = train_df[self.trained_features]; y = train_df[\"target\"]\n",
        "        if len(X) < 50: print(f\"Error: Insufficient data ({len(X)} samples) for training.\"); return None\n",
        "\n",
        "        test_size = self.config.get('test_size', 0.2)\n",
        "        split_index = int(len(X) * (1 - test_size)); X_train, X_val = X.iloc[:split_index], X.iloc[split_index:]; y_train, y_val = y.iloc[:split_index], y.iloc[split_index:]\n",
        "        print(f\"Train/Validation split: {len(X_train)} train, {len(X_val)} validation.\")\n",
        "\n",
        "        def objective(trial):\n",
        "            params = {'objective': 'regression_l1', 'metric': 'mae', 'verbosity': -1, 'boosting_type': 'gbdt', 'random_state': 42,\n",
        "                      'n_estimators': trial.suggest_int('n_estimators', 100, 1500, step=100), 'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
        "                      'num_leaves': trial.suggest_int('num_leaves', 10, 100), 'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "                      'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True), 'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
        "                      'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0), 'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "                      'bagging_freq': trial.suggest_int('bagging_freq', 1, 7), 'min_child_samples': trial.suggest_int('min_child_samples', 5, 50)}\n",
        "            model = lgb.LGBMRegressor(**params)\n",
        "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mae', callbacks=[lgb.early_stopping(25, False)])\n",
        "            preds = model.predict(X_val); return mean_absolute_error(y_val, preds)\n",
        "\n",
        "        n_trials = self.config.get('optuna_trials', 50)\n",
        "        print(f\"Starting Optuna search ({n_trials} trials)...\")\n",
        "        study = optuna.create_study(direction='minimize')\n",
        "        try: study.optimize(objective, n_trials=n_trials, timeout=1200)\n",
        "        except Exception as e: print(f\"Error during Optuna: {e}\"); traceback.print_exc(); return None\n",
        "        print(f\"Optuna finished. Best MAE: {study.best_value:.5f}. Params: {study.best_params}\")\n",
        "        print(\"Training final model on all data...\")\n",
        "        final_params = study.best_params; final_params.update({'objective':'regression_l1','metric':'mae','verbosity':-1,'boosting_type':'gbdt','random_state':42})\n",
        "        final_model = lgb.LGBMRegressor(**final_params); final_model.fit(X, y)\n",
        "        self.model = final_model.booster_\n",
        "        if save:\n",
        "            try:\n",
        "                self.model.save_model(self.model_path); print(f\"Model saved: {self.model_path}\")\n",
        "                # Save the features actually used for training\n",
        "                with open(self.features_json_path, 'w') as f: json.dump(self.trained_features, f, indent=4)\n",
        "                print(f\"Features saved: {self.features_json_path}\")\n",
        "            except Exception as e: print(f\"Warning: Error saving model/features: {e}\")\n",
        "        print(\"--- Model Training Finished ---\")\n",
        "        return self.model\n",
        "\n",
        "    def load_model(self, model_path=None, features_path=None):\n",
        "        \"\"\"Loads model and feature list.\"\"\"\n",
        "        _model_path = model_path or self.model_path; _features_path = features_path or self.features_json_path\n",
        "        print(f\"Loading model: {_model_path}\"); print(f\"Loading features: {_features_path}\")\n",
        "        try:\n",
        "            self.model = lgb.Booster(model_file=_model_path)\n",
        "            with open(_features_path, 'r') as f: self.trained_features = json.load(f)\n",
        "            print(f\"Model and {len(self.trained_features)} features loaded: {self.trained_features}\")\n",
        "            if self.target_mean is None or self.target_std is None: self._load_scaling_info()\n",
        "            return True\n",
        "        except FileNotFoundError: print(f\"Error: Model/features file not found.\"); self.model=None; self.trained_features=[]; return False\n",
        "        except Exception as e: print(f\"Error loading model/features: {e}\"); self.model=None; self.trained_features=[]; return False\n",
        "\n",
        "    def _load_scaling_info(self):\n",
        "        \"\"\"Loads scaling info from prepared data.\"\"\"\n",
        "        try:\n",
        "            df_temp = pd.read_csv(self.prepared_data_path, parse_dates=['timestamp'], usecols=['timestamp', 'close'])\n",
        "            if df_temp.empty: print(\"Warning: Prepared data empty.\"); return\n",
        "            _ = self.generate_labels(df_temp, scale=True)\n",
        "            if self.target_mean is None or self.target_std is None: print(\"Warning: Could not determine scaling parameters.\")\n",
        "        except FileNotFoundError: print(f\"Warning: Cannot load scaling info, file not found: {self.prepared_data_path}\")\n",
        "        except Exception as e: print(f\"Warning: Error loading scaling info: {e}\")\n",
        "\n",
        "    def predict_signals(self, df, model=None, threshold=None):\n",
        "        \"\"\"Makes predictions and generates signals.\"\"\"\n",
        "        if df is None or df.empty: print(\"Predict Error: Input empty.\"); return None\n",
        "        _model = model or self.model\n",
        "        if _model is None: print(\"Predict Error: No model.\"); return None\n",
        "        # Use the feature list saved during the *last successful training*\n",
        "        if not self.trained_features:\n",
        "            print(\"Predict Error: Trained features unknown (model not loaded or trained?).\")\n",
        "            if not self.load_model(): return None # Attempt to load model and features\n",
        "            if not self.trained_features: return None # Still no features after loading\n",
        "\n",
        "        # Use threshold from config if not provided\n",
        "        threshold = threshold if threshold is not None else self.config.get('backtest_threshold', 0.5)\n",
        "\n",
        "        # Check for missing features IN THE INPUT DATAFRAME based on what the model was trained on\n",
        "        missing_features = [f for f in self.trained_features if f not in df.columns]\n",
        "        if missing_features:\n",
        "            print(f\"Predict Error: Input DataFrame is missing required features: {missing_features}\")\n",
        "            print(f\"  Required by loaded model: {self.trained_features}\")\n",
        "            print(f\"  Available in DataFrame: {df.columns.tolist()}\")\n",
        "            return None\n",
        "\n",
        "        # Select only the features the model was trained on\n",
        "        X_predict = df[self.trained_features]\n",
        "\n",
        "        # Check for NaNs *in the specific features required by the model* for the rows being predicted\n",
        "        if X_predict.isnull().values.any():\n",
        "            print(f\"Predict Warning: NaNs found in required features {self.trained_features} before prediction. Check data prep.\")\n",
        "            print(X_predict[X_predict.isnull().any(axis=1)]) # Print rows with NaNs\n",
        "            # Decide how to handle: return None, fillna, etc.\n",
        "            # For now, let prediction proceed, LightGBM might handle some NaNs depending on training\n",
        "            # return None # Stricter approach\n",
        "\n",
        "        try: predictions_scaled = _model.predict(X_predict)\n",
        "        except Exception as e: print(f\"Predict Error: Prediction failed: {e}\"); traceback.print_exc(); return None\n",
        "\n",
        "        result_df = df.copy(); result_df[\"pred_scaled\"] = predictions_scaled\n",
        "        if self.target_mean is not None and self.target_std is not None and self.target_std != 0:\n",
        "            result_df[\"pred_unscaled_return\"] = (result_df[\"pred_scaled\"] * self.target_std) + self.target_mean\n",
        "        else:\n",
        "            result_df[\"pred_unscaled_return\"] = np.nan\n",
        "            # Warning moved to load_scaling_info\n",
        "            # print(\"Predict Warning: Scaling parameters (mean/std) not available. Cannot unscale predictions.\")\n",
        "\n",
        "        buy_signal = result_df[\"pred_scaled\"] > threshold; sell_signal = result_df[\"pred_scaled\"] < -threshold\n",
        "        result_df[\"signal\"] = np.select([buy_signal, sell_signal], [1, -1], default=0)\n",
        "        return result_df\n",
        "\n",
        "    # ==========================================================================\n",
        "    # Backtesting\n",
        "    # ==========================================================================\n",
        "    def backtest(self, df=None, initial_balance=None, threshold=None, commission_pct=None, leverage=None, stop_loss_pct=None, take_profit_pct=None):\n",
        "        \"\"\"Performs backtest using parameters from config if not provided.\"\"\"\n",
        "        print(\"\\n--- Starting Backtest ---\")\n",
        "        # Load parameters from config if not provided\n",
        "        initial_balance = initial_balance if initial_balance is not None else self.config.get('initial_balance', 10000)\n",
        "        threshold = threshold if threshold is not None else self.config.get('backtest_threshold', 0.5)\n",
        "        commission_pct = commission_pct if commission_pct is not None else self.config.get('commission_pct', 0.0006)\n",
        "        leverage = leverage if leverage is not None else self.config.get('leverage', 1)\n",
        "        stop_loss_pct = stop_loss_pct if stop_loss_pct is not None else self.config.get('stop_loss_pct', None)\n",
        "        take_profit_pct = take_profit_pct if take_profit_pct is not None else self.config.get('take_profit_pct', None)\n",
        "\n",
        "        backtest_df = None\n",
        "        if df is None:\n",
        "            try:\n",
        "                df_loaded = pd.read_csv(self.prepared_data_path, parse_dates=['timestamp'])\n",
        "                print(f\"Loaded prepared data for backtest ({len(df_loaded)} rows).\")\n",
        "                if self.model is None or not self.trained_features:\n",
        "                    if not self.load_model(): print(\"Backtest Error: Failed to load model.\"); return None, None\n",
        "                # Ensure predictions are made if needed\n",
        "                if 'signal' not in df_loaded.columns or 'pred_scaled' not in df_loaded.columns:\n",
        "                    print(f\"Predicting signals with threshold={threshold}...\")\n",
        "                    # Pass the loaded model to ensure consistency\n",
        "                    df_processed = self.predict_signals(df_loaded, model=self.model, threshold=threshold)\n",
        "                    if df_processed is None or df_processed.empty: print(\"Backtest Error: Signal prediction failed.\"); return None, None\n",
        "                    backtest_df = df_processed\n",
        "                else: backtest_df = df_loaded\n",
        "            except FileNotFoundError: print(f\"Backtest Error: Prepared data file not found: {self.prepared_data_path}\"); return None, None\n",
        "            except Exception as e: print(f\"Backtest Error: Error loading/preparing data: {e}\"); traceback.print_exc(); return None, None\n",
        "        else:\n",
        "             # If a dataframe is passed, ensure it has signals\n",
        "             if 'signal' not in df.columns or 'pred_scaled' not in df.columns:\n",
        "                 print(\"Predicting signals for provided DataFrame...\")\n",
        "                 # Pass the loaded model to ensure consistency\n",
        "                 df_processed = self.predict_signals(df.copy(), model=self.model, threshold=threshold)\n",
        "                 if df_processed is None or df_processed.empty: print(\"Backtest Error: Signal prediction failed for provided df.\"); return None, None\n",
        "                 backtest_df = df_processed\n",
        "             else:\n",
        "                 backtest_df = df.copy()\n",
        "\n",
        "        # Ensure all necessary columns are present AFTER prediction\n",
        "        required_cols = ['timestamp', 'open', 'high', 'low', 'close', 'signal', 'pred_scaled']\n",
        "        missing_cols = [col for col in required_cols if col not in backtest_df.columns]\n",
        "        if missing_cols: print(f\"Backtest Error: Missing columns after prediction: {missing_cols}\"); return None, None\n",
        "        if backtest_df.empty: print(\"Backtest Error: DataFrame empty.\"); return None, None\n",
        "        backtest_df.sort_values('timestamp', inplace=True); backtest_df.reset_index(drop=True, inplace=True)\n",
        "        print(f\"Running backtest on {len(backtest_df)} points.\")\n",
        "        print(f\"Params: Init Bal=${initial_balance:,.2f}, Comm={commission_pct*100:.4f}%, Lev={leverage}x, SL={stop_loss_pct*100 if stop_loss_pct else 'N/A'}%, TP={take_profit_pct*100 if take_profit_pct else 'N/A'}%\")\n",
        "        print(\"\\nPrediction Statistics (pred_scaled):\"); print(backtest_df['pred_scaled'].describe()); print(\"-\" * 20)\n",
        "        balance = initial_balance; position = 0; entry_price = 0.0; position_size_asset = 0.0\n",
        "        stop_loss_price = None; take_profit_price = None; entry_timestamp = None; entry_commission = 0.0\n",
        "        equity_curve = []; closed_trades = []\n",
        "        for row in backtest_df.itertuples():\n",
        "            current_price = row.close; current_signal = row.signal; current_timestamp = row.timestamp\n",
        "            current_high = row.high; current_low = row.low; unrealized_pnl = 0\n",
        "            if position != 0: pnl_per_asset = (current_price - entry_price) if position == 1 else (entry_price - current_price); unrealized_pnl = pnl_per_asset * position_size_asset\n",
        "            current_equity = balance + unrealized_pnl; equity_curve.append(current_equity)\n",
        "            exit_reason = None; exit_price = current_price\n",
        "            if position == 1:\n",
        "                if stop_loss_pct and stop_loss_price is not None and current_low <= stop_loss_price: exit_reason = \"Stop Loss Hit\"; exit_price = stop_loss_price; current_signal = -1\n",
        "                elif take_profit_pct and take_profit_price is not None and current_high >= take_profit_price: exit_reason = \"Take Profit Hit\"; exit_price = take_profit_price; current_signal = -1\n",
        "            elif position == -1:\n",
        "                if stop_loss_pct and stop_loss_price is not None and current_high >= stop_loss_price: exit_reason = \"Stop Loss Hit\"; exit_price = stop_loss_price; current_signal = 1\n",
        "                elif take_profit_pct and take_profit_price is not None and current_low <= take_profit_price: exit_reason = \"Take Profit Hit\"; exit_price = take_profit_price; current_signal = 1\n",
        "            if position != 0 and (current_signal == -position or current_signal == 0 or exit_reason is not None):\n",
        "                pnl_per_asset = (exit_price - entry_price) if position == 1 else (entry_price - exit_price); pnl_gross = pnl_per_asset * position_size_asset\n",
        "                exit_value = exit_price * position_size_asset; commission_exit = exit_value * commission_pct\n",
        "                commission_total = entry_commission + commission_exit; pnl_net = pnl_gross - commission_total; balance += pnl_net\n",
        "                closed_trades.append({\"entry_timestamp\": entry_timestamp, \"exit_timestamp\": current_timestamp, \"direction\": \"long\" if position == 1 else \"short\",\"entry_price\": entry_price, \"exit_price\": exit_price, \"size_asset\": position_size_asset, \"pnl_gross\": pnl_gross,\"commission\": commission_total, \"pnl_net\": pnl_net, \"equity_after_trade\": balance, \"exit_reason\": exit_reason if exit_reason else \"Signal\"})\n",
        "                position = 0; entry_price = 0.0; position_size_asset = 0.0; stop_loss_price = None; take_profit_price = None; entry_timestamp = None; entry_commission = 0.0\n",
        "            if position == 0 and current_signal != 0 and exit_reason is None:\n",
        "                position = current_signal; entry_price = current_price; entry_timestamp = current_timestamp\n",
        "                position_size_usd = current_equity * leverage; position_size_asset = position_size_usd / entry_price\n",
        "                entry_value = entry_price * position_size_asset; entry_commission = entry_value * commission_pct; balance -= entry_commission\n",
        "                if position == 1:\n",
        "                    if stop_loss_pct: stop_loss_price = entry_price * (1 - stop_loss_pct)\n",
        "                    if take_profit_pct: take_profit_price = entry_price * (1 + take_profit_pct)\n",
        "                elif position == -1:\n",
        "                    if stop_loss_pct: stop_loss_price = entry_price * (1 + stop_loss_pct)\n",
        "                    if take_profit_pct: take_profit_price = entry_price * (1 - take_profit_pct)\n",
        "        if position != 0:\n",
        "            last_price = backtest_df['close'].iloc[-1]; last_timestamp = backtest_df['timestamp'].iloc[-1]; print(f\"Closing open position at end (Price: {last_price:.2f})\")\n",
        "            pnl_per_asset = (last_price - entry_price) if position == 1 else (entry_price - last_price); pnl_gross = pnl_per_asset * position_size_asset\n",
        "            exit_value = last_price * position_size_asset; commission_exit = exit_value * commission_pct; commission_total = entry_commission + commission_exit\n",
        "            pnl_net = pnl_gross - commission_total; balance += pnl_net; equity_curve[-1] = balance\n",
        "            closed_trades.append({\"entry_timestamp\": entry_timestamp, \"exit_timestamp\": last_timestamp, \"direction\": \"long\" if position == 1 else \"short\",\"entry_price\": entry_price, \"exit_price\": last_price, \"size_asset\": position_size_asset, \"pnl_gross\": pnl_gross,\"commission\": commission_total, \"pnl_net\": pnl_net, \"equity_after_trade\": balance, \"exit_reason\": \"End of Backtest\"})\n",
        "        if not equity_curve: print(\"Backtest Error: No equity data.\"); return None, None\n",
        "        result_df = backtest_df.iloc[:len(equity_curve)].copy(); result_df[\"equity\"] = equity_curve; trades_log_df = pd.DataFrame(closed_trades)\n",
        "        print(\"\\n--- Backtest Results ---\")\n",
        "        if not result_df.empty: print(f\"Period: {result_df['timestamp'].iloc[0]} to {result_df['timestamp'].iloc[-1]} ({(result_df['timestamp'].iloc[-1] - result_df['timestamp'].iloc[0]).days} days)\")\n",
        "        final_balance = equity_curve[-1]; total_return_pct = (final_balance - initial_balance) / initial_balance * 100\n",
        "        print(f\"Initial Balance: ${initial_balance:,.2f}\"); print(f\"Final Balance:   ${final_balance:,.2f}\"); print(f\"Total Return:    {total_return_pct:.2f}%\")\n",
        "        equity_series = pd.Series(equity_curve); rolling_max = equity_series.cummax(); drawdown = (equity_series - rolling_max) / rolling_max\n",
        "        max_drawdown_pct = abs(drawdown.min() * 100) if not drawdown.empty and drawdown.min() < 0 else 0; print(f\"Max Drawdown:    {max_drawdown_pct:.2f}%\")\n",
        "        num_trades = len(trades_log_df); print(f\"Total Trades:    {num_trades}\")\n",
        "        if num_trades > 0:\n",
        "            winning_trades = trades_log_df[trades_log_df['pnl_net'] > 0]; losing_trades = trades_log_df[trades_log_df['pnl_net'] <= 0]\n",
        "            num_winners = len(winning_trades); win_rate_pct = (num_winners / num_trades) * 100\n",
        "            total_profit = winning_trades['pnl_net'].sum(); total_loss = abs(losing_trades['pnl_net'].sum())\n",
        "            profit_factor = total_profit / total_loss if total_loss > 0 else float('inf') if total_profit > 0 else 0\n",
        "            avg_profit_win = winning_trades['pnl_net'].mean() if num_winners > 0 else 0; avg_loss_lose = losing_trades['pnl_net'].mean() if len(losing_trades) > 0 else 0\n",
        "            avg_pnl_per_trade = trades_log_df['pnl_net'].mean(); payoff_ratio = abs(avg_profit_win / avg_loss_lose) if avg_loss_lose != 0 else float('inf')\n",
        "            print(f\"Win Rate:        {win_rate_pct:.2f}% ({num_winners}/{num_trades})\"); print(f\"Profit Factor:   {profit_factor:.2f}\"); print(f\"Payoff Ratio:    {payoff_ratio:.2f}\")\n",
        "            print(f\"Avg PnL/Trade:   ${avg_pnl_per_trade:.2f}\"); print(f\"Avg Win Trade:   ${avg_profit_win:.2f}\"); print(f\"Avg Loss Trade:  ${avg_loss_lose:.2f}\")\n",
        "        else: print(\"No trades executed.\")\n",
        "        print(\"------------------------\")\n",
        "        if 'matplotlib' in sys.modules and plt is not None: self.plot_equity_curve(result_df, initial_balance, final_balance, total_return_pct) # Call helper\n",
        "        if not trades_log_df.empty: self.save_backtest_log(trades_log_df) # Call helper\n",
        "        print(\"--- Backtest Finished ---\")\n",
        "        return result_df, trades_log_df\n",
        "\n",
        "    def save_backtest_log(self, trades_log_df):\n",
        "        \"\"\"Saves the backtest trade log to a CSV file.\"\"\"\n",
        "        try:\n",
        "            log_cols = {\"entry_timestamp\": \"Entry Time\", \"exit_timestamp\": \"Exit Time\", \"direction\": \"Direction\", \"entry_price\": \"Entry Price\", \"exit_price\": \"Exit Price\", \"size_asset\": \"Size (Asset)\", \"pnl_gross\": \"PnL Gross\", \"commission\": \"Commission\", \"pnl_net\": \"PnL Net\", \"equity_after_trade\": \"Equity After\", \"exit_reason\": \"Exit Reason\"}\n",
        "            log_cols_present = {k: v for k, v in log_cols.items() if k in trades_log_df.columns}; trades_log_to_save = trades_log_df[log_cols_present.keys()].rename(columns=log_cols_present)\n",
        "            trades_log_to_save.to_csv(self.backtest_log_path, index=False, float_format='%.8f'); print(f\"Backtest trades log saved: {self.backtest_log_path}\")\n",
        "        except Exception as e: print(f\"Warning: Error saving backtest trades log: {e}\")\n",
        "\n",
        "    def plot_equity_curve(self, result_df, initial_balance, final_balance, total_return_pct):\n",
        "         \"\"\"Plots the backtest equity curve.\"\"\"\n",
        "         try:\n",
        "            plt.figure(figsize=(14, 7)); plt.plot(result_df[\"timestamp\"], result_df[\"equity\"], label=\"Equity Curve\", color='dodgerblue')\n",
        "            plt.fill_between(result_df[\"timestamp\"], initial_balance, result_df[\"equity\"], where=result_df[\"equity\"] >= initial_balance, color='green', alpha=0.3, interpolate=True)\n",
        "            plt.fill_between(result_df[\"timestamp\"], initial_balance, result_df[\"equity\"], where=result_df[\"equity\"] < initial_balance, color='red', alpha=0.3, interpolate=True)\n",
        "            plt.axhline(initial_balance, color='grey', linestyle='--', label='Initial Balance')\n",
        "            plt.title(f\"Backtest Equity Curve: {self.symbol} {self.timeframe}\\nFinal Balance: ${final_balance:,.2f} ({total_return_pct:.2f}%)\")\n",
        "            plt.xlabel(\"Time\"); plt.ylabel(\"Equity (USD)\"); plt.grid(True, linestyle='--', alpha=0.6); plt.legend(); plt.tight_layout()\n",
        "            plt.savefig(self.backtest_equity_plot_path); print(f\"Backtest equity curve saved: {self.backtest_equity_plot_path}\"); plt.show()\n",
        "         except Exception as e: print(f\"Warning: Error plotting equity curve: {e}\")\n",
        "\n",
        "\n",
        "    # ==========================================================================\n",
        "    # Live Simulation\n",
        "    # ==========================================================================\n",
        "    def _log_simulation_action(self, timestamp, action_type, details):\n",
        "        \"\"\"Logs simulation actions to a file.\"\"\"\n",
        "        try:\n",
        "            ts_iso = pd.Timestamp(timestamp).tz_convert('utc').isoformat() if pd.Timestamp(timestamp).tzinfo else pd.Timestamp(timestamp).tz_localize('utc').isoformat()\n",
        "            log_entry = {\"timestamp\": ts_iso, \"action\": action_type, **details}\n",
        "            # Handle potential numpy types for JSON serialization\n",
        "            log_entry_serializable = json.loads(json.dumps(log_entry, default=lambda x: str(x) if isinstance(x, (np.int_, np.float_, np.number, pd.Timestamp)) else None))\n",
        "            with open(self.simulation_log_path, \"a\") as f: f.write(json.dumps(log_entry_serializable) + \"\\n\")\n",
        "        except Exception as e: print(f\"Error logging sim action: {e}. Details: {details}\")\n",
        "\n",
        "    def _simulation_loop(self, initial_equity, threshold, fetch_limit, loop_interval_seconds, commission_pct, leverage, stop_loss_pct, take_profit_pct):\n",
        "        \"\"\"Core loop for the live simulation thread.\"\"\"\n",
        "        print(f\"\\n--- Live Simulation Thread Started ---\")\n",
        "        print(f\"Interval:{loop_interval_seconds}s, Thresh:{threshold}, Comm:{commission_pct*100:.4f}%, Lev:{leverage}x, SL:{stop_loss_pct*100 if stop_loss_pct else 'N/A'}%, TP:{take_profit_pct*100 if take_profit_pct else 'N/A'}%\")\n",
        "        self.live_equity_history = [initial_equity]; self.live_timestamp_history = [datetime.now(timezone.utc)]\n",
        "        self.current_live_position = {\"side\": None, \"entry_price\": 0.0, \"size\": 0.0, \"timestamp\": None, \"sl_price\": None, \"tp_price\": None}\n",
        "        balance = initial_equity\n",
        "        if self.model is None or not self.trained_features:\n",
        "            if not self.load_model(): print(\"FATAL SIM ERROR: Failed to load model.\"); self.simulation_running = False; self._log_simulation_action(datetime.now(timezone.utc), \"ERROR\", {\"message\": \"Failed to load model\"}); return\n",
        "        print(f\"Sim using features: {self.trained_features}\")\n",
        "\n",
        "        # Check if L2 features are expected and if the exchange supports fetching them\n",
        "        use_l2 = self.config.get('use_l2_features', False)\n",
        "        can_fetch_l2 = self.exchange and self.exchange.has.get('fetchL2OrderBook')\n",
        "        if use_l2 and not can_fetch_l2:\n",
        "            print(\"*** WARNING: L2 features enabled, but exchange does not support fetchL2OrderBook via CCXT. L2 features will be NaN. ***\")\n",
        "            use_l2 = False # Disable L2 usage for the loop\n",
        "\n",
        "        while self.simulation_running and not self.simulation_stop_event.is_set():\n",
        "            loop_start_time = time.monotonic(); now_utc = datetime.now(timezone.utc)\n",
        "            try:\n",
        "                # --- Fetch OHLCV Data ---\n",
        "                required_candles = self.feature_window + 10; adjusted_fetch_limit = max(fetch_limit, required_candles)\n",
        "                df_hist = self.fetch_ohlcv(limit=adjusted_fetch_limit)\n",
        "                if df_hist is None or df_hist.empty or len(df_hist) < required_candles:\n",
        "                    print(f\"[{now_utc.strftime('%H:%M:%S')}] Insufficient OHLCV ({len(df_hist) if df_hist is not None else 0}). Wait...\")\n",
        "                    self._log_simulation_action(now_utc, \"WAIT\", {\"reason\": \"Insufficient OHLCV\", \"count\": len(df_hist) if df_hist is not None else 0})\n",
        "                    if self.live_equity_history: self.live_equity_history.append(self.live_equity_history[-1]); self.live_timestamp_history.append(now_utc)\n",
        "                    elapsed_time = time.monotonic() - loop_start_time; sleep_time = max(0, loop_interval_seconds - elapsed_time)\n",
        "                    if sleep_time > 0: self.simulation_stop_event.wait(sleep_time)\n",
        "                    continue\n",
        "\n",
        "                # Clean OHLCV data first\n",
        "                df_hist_clean = self.clean_ohlcv_data(df_hist)\n",
        "                if df_hist_clean.empty: continue # Skip if cleaning failed\n",
        "\n",
        "                # --- L2 Data Handling for Simulation (Fetch & Merge) ---\n",
        "                l2_features_calculated = {}\n",
        "                if use_l2 and can_fetch_l2:\n",
        "                    # print(f\"[{now_utc.strftime('%H:%M:%S')}] Fetching L2 snapshot...\") # Optional: Verbose logging\n",
        "                    latest_l2_snapshot = self.fetch_l2_order_book()\n",
        "                    if latest_l2_snapshot and 'bids' in latest_l2_snapshot and 'asks' in latest_l2_snapshot:\n",
        "                        l2_features_calculated = self._calculate_l2_features_from_snapshot(latest_l2_snapshot['bids'], latest_l2_snapshot['asks'])\n",
        "                        # print(f\"[{now_utc.strftime('%H:%M:%S')}] L2 features calculated: {l2_features_calculated}\") # Optional: Verbose logging\n",
        "                    else:\n",
        "                        print(f\"[{now_utc.strftime('%H:%M:%S')}] Failed to fetch or parse L2 data for sim step. L2 features will be NaN.\")\n",
        "                        self._log_simulation_action(now_utc, \"WARN\", {\"message\": \"L2 fetch/parse failed\"})\n",
        "                        # Initialize with NaNs if fetch failed but L2 is expected\n",
        "                        l2_features_calculated = {col: np.nan for col in self.l2_features}\n",
        "\n",
        "                # --- Prepare Features (including merging L2 if available) ---\n",
        "                # Calculate base features on the cleaned historical data\n",
        "                df_with_base_features = self._calculate_zscore_features(df_hist_clean)\n",
        "                df_with_base_features = self._calculate_spread_features(df_with_base_features)\n",
        "                if HAS_PYEMD: df_with_base_features = self._calculate_hht_features(df_with_base_features)\n",
        "\n",
        "                # Add L2 columns (ensures they exist, might be NaN initially)\n",
        "                if use_l2:\n",
        "                    for col in self.l2_features:\n",
        "                        if col not in df_with_base_features.columns:\n",
        "                            df_with_base_features[col] = np.nan\n",
        "                    # Merge the just-calculated L2 features into the *last* row\n",
        "                    if l2_features_calculated:\n",
        "                        last_index = df_with_base_features.index[-1]\n",
        "                        for key, value in l2_features_calculated.items():\n",
        "                            if key in df_with_base_features.columns:\n",
        "                                df_with_base_features.loc[last_index, key] = value\n",
        "\n",
        "                # Define active features again, including L2 if used\n",
        "                active_feature_list_sim = list(self.ohlcv_base_features)\n",
        "                if HAS_PYEMD: active_feature_list_sim.extend(self.hht_features)\n",
        "                if use_l2: active_feature_list_sim.extend(self.l2_features)\n",
        "                active_feature_list_sim = [f for f in active_feature_list_sim if f in df_with_base_features.columns]\n",
        "\n",
        "                # Drop rows with NaNs in active features (needed for model input)\n",
        "                # This step is crucial AFTER potentially merging L2 features\n",
        "                df_prepared = df_with_base_features.dropna(subset=active_feature_list_sim).copy()\n",
        "\n",
        "                if df_prepared is None or df_prepared.empty:\n",
        "                     print(f\"[{now_utc.strftime('%H:%M:%S')}] Feature prep failed or resulted in empty df after dropna. Wait...\")\n",
        "                     self._log_simulation_action(now_utc, \"WAIT\", {\"reason\": \"Feature prep failed/empty after dropna\"})\n",
        "                     if self.live_equity_history: self.live_equity_history.append(self.live_equity_history[-1]); self.live_timestamp_history.append(now_utc)\n",
        "                     elapsed_time = time.monotonic() - loop_start_time; sleep_time = max(0, loop_interval_seconds - elapsed_time)\n",
        "                     if sleep_time > 0: self.simulation_stop_event.wait(sleep_time)\n",
        "                     continue\n",
        "\n",
        "                # --- Predict Signal --- #\n",
        "                latest_features_df = df_prepared.iloc[-1:]\n",
        "                predicted_df = self.predict_signals(latest_features_df, threshold=threshold)\n",
        "\n",
        "                if predicted_df is None or predicted_df.empty:\n",
        "                    print(f\"[{now_utc.strftime('%H:%M:%S')}] Signal prediction failed. Wait...\")\n",
        "                    self._log_simulation_action(now_utc, \"WAIT\", {\"reason\": \"Signal prediction failed\"})\n",
        "                    if self.live_equity_history: self.live_equity_history.append(self.live_equity_history[-1]); self.live_timestamp_history.append(now_utc)\n",
        "                    elapsed_time = time.monotonic() - loop_start_time; sleep_time = max(0, loop_interval_seconds - elapsed_time)\n",
        "                    if sleep_time > 0: self.simulation_stop_event.wait(sleep_time)\n",
        "                    continue\n",
        "\n",
        "                # --- Execute Trading Logic --- #\n",
        "                current_signal = predicted_df[\"signal\"].iloc[0]; current_price = predicted_df[\"close\"].iloc[0]\n",
        "                current_timestamp = pd.Timestamp(predicted_df[\"timestamp\"].iloc[0]); pred_scaled = predicted_df[\"pred_scaled\"].iloc[0]\n",
        "                pred_unscaled = predicted_df[\"pred_unscaled_return\"].iloc[0] if 'pred_unscaled_return' in predicted_df.columns else np.nan\n",
        "                unrealized_pnl = 0; pos = self.current_live_position\n",
        "                if pos[\"side\"] is not None:\n",
        "                    pnl_per_asset = (current_price - pos[\"entry_price\"]) if pos[\"side\"] == \"long\" else (pos[\"entry_price\"] - current_price)\n",
        "                    unrealized_pnl = pnl_per_asset * pos[\"size\"]\n",
        "                current_equity = balance + unrealized_pnl; self.live_equity_history.append(current_equity); self.live_timestamp_history.append(current_timestamp)\n",
        "                exit_reason = None; exit_price = current_price; last_high = predicted_df[\"high\"].iloc[0]; last_low = predicted_df[\"low\"].iloc[0]\n",
        "                if pos[\"side\"] == \"long\":\n",
        "                    if stop_loss_pct and pos[\"sl_price\"] is not None and last_low <= pos[\"sl_price\"]: exit_reason = \"Stop Loss Hit\"; exit_price = pos[\"sl_price\"]; current_signal = -1\n",
        "                    elif take_profit_pct and pos[\"tp_price\"] is not None and last_high >= pos[\"tp_price\"]: exit_reason = \"Take Profit Hit\"; exit_price = pos[\"tp_price\"]; current_signal = -1\n",
        "                elif pos[\"side\"] == \"short\":\n",
        "                    if stop_loss_pct and pos[\"sl_price\"] is not None and last_high >= pos[\"sl_price\"]: exit_reason = \"Stop Loss Hit\"; exit_price = pos[\"sl_price\"]; current_signal = 1\n",
        "                    elif take_profit_pct and pos[\"tp_price\"] is not None and last_low <= pos[\"tp_price\"]: exit_reason = \"Take Profit Hit\"; exit_price = pos[\"tp_price\"]; current_signal = 1\n",
        "                executed_action = \"HOLD\" if pos[\"side\"] else \"FLAT\"; log_details = {\"price\": current_price, \"signal\": current_signal, \"pred_scaled\": pred_scaled, \"pred_unscaled\": pred_unscaled, \"equity_before\": current_equity}\n",
        "                # Include calculated L2 features in log if available\n",
        "                if l2_features_calculated: log_details.update({k: v for k, v in l2_features_calculated.items() if pd.notna(v)})\n",
        "\n",
        "                current_pos_side_int = 1 if pos[\"side\"] == \"long\" else -1 if pos[\"side\"] == \"short\" else 0\n",
        "                if pos[\"side\"] is not None and (current_signal == -current_pos_side_int or current_signal == 0 or exit_reason is not None):\n",
        "                    pnl_per_asset = (exit_price - pos[\"entry_price\"]) if pos[\"side\"] == \"long\" else (pos[\"entry_price\"] - exit_price)\n",
        "                    pnl_gross = pnl_per_asset * pos[\"size\"]; exit_value = exit_price * pos[\"size\"]; commission_exit = exit_value * commission_pct\n",
        "                    pnl_net = pnl_gross - commission_exit; balance += pnl_gross - commission_exit # Update balance only on exit\n",
        "                    executed_action = f\"EXIT {pos['side'].upper()}\"\n",
        "                    log_details.update({\"exit_reason\": exit_reason if exit_reason else \"Signal\", \"entry_price\": pos[\"entry_price\"], \"exit_price\": exit_price, \"size\": pos[\"size\"],\n",
        "                                        \"pnl_gross\": pnl_gross, \"commission_exit\": commission_exit, \"pnl_net_trade\": pnl_net, \"final_balance\": balance})\n",
        "                    print(f\"[{current_timestamp.strftime('%H:%M:%S')}] {executed_action} @{exit_price:.2f} | PnL Net:{pnl_net:.4f} | Reason:{log_details['exit_reason']} | Bal:{balance:.2f}\")\n",
        "                    self.current_live_position = {\"side\": None, \"entry_price\": 0.0, \"size\": 0.0, \"timestamp\": None, \"sl_price\": None, \"tp_price\": None}\n",
        "                elif pos[\"side\"] is None and current_signal != 0 and exit_reason is None:\n",
        "                    entry_side = \"long\" if current_signal == 1 else \"short\"; entry_price = current_price\n",
        "                    position_size_usd = current_equity * leverage; position_size_asset = position_size_usd / entry_price\n",
        "                    entry_value = entry_price * position_size_asset; commission_entry = entry_value * commission_pct\n",
        "                    if balance < commission_entry:\n",
        "                         print(f\"[{current_timestamp.strftime('%H:%M:%S')}] Insufficient balance ({balance:.2f}) for entry commission ({commission_entry:.2f}). Skip.\")\n",
        "                         self._log_simulation_action(current_timestamp, \"SKIP_ENTRY\", {\"reason\": \"Insufficient balance\", \"balance\": balance, \"commission\": commission_entry})\n",
        "                    else:\n",
        "                        balance -= commission_entry; sl_price, tp_price = None, None\n",
        "                        if entry_side == \"long\":\n",
        "                            if stop_loss_pct: sl_price = entry_price * (1 - stop_loss_pct)\n",
        "                            if take_profit_pct: tp_price = entry_price * (1 + take_profit_pct)\n",
        "                        else:\n",
        "                            if stop_loss_pct: sl_price = entry_price * (1 + stop_loss_pct)\n",
        "                            if take_profit_pct: tp_price = entry_price * (1 - take_profit_pct)\n",
        "                        self.current_live_position = {\"side\": entry_side, \"entry_price\": entry_price, \"size\": position_size_asset, \"timestamp\": current_timestamp, \"sl_price\": sl_price, \"tp_price\": tp_price}\n",
        "                        executed_action = f\"ENTER {entry_side.upper()}\"\n",
        "                        log_details.update({\"entry_price\": entry_price, \"size\": position_size_asset, \"commission_entry\": commission_entry, \"sl_price\": sl_price, \"tp_price\": tp_price, \"final_balance\": balance})\n",
        "                        print(f\"[{current_timestamp.strftime('%H:%M:%S')}] {executed_action} @{entry_price:.2f} | Size:{position_size_asset:.6f} | SL:{sl_price} | TP:{tp_price} | Bal:{balance:.2f}\")\n",
        "                # Recalculate unrealized PnL for logging after potential actions\n",
        "                pos_after = self.current_live_position; unrealized_pnl_after = 0\n",
        "                if pos_after[\"side\"] is not None:\n",
        "                    pnl_per_asset_after = (current_price - pos_after[\"entry_price\"]) if pos_after[\"side\"] == \"long\" else (pos_after[\"entry_price\"] - current_price)\n",
        "                    unrealized_pnl_after = pnl_per_asset_after * pos_after[\"size\"]\n",
        "                log_details[\"equity_after\"] = balance + unrealized_pnl_after # Log equity reflecting current balance and unrealized PnL\n",
        "                self._log_simulation_action(current_timestamp, executed_action, log_details)\n",
        "            except ccxt.NetworkError as e:\n",
        "                print(f\"[{now_utc.strftime('%H:%M:%S')}] SIM Network Error: {e}. Retry...\")\n",
        "                self._log_simulation_action(now_utc, \"ERROR\", {\"message\": f\"Network Error: {e}\"})\n",
        "                if self.live_equity_history: self.live_equity_history.append(self.live_equity_history[-1]); self.live_timestamp_history.append(now_utc)\n",
        "                time.sleep(loop_interval_seconds / 2)\n",
        "            except Exception as e:\n",
        "                print(f\"[{now_utc.strftime('%H:%M:%S')}] SIM Loop Error: {e}\"); traceback.print_exc()\n",
        "                self._log_simulation_action(now_utc, \"ERROR\", {\"message\": f\"Loop Error: {e}\", \"traceback\": traceback.format_exc()})\n",
        "                if self.live_equity_history: self.live_equity_history.append(self.live_equity_history[-1]); self.live_timestamp_history.append(now_utc)\n",
        "            elapsed_time = time.monotonic() - loop_start_time; sleep_time = max(0, loop_interval_seconds - elapsed_time)\n",
        "            if sleep_time > 0: self.simulation_stop_event.wait(sleep_time)\n",
        "        print(\"--- Live Simulation Loop Finished ---\")\n",
        "        if self.current_live_position[\"side\"] is not None:\n",
        "             print(\"Closing open position at end of simulation...\")\n",
        "             final_price_df = self.fetch_ohlcv(limit=1)\n",
        "             if final_price_df is not None and not final_price_df.empty:\n",
        "                 final_price = final_price_df['close'].iloc[-1]; final_timestamp = pd.Timestamp(final_price_df['timestamp'].iloc[-1])\n",
        "                 pos = self.current_live_position; pnl_per_asset = (final_price - pos[\"entry_price\"]) if pos[\"side\"] == \"long\" else (pos[\"entry_price\"] - final_price)\n",
        "                 pnl_gross = pnl_per_asset * pos[\"size\"]; exit_value = final_price * pos[\"size\"]; commission_exit = exit_value * commission_pct\n",
        "                 pnl_net = pnl_gross - commission_exit; balance += pnl_gross - commission_exit\n",
        "                 self.live_equity_history.append(balance); self.live_timestamp_history.append(final_timestamp)\n",
        "                 log_details = {\"exit_reason\": \"End of Sim\", \"entry_price\": pos[\"entry_price\"], \"exit_price\": final_price, \"size\": pos[\"size\"],\n",
        "                                \"pnl_gross\": pnl_gross, \"commission_exit\": commission_exit, \"pnl_net_trade\": pnl_net, \"final_balance\": balance}\n",
        "                 self._log_simulation_action(final_timestamp, f\"EXIT {pos['side'].upper()} (END)\", log_details)\n",
        "                 print(f\"  Closed {pos['side']} @{final_price:.2f}, PnL Net:{pnl_net:.4f}, Final Bal:{balance:.2f}\")\n",
        "             else:\n",
        "                 print(\"  Warning: Could not fetch final price to close position.\")\n",
        "                 self._log_simulation_action(datetime.now(timezone.utc), \"ERROR\", {\"message\": \"Failed to fetch final price for closing\"})\n",
        "                 if self.live_equity_history: self.live_equity_history.append(self.live_equity_history[-1]); self.live_timestamp_history.append(datetime.now(timezone.utc))\n",
        "        self.simulation_running = False\n",
        "\n",
        "    def run_live_simulation(self, initial_equity=None, threshold=None, fetch_limit=None, commission_pct=None, leverage=None, stop_loss_pct=None, take_profit_pct=None):\n",
        "        \"\"\"Starts the live simulation thread using config for defaults.\"\"\"\n",
        "        if self.simulation_running: print(\"Simulation already running.\"); return\n",
        "        if self.model is None or not self.trained_features:\n",
        "             print(\"Error: Model not loaded.\");\n",
        "             if not self.load_model(): return\n",
        "        if not self.exchange: print(\"Error: Exchange not initialized.\"); return\n",
        "\n",
        "        # Load parameters from config if not provided as args\n",
        "        initial_equity = initial_equity if initial_equity is not None else self.config.get('initial_balance', 10000)\n",
        "        threshold = threshold if threshold is not None else self.config.get('simulation_threshold', 0.5)\n",
        "        fetch_limit = fetch_limit if fetch_limit is not None else self.config.get('fetch_live_limit', 300)\n",
        "        commission_pct = commission_pct if commission_pct is not None else self.config.get('commission_pct', 0.0006)\n",
        "        leverage = leverage if leverage is not None else self.config.get('leverage', 1)\n",
        "        stop_loss_pct = stop_loss_pct if stop_loss_pct is not None else self.config.get('stop_loss_pct', None)\n",
        "        take_profit_pct = take_profit_pct if take_profit_pct is not None else self.config.get('take_profit_pct', None)\n",
        "\n",
        "        try:\n",
        "            loop_interval_seconds = self.exchange.parse_timeframe(self.timeframe); min_interval = 15\n",
        "            if loop_interval_seconds < min_interval: print(f\"Warning: Timeframe interval ({loop_interval_seconds}s) < {min_interval}s. Using {min_interval}s.\"); loop_interval_seconds = min_interval\n",
        "        except Exception as e: print(f\"Error parsing timeframe '{self.timeframe}'. Using 60s. Error: {e}\"); loop_interval_seconds = 60\n",
        "\n",
        "        self.simulation_running = True; self.simulation_stop_event.clear(); self.live_equity_history = []; self.live_timestamp_history = []\n",
        "        self.simulation_thread = threading.Thread(target=self._simulation_loop, kwargs={'initial_equity': initial_equity, 'threshold': threshold, 'fetch_limit': fetch_limit, 'loop_interval_seconds': loop_interval_seconds,\n",
        "                                                                                         'commission_pct': commission_pct, 'leverage': leverage, 'stop_loss_pct': stop_loss_pct, 'take_profit_pct': take_profit_pct}, daemon=True)\n",
        "        self.simulation_thread.start(); print(f\"Live simulation thread started (Interval: {loop_interval_seconds}s).\")\n",
        "\n",
        "    def stop_live_simulation(self, wait_time=15):\n",
        "        \"\"\"Stops the running live simulation thread.\"\"\"\n",
        "        if not self.simulation_running or self.simulation_thread is None: print(\"Simulation not running.\"); return\n",
        "        print(\"Attempting to stop simulation gracefully...\"); self.simulation_stop_event.set(); self.simulation_thread.join(timeout=wait_time)\n",
        "        if self.simulation_thread.is_alive(): print(f\"Warning: Simulation thread did not stop within {wait_time}s.\")\n",
        "        else: print(\"Simulation thread stopped.\")\n",
        "        self.simulation_running = False; self.simulation_thread = None\n",
        "        print(\"Plotting final simulation equity...\"); self.plot_simulation_equity()\n",
        "\n",
        "    def plot_simulation_equity(self):\n",
        "        \"\"\"Plots the equity curve from the live simulation.\"\"\"\n",
        "        if not self.live_equity_history or not self.live_timestamp_history: print(\"No simulation equity data.\"); return\n",
        "        if 'matplotlib' not in sys.modules or plt is None: print(\"Matplotlib not available.\"); return\n",
        "        try:\n",
        "            min_len = min(len(self.live_timestamp_history), len(self.live_equity_history))\n",
        "            if min_len <= 1: print(\"Not enough sim data to plot.\"); return\n",
        "            sim_timestamps = pd.to_datetime(self.live_timestamp_history[:min_len]); sim_equity = self.live_equity_history[:min_len]\n",
        "            final_sim_equity = sim_equity[-1]; initial_sim_equity = sim_equity[0]\n",
        "            sim_return_pct = ((final_sim_equity - initial_sim_equity) / initial_sim_equity * 100) if initial_sim_equity != 0 else 0\n",
        "            plt.figure(figsize=(14, 7)); plt.plot(sim_timestamps, sim_equity, label=\"Simulation Equity\", color='dodgerblue')\n",
        "            plt.fill_between(sim_timestamps, initial_sim_equity, sim_equity, where=np.array(sim_equity) >= initial_sim_equity, color='green', alpha=0.3, interpolate=True)\n",
        "            plt.fill_between(sim_timestamps, initial_sim_equity, sim_equity, where=np.array(sim_equity) < initial_sim_equity, color='red', alpha=0.3, interpolate=True)\n",
        "            plt.axhline(initial_sim_equity, color='grey', linestyle='--', label=f'Initial Equity (${initial_sim_equity:,.2f})')\n",
        "            plt.title(f\"Live Simulation Equity Curve: {self.symbol} {self.timeframe}\\nFinal Equity: ${final_sim_equity:,.2f} ({sim_return_pct:.2f}%)\")\n",
        "            plt.xlabel(\"Time\"); plt.ylabel(\"Simulated Equity (USD)\"); plt.grid(True, linestyle='--', alpha=0.6); plt.legend(); plt.tight_layout()\n",
        "            plt.savefig(self.simulation_equity_plot_path); print(f\"Simulation equity curve saved: {self.simulation_equity_plot_path}\"); plt.show()\n",
        "        except Exception as e: print(f\"Error plotting simulation equity: {e}\"); traceback.print_exc()\n",
        "\n",
        "\n",
        "    # ==========================================================================\n",
        "    # Visualization Methods\n",
        "    # ==========================================================================\n",
        "    def plot_feature_importance(self, model_booster=None, use_shap=None):\n",
        "        \"\"\"Plots feature importance using SHAP or LightGBM.\"\"\"\n",
        "        # Use config setting unless overridden\n",
        "        use_shap = use_shap if use_shap is not None else self.config.get('use_shap_override', True)\n",
        "\n",
        "        if use_shap and not HAS_SHAP: print(\"SHAP not available, using LightGBM.\"); use_shap = False\n",
        "        if 'matplotlib' not in sys.modules or plt is None: print(\"Matplotlib not available.\"); return\n",
        "        print(\"\\n--- Plotting Feature Importance ---\")\n",
        "        _model = model_booster or self.model\n",
        "        if _model is None:\n",
        "            if not self.load_model(): print(\"Error: Failed to load model.\"); return\n",
        "            _model = self.model\n",
        "        if not isinstance(_model, lgb.Booster): print(\"Error: Invalid model object.\"); return\n",
        "        model_feature_names = _model.feature_name()\n",
        "        if not model_feature_names: print(\"Error: Could not get feature names from model.\"); return\n",
        "\n",
        "        if use_shap:\n",
        "            print(\"Attempting SHAP plot...\")\n",
        "            try:\n",
        "                # Load only required columns from prepared data\n",
        "                load_cols = ['timestamp'] + model_feature_names\n",
        "                df_train = pd.read_csv(self.prepared_data_path, parse_dates=['timestamp'], usecols=lambda c: c in load_cols)\n",
        "                missing_data = [f for f in model_feature_names if f not in df_train.columns]\n",
        "                if missing_data: print(f\"Error: Missing features in data for SHAP: {missing_data}. Fallback.\"); self.plot_feature_importance(model_booster=_model, use_shap=False); return\n",
        "                X_shap = df_train[model_feature_names].dropna()\n",
        "                if X_shap.empty: print(\"Error: No valid data for SHAP after dropna. Fallback.\"); self.plot_feature_importance(model_booster=_model, use_shap=False); return\n",
        "                max_shap_samples = 5000\n",
        "                if len(X_shap) > max_shap_samples: print(f\"Sampling {max_shap_samples} rows for SHAP.\"); X_shap = X_shap.sample(max_shap_samples, random_state=42)\n",
        "                print(f\"Calculating SHAP values for {len(X_shap)} samples...\")\n",
        "                explainer = shap.Explainer(_model, X_shap); shap_values = explainer(X_shap)\n",
        "                # Plot SHAP Bar Plot\n",
        "                plt.figure(figsize=(10, max(6, len(model_feature_names) * 0.3))); shap.summary_plot(shap_values, X_shap, plot_type=\"bar\", show=False)\n",
        "                plt.title(f\"SHAP Importance (Mean Abs Value)\\n{self.symbol} {self.timeframe}\"); plt.tight_layout(); plt.savefig(self.shap_bar_plot_path); print(f\"SHAP bar plot saved: {self.shap_bar_plot_path}\"); plt.show(block=False)\n",
        "                # Plot SHAP Dot Plot\n",
        "                plt.figure(figsize=(10, max(6, len(model_feature_names) * 0.3))); shap.summary_plot(shap_values, X_shap, show=False)\n",
        "                plt.title(f\"SHAP Summary Plot\\n{self.symbol} {self.timeframe}\"); plt.tight_layout(); plt.savefig(self.shap_dot_plot_path); print(f\"SHAP dot plot saved: {self.shap_dot_plot_path}\"); plt.show(block=False); plt.pause(0.1)\n",
        "                return\n",
        "            except Exception as e: print(f\"Warning: SHAP plot failed: {e}. Fallback.\"); traceback.print_exc()\n",
        "\n",
        "        print(\"Using LightGBM importance plot.\")\n",
        "        try:\n",
        "            plt.figure(figsize=(10, max(6, len(model_feature_names) * 0.3))); lgb.plot_importance(_model, importance_type='gain', max_num_features=len(model_feature_names))\n",
        "            plt.title(f\"LightGBM Importance (Gain)\\n{self.symbol} {self.timeframe}\"); plt.tight_layout(); plt.savefig(self.lgbm_importance_plot_path); print(f\"LGBM plot saved: {self.lgbm_importance_plot_path}\"); plt.show(block=False); plt.pause(0.1)\n",
        "        except Exception as e: print(f\"Error plotting LGBM importance: {e}\")\n",
        "\n",
        "    def plot_emd_decomposition(self, data=None, use_saved_data=True):\n",
        "        \"\"\"Performs EMD using PyEMD and plots the IMFs manually.\"\"\"\n",
        "        if not HAS_PYEMD: print(\"Cannot plot EMD: PyEMD not available.\"); return\n",
        "        if 'matplotlib' not in sys.modules or plt is None: print(\"Matplotlib not available.\"); return\n",
        "        print(\"\\n--- Plotting EMD Decomposition (using PyEMD) ---\")\n",
        "        plot_df = None; source_description = \"\"\n",
        "        if data is not None and isinstance(data, pd.DataFrame) and 'close' in data and 'timestamp' in data:\n",
        "            plot_df = data.copy(); source_description = f\"provided DataFrame ({len(plot_df)} pts)\"\n",
        "        elif use_saved_data:\n",
        "            try: plot_df = pd.read_csv(self.prepared_data_path, parse_dates=['timestamp'], usecols=['timestamp', 'close']); source_description = f\"saved file ({len(plot_df)} pts)\"\n",
        "            except Exception as e: print(f\"Error loading data for EMD plot: {e}\"); return\n",
        "        else: print(\"Error: No data for EMD plot.\"); return\n",
        "        if plot_df is None or plot_df.empty or 'close' not in plot_df or 'timestamp' not in plot_df: print(\"Error: Invalid DataFrame for EMD plot.\"); return\n",
        "        print(f\"Using data from {source_description}.\")\n",
        "        signal = plot_df['close'].values.astype(float); time_vector = plot_df['timestamp'].values\n",
        "        if signal is None or len(signal) < 20: print(\"Error: Not enough data for EMD.\"); return\n",
        "        print(\"Performing EMD for visualization (using PyEMD)...\")\n",
        "        try:\n",
        "            emd = EMD(); imfs_residue = emd(signal)\n",
        "            if imfs_residue is None or not isinstance(imfs_residue, np.ndarray) or imfs_residue.shape[0] < 1: print(\"Error: PyEMD EMD failed.\"); return\n",
        "            num_components = imfs_residue.shape[0]; num_imfs = num_components - 1\n",
        "            print(f\"EMD complete. Plotting {num_imfs} IMFs and residue...\")\n",
        "            fig, axes = plt.subplots(num_components + 1, 1, figsize=(12, 2 * (num_components + 1)), sharex=True)\n",
        "            axes[0].plot(time_vector, signal, color='grey', linewidth=1.5); axes[0].set_ylabel(\"Original\"); axes[0].grid(True, linestyle='--', alpha=0.6)\n",
        "            for i in range(num_components):\n",
        "                label = f\"IMF {i+1}\" if i < num_imfs else \"Residue\"; axes[i+1].plot(time_vector, imfs_residue[i, :], linewidth=1)\n",
        "                axes[i+1].set_ylabel(label); axes[i+1].grid(True, linestyle='--', alpha=0.6)\n",
        "            axes[-1].set_xlabel(\"Time\"); fig.suptitle(f\"EMD Decomposition ({num_imfs} IMFs + Residue) - PyEMD\\n{self.symbol} {self.timeframe}\")\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "            plt.savefig(self.imf_plot_path, dpi=150); print(f\"EMD IMF plot saved: {self.imf_plot_path}\"); plt.show(block=False); plt.pause(0.1)\n",
        "        except Exception as e: print(f\"Error during PyEMD EMD or plotting: {e}\"); traceback.print_exc()\n",
        "\n",
        "    def plot_features_vs_price(self, df=None, use_saved_data=True, features_to_plot=None):\n",
        "        \"\"\"Plots selected features against the close price.\"\"\"\n",
        "        if 'matplotlib' not in sys.modules or plt is None: print(\"Matplotlib not available.\"); return\n",
        "        print(\"\\n--- Plotting Features vs Price ---\")\n",
        "        # Determine default features if none provided\n",
        "        if features_to_plot is None:\n",
        "            features_to_plot = []\n",
        "            if HAS_PYEMD and self.hht_features: features_to_plot.extend(self.hht_features)\n",
        "            # Only include L2 features if they are expected to be non-NaN (e.g., from sim or pre-merged data)\n",
        "            if self.config.get('use_l2_features', False) and self.l2_features: features_to_plot.extend(self.l2_features)\n",
        "            # Fallback to z-scores if no HHT/L2\n",
        "            if not features_to_plot: features_to_plot = [f for f in self.ohlcv_base_features if f.startswith('z_')]\n",
        "            if not features_to_plot: print(\"No default features available to plot.\"); return\n",
        "\n",
        "        required_cols_base = ['timestamp', 'close']; plot_df = None; source_description = \"\"\n",
        "        if df is not None and isinstance(df, pd.DataFrame): plot_df = df.copy(); source_description = f\"provided DataFrame ({len(plot_df)} pts)\"\n",
        "        elif use_saved_data:\n",
        "            try:\n",
        "                # Load all potentially plottable features plus base columns\n",
        "                all_potential_features = list(set(self.hht_features + self.l2_features + self.ohlcv_base_features + features_to_plot))\n",
        "                load_cols = list(set(required_cols_base + all_potential_features))\n",
        "                plot_df = pd.read_csv(self.prepared_data_path, parse_dates=['timestamp'], usecols=lambda c: c in load_cols)\n",
        "                source_description = f\"saved file ({len(plot_df)} pts)\"\n",
        "            except Exception as e: print(f\"Error loading data for feature plot: {e}\"); return\n",
        "        else: print(\"Error: No data source for feature plot.\"); return\n",
        "\n",
        "        if plot_df is None or plot_df.empty: print(\"Error: DataFrame empty for plot.\"); return\n",
        "\n",
        "        # Filter to features actually present in the loaded data\n",
        "        actual_features_to_plot = [f for f in features_to_plot if f in plot_df.columns]\n",
        "        if not actual_features_to_plot: print(f\"Error: None of requested features {features_to_plot} found in the loaded data.\"); return\n",
        "        if len(actual_features_to_plot) < len(features_to_plot): print(f\"Warning: Plotting only available features: {actual_features_to_plot}\")\n",
        "\n",
        "        required_cols = required_cols_base + actual_features_to_plot; print(f\"Plotting {actual_features_to_plot} vs Price using {source_description}.\")\n",
        "\n",
        "        # Drop rows where *any* of the required columns for plotting are NaN\n",
        "        # This is important as L2 features might be NaN in historical data\n",
        "        plot_df.dropna(subset=required_cols, inplace=True)\n",
        "        if plot_df.empty: print(\"Error: DataFrame empty after dropping NaNs from required plot columns (check L2 availability in source).\"); return\n",
        "\n",
        "        try:\n",
        "            num_plots = 1 + len(actual_features_to_plot); fig, axes = plt.subplots(num_plots, 1, figsize=(14, 3 * num_plots), sharex=True)\n",
        "            if num_plots == 1: axes = [axes]; axes = np.array(axes).flatten() # Ensure axes is always iterable\n",
        "\n",
        "            # Plot Price\n",
        "            axes[0].plot(plot_df['timestamp'], plot_df['close'], label='Close Price', color='blue', linewidth=1.5); axes[0].set_ylabel('Price (USD)')\n",
        "            axes[0].set_title(f'Price & Feature Interaction ({self.symbol} {self.timeframe})'); axes[0].legend(loc='upper left'); axes[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "            # Plot Features\n",
        "            colors = plt.cm.viridis(np.linspace(0, 0.9, len(actual_features_to_plot)))\n",
        "            for i, feature in enumerate(actual_features_to_plot):\n",
        "                ax_idx = i + 1; axes[ax_idx].plot(plot_df['timestamp'], plot_df[feature], label=feature, color=colors[i], linewidth=1)\n",
        "                ylabel = feature.replace('_', ' ').replace('hht', 'HHT').replace('z ', 'Z-Score ').replace('l2 ', 'L2 ').title(); axes[ax_idx].set_ylabel(ylabel)\n",
        "                axes[ax_idx].legend(loc='upper left'); axes[ax_idx].grid(True, linestyle='--', alpha=0.6)\n",
        "                # Add zero line for certain features\n",
        "                if any(k in feature for k in ['z_', 'freq', 'accel', 'jerk', 'obi']):\n",
        "                     axes[ax_idx].axhline(0, color='grey', linestyle=':', linewidth=1)\n",
        "\n",
        "            axes[-1].set_xlabel(\"Time\"); fig.align_ylabels(axes); plt.tight_layout()\n",
        "            plt.savefig(self.feature_plot_path, dpi=150); print(f\"Feature vs Price plot saved: {self.feature_plot_path}\"); plt.show(block=False); plt.pause(0.1)\n",
        "        except Exception as e: print(f\"Error plotting features vs price: {e}\"); traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVk0gvv5YYI5"
      },
      "source": [
        "## 6. Main Execution Workflow (Main Bot)\n",
        "\n",
        "Instantiate the bot using the loaded configuration and run the main steps: data fetching, feature preparation, model training, backtesting, and optional simulation.\n",
        "\n",
        "*(Note: L2 features are fetched via REST API snapshots only during the live simulation loop. For historical backtesting/training, L2 features will be NaN unless a separate historical L2 data source is manually merged before the `prepare_features` step.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4ajPGzuYYI5",
        "outputId": "5ce6b395-7444-427d-9e1d-7b5d30473490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Initializing Bot ---\n",
            "Initializing CCXT Bybit exchange...\n",
            "  -> Using provided API Key for authentication.\n",
            "  -> Loading markets...\n",
            "FATAL: Unexpected error initializing CCXT: bybit GET https://api.bybit.com/v5/asset/coin/query-info? 403 Forbidden <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
            "<HTML><HEAD><META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\">\n",
            "<TITLE>ERROR: The request could not be satisfied</TITLE>\n",
            "</HEAD><BODY>\n",
            "<H1>403 ERROR</H1>\n",
            "<H2>The request could not be satisfied.</H2>\n",
            "<HR noshade size=\"1px\">\n",
            "The Amazon CloudFront distribution is configured to block access from your country.\n",
            "We can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner.\n",
            "<BR clear=\"all\">\n",
            "If you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.\n",
            "<BR clear=\"all\">\n",
            "<HR noshade size=\"1px\">\n",
            "<PRE>\n",
            "Generated by cloudfront (CloudFront)\n",
            "Request ID: ge8_k6uMhPOXdxY6akLpfk__9DcOwldx2Bo95rT_maGQfOcckeBSsw==\n",
            "</PRE>\n",
            "<ADDRESS>\n",
            "</ADDRESS>\n",
            "</BODY></HTML>\n",
            "\n",
            "Bot Initialized:\n",
            "  - Base Directory: /content/drive/MyDrive/trading_bot_project_v2\n",
            "  - Symbol: BTC/USDT\n",
            "  - Timeframe: 1m\n",
            "  - Use L2 Features: True\n",
            "  - L2 Raw Data Path: /content/drive/MyDrive/trading_bot_project_v2/l2_data/btcusdt_l2_data_5m.jsonl\n",
            "  - PyEMD Available: True\n",
            "  - SHAP Available: True\n",
            "  - Exchange Initialized: No\n",
            "------------------------------\n",
            "\n",
            "Failed to initialize CCXT exchange in Bot class. Cannot proceed with execution.\n",
            "\n",
            "Skipping Data Fetch/Preparation (Bot not initialized).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ccxt/base/exchange.py\", line 581, in fetch\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://api.bybit.com/v5/asset/coin/query-info\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-5-0bf26a5c42b9>\", line 105, in __init__\n",
            "    self.exchange.load_markets() # Test connection\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ccxt/base/exchange.py\", line 1516, in load_markets\n",
            "    currencies = self.fetch_currencies()\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ccxt/bybit.py\", line 1596, in fetch_currencies\n",
            "    response = self.privateGetV5AssetCoinQueryInfo(params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ccxt/base/types.py\", line 35, in unbound_method\n",
            "    return _self.request(self.path, self.api, self.method, params, config=self.config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ccxt/base/exchange.py\", line 4468, in request\n",
            "    return self.fetch2(path, api, method, params, headers, body, config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ccxt/base/exchange.py\", line 4462, in fetch2\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ccxt/base/exchange.py\", line 4453, in fetch2\n",
            "    return self.fetch(request['url'], request['method'], request['headers'], request['body'])\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ccxt/base/exchange.py\", line 599, in fetch\n",
            "    self.handle_http_status_code(http_status_code, http_status_text, url, method, http_response)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ccxt/base/exchange.py\", line 1695, in handle_http_status_code\n",
            "    raise ErrorClass(self.id + ' ' + method + ' ' + url + ' ' + codeAsString + ' ' + reason + ' ' + body)\n",
            "ccxt.base.errors.RateLimitExceeded: bybit GET https://api.bybit.com/v5/asset/coin/query-info? 403 Forbidden <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
            "<HTML><HEAD><META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\">\n",
            "<TITLE>ERROR: The request could not be satisfied</TITLE>\n",
            "</HEAD><BODY>\n",
            "<H1>403 ERROR</H1>\n",
            "<H2>The request could not be satisfied.</H2>\n",
            "<HR noshade size=\"1px\">\n",
            "The Amazon CloudFront distribution is configured to block access from your country.\n",
            "We can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner.\n",
            "<BR clear=\"all\">\n",
            "If you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.\n",
            "<BR clear=\"all\">\n",
            "<HR noshade size=\"1px\">\n",
            "<PRE>\n",
            "Generated by cloudfront (CloudFront)\n",
            "Request ID: ge8_k6uMhPOXdxY6akLpfk__9DcOwldx2Bo95rT_maGQfOcckeBSsw==\n",
            "</PRE>\n",
            "<ADDRESS>\n",
            "</ADDRESS>\n",
            "</BODY></HTML>\n"
          ]
        }
      ],
      "source": [
        "# Ensure config is loaded and exchange initialized before proceeding\n",
        "if 'config' not in locals() or not config:\n",
        "    print(\"ERROR: Configuration not loaded. Please run the config and import cells.\")\n",
        "    # Or raise an error: raise ValueError(\"Configuration not loaded.\")\n",
        "    bot_initialized = False # Ensure flag is set if config fails\n",
        "else:\n",
        "    # --- 1. Initialize the Bot ---\n",
        "    print(\"\\n--- Initializing Bot ---\")\n",
        "    bot = CombinedTradingBot(\n",
        "        config=config, # Pass the loaded config dictionary\n",
        "        api_key=BYBIT_API_KEY,      # Pass the loaded API key\n",
        "        api_secret=BYBIT_API_SECRET # Pass the loaded API secret\n",
        "    )\n",
        "\n",
        "    # --- Exit if Exchange Initialization Failed ---\n",
        "    if not bot.exchange:\n",
        "        print(\"\\nFailed to initialize CCXT exchange in Bot class. Cannot proceed with execution.\")\n",
        "        # Optionally raise an error or handle differently\n",
        "        # sys.exit(1) # Or handle more gracefully depending on desired notebook flow\n",
        "        bot_initialized = False\n",
        "    else:\n",
        "        bot_initialized = True\n",
        "\n",
        "# --- 2. Fetch and Prepare Data ---\n",
        "prepared_data = None # Initialize\n",
        "if bot_initialized:\n",
        "    print(\"\\n--- Fetching & Preparing Data ---\")\n",
        "    # Use fetch limit from config\n",
        "    raw_data = bot.fetch_ohlcv(limit=config.get('fetch_ohlcv_limit'))\n",
        "    if raw_data is not None and not raw_data.empty:\n",
        "        # *** IMPORTANT: Historical L2 Data Pre-processing Step Needed Here ***\n",
        "        # If config['use_l2_features'] is True AND you want L2 features for\n",
        "        # backtesting/training, you MUST implement logic here to fetch/load\n",
        "        # HISTORICAL L2 snapshots (e.g., from the .jsonl file created by the collector)\n",
        "        # and merge them into 'raw_data' (e.g., adding 'l2_bids', 'l2_asks' columns\n",
        "        # aligned by timestamp) before passing it to bot.prepare_features.\n",
        "        # Fetching historical L2 via REST API in prepare_features is impractical.\n",
        "        if config.get('use_l2_features', False):\n",
        "            print(\"*** L2 features enabled. Ensure HISTORICAL L2 data ('l2_bids', 'l2_asks') is merged ***\")\n",
        "            print(f\"*** into the DataFrame MANUALLY (e.g., from {bot.l2_raw_data_path}) before this step ***\")\n",
        "            print(\"*** if needed for training/backtesting. Otherwise, L2 features will be NaN. ***\")\n",
        "\n",
        "        # Prepare features (will add L2 columns as NaN if enabled and data not pre-merged)\n",
        "        prepared_data = bot.prepare_features(raw_data, save=True)\n",
        "\n",
        "        if prepared_data is None or prepared_data.empty:\n",
        "            print(\"\\nData preparation resulted in empty dataframe. Check feature calculation and NaN handling.\")\n",
        "            # Handle error - maybe stop execution for this run\n",
        "        else:\n",
        "             print(f\"\\nData preparation complete. Shape: {prepared_data.shape}\")\n",
        "             print(f\"Columns: {prepared_data.columns.tolist()}\")\n",
        "             # Check if L2 features have non-NaN values if they were supposed to be used\n",
        "             if config.get('use_l2_features', False):\n",
        "                 l2_cols_in_data = [f for f in bot.l2_features if f in prepared_data.columns]\n",
        "                 if l2_cols_in_data:\n",
        "                     nan_counts = prepared_data[l2_cols_in_data].isnull().sum()\n",
        "                     if nan_counts.all(): # Check if ALL values in ALL L2 columns are NaN\n",
        "                         print(\"*** NOTE: L2 features are enabled, but all values are NaN in the prepared historical data. ***\")\n",
        "                         print(\"*** This is expected if historical L2 data was not manually merged beforehand. ***\")\n",
        "                     elif nan_counts.any(): # Check if SOME values are NaN\n",
        "                         print(\"*** NOTE: Some NaN values found in L2 features in prepared historical data. ***\")\n",
        "                     else:\n",
        "                         print(\"L2 feature columns appear to have non-NaN values in prepared historical data (was it pre-merged?).\")\n",
        "                 else:\n",
        "                      print(\"*** WARNING: L2 features enabled in config, but no L2 columns found in prepared data. ***\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nFailed to fetch initial OHLCV data. Cannot proceed.\")\n",
        "        # Handle error\n",
        "else:\n",
        "    print(\"\\nSkipping Data Fetch/Preparation (Bot not initialized).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ONqnyrtYYI5",
        "outputId": "a6edb2b9-d9ca-498b-c2c0-8bde28997d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Skipping Data Visualization (Bot not initialized or no prepared data).\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Visualize Data (if prepared) ---\n",
        "if 'bot' in locals() and bot_initialized and prepared_data is not None and not prepared_data.empty:\n",
        "    print(\"\\n--- Visualizing Data ---\")\n",
        "    # Plot EMD if PyEMD is available\n",
        "    if HAS_PYEMD:\n",
        "        bot.plot_emd_decomposition(data=prepared_data)\n",
        "    else:\n",
        "         print(\"\\nSkipping EMD plot: PyEMD not available.\")\n",
        "\n",
        "    # Plot Features vs Price\n",
        "    # Default features determined inside the function based on config and availability\n",
        "    # Note: L2 features will only plot if they have non-NaN values (i.e., were pre-merged)\n",
        "    bot.plot_features_vs_price(df=prepared_data)\n",
        "else:\n",
        "    print(\"\\nSkipping Data Visualization (Bot not initialized or no prepared data).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5xVezJ0YYI5"
      },
      "outputs": [],
      "source": [
        "# --- 4. Train Model ---\n",
        "trained_model_booster = None # Initialize\n",
        "if 'bot' in locals() and bot_initialized and prepared_data is not None and not prepared_data.empty:\n",
        "    print(\"\\n--- Training Model ---\")\n",
        "    # train_model uses config['optuna_trials'], config['test_size'] internally\n",
        "    # It will determine features based on config['use_l2_features'] and data availability\n",
        "    # It will WARN if L2 features are enabled but NaN in training data.\n",
        "    trained_model_booster = bot.train_model(df=prepared_data, save=True)\n",
        "    if not trained_model_booster:\n",
        "        print(\"\\nModel training failed.\")\n",
        "        # Handle error\n",
        "    else:\n",
        "         print(\"\\nModel training successful.\")\n",
        "\n",
        "         # --- 5. Plot Feature Importance ---\n",
        "         print(\"\\n--- Plotting Feature Importance ---\")\n",
        "         # plot_feature_importance uses config['use_shap_override'] internally\n",
        "         bot.plot_feature_importance(model_booster=trained_model_booster)\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Model Training (Bot not initialized or no prepared data).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2PIk45WYYI5"
      },
      "outputs": [],
      "source": [
        "# --- 6. Run Backtest ---\n",
        "if 'bot' in locals() and bot_initialized and trained_model_booster is not None:\n",
        "    print(\"\\n--- Running Backtest ---\")\n",
        "    # backtest uses config for defaults (threshold, commission, leverage, sl/tp)\n",
        "    # It will load the prepared data and predict signals using the trained model\n",
        "    # Note: L2 features will likely be NaN during backtest unless historical L2 was pre-merged.\n",
        "    backtest_results_df, trades_log_df = bot.backtest(df=None) # Pass df=None to force loading/predicting\n",
        "\n",
        "    if trades_log_df is not None and not trades_log_df.empty:\n",
        "          print(\"\\nBacktest Trades Log Sample:\")\n",
        "          # Display more rows if needed\n",
        "          with pd.option_context('display.max_rows', 10, 'display.max_columns', None, 'display.width', 1000):\n",
        "               print(trades_log_df.head())\n",
        "    elif trades_log_df is not None: # Check if it's an empty DataFrame\n",
        "          print(\"\\nNo trades executed during backtest.\")\n",
        "    else: # Backtest failed (returned None, None)\n",
        "         print(\"\\nBacktest did not produce results. Check logs for errors.\")\n",
        "else:\n",
        "    print(\"\\nSkipping Backtest (Bot not initialized or model not trained).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC4bYWcJYYI5"
      },
      "outputs": [],
      "source": [
        "# --- 7. Run Live Simulation (Optional) ---\n",
        "if 'bot' in locals() and bot_initialized and trained_model_booster is not None:\n",
        "    print(\"\\n--- Live Simulation Section ---\")\n",
        "    run_simulation = config.get('run_simulation_flag', False) # Get from config\n",
        "    sim_duration = config.get('simulation_duration_seconds', 300)\n",
        "\n",
        "    if run_simulation:\n",
        "        if not bot.api_key or not bot.api_secret:\n",
        "             print(\"\\n*** WARNING: Live simulation started without API keys. Real trading actions would fail. ***\")\n",
        "             # L2 fetching might also fail without API keys depending on exchange\n",
        "             if config.get('use_l2_features', False):\n",
        "                  print(\"*** L2 fetching in simulation might fail without API keys. ***\")\n",
        "\n",
        "        # Check if L2 features are enabled and if fetching is possible\n",
        "        if config.get('use_l2_features', False):\n",
        "            if bot.exchange and bot.exchange.has.get('fetchL2OrderBook'):\n",
        "                print(\"*** Live Simulation will attempt to fetch L2 snapshots and calculate L2 features. ***\")\n",
        "            else:\n",
        "                 print(\"*** WARNING: L2 features enabled, but exchange/CCXT does not support fetchL2OrderBook. L2 features will be NaN in simulation. ***\")\n",
        "\n",
        "        print(f\"Attempting to run live simulation for approx {sim_duration} seconds...\")\n",
        "        # run_live_simulation uses config for defaults\n",
        "        bot.run_live_simulation()\n",
        "        try:\n",
        "            start_sim_time = time.time()\n",
        "            while bot.simulation_running and (time.time() - start_sim_time) < sim_duration:\n",
        "                print(f\"Simulation running... (Time elapsed: {int(time.time() - start_sim_time)}s / {sim_duration}s)\", end='\\r')\n",
        "                time.sleep(5)\n",
        "            print(\"\\nSimulation duration reached or stop signaled.\")\n",
        "        except KeyboardInterrupt: print(\"\\nKeyboard interrupt detected. Stopping simulation...\")\n",
        "        finally:\n",
        "            if bot.simulation_running: bot.stop_live_simulation() # Ensure simulation stops cleanly\n",
        "    else:\n",
        "        print(\"RUN_SIMULATION_FLAG is set to False in config. Skipping live simulation.\")\n",
        "else:\n",
        "    print(\"\\nSkipping Simulation (Bot not initialized or model not trained).\")\n",
        "\n",
        "\n",
        "print(\"\\n=\"*50)\n",
        "print(\"Notebook Execution Finished\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Keep plots open if not in interactive mode (e.g., running as script)\n",
        "# Check if plt was successfully imported\n",
        "# if 'plt' in locals() and plt is not None:\n",
        "#      INTERACTIVE_MODE = 'ipykernel' in sys.modules\n",
        "#      if not INTERACTIVE_MODE:\n",
        "#          print(\"Displaying plots. Close plot windows to exit.\")\n",
        "#          plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}