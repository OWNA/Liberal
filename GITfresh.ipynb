{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OWNA/Liberal/blob/main/GITfresh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup Environment (ULTRA CLEAN INSTALL ATTEMPT)\n",
        "\n",
        "# --- Step 1: Aggressively Uninstall Key Libraries ---\n",
        "print(\"Aggressively uninstalling numpy, scipy, pandas, lightgbm, EMD-signal, pandas-ta, shap, scikit-learn...\")\n",
        "!pip uninstall numpy -y --quiet\n",
        "!pip uninstall scipy -y --quiet\n",
        "!pip uninstall pandas -y --quiet\n",
        "!pip uninstall lightgbm -y --quiet\n",
        "!pip uninstall EMD-signal -y --quiet\n",
        "!pip uninstall pandas-ta -y --quiet\n",
        "!pip uninstall shap -y --quiet\n",
        "!pip uninstall scikit-learn -y --quiet\n",
        "\n",
        "# --- Step 2: Install NumPy pinned to <2.0 FIRST ---\n",
        "print(\"\\nInstalling NumPy <2.0 (target 1.26.4)...\")\n",
        "!pip install \"numpy==1.26.4\" --quiet # Force specific 1.x version\n",
        "\n",
        "# --- Step 3: Install specific compatible versions of Pandas and SciPy ---\n",
        "print(\"\\nInstalling specific compatible versions of Pandas and SciPy...\")\n",
        "!pip install \"pandas==2.0.3\" --quiet # Known to work well with NumPy 1.26.x\n",
        "!pip install \"scipy==1.11.4\" --quiet  # Known to work well with NumPy 1.26.x & Pandas 2.0.x\n",
        "\n",
        "# --- Step 4: Install LightGBM and scikit-learn ---\n",
        "print(\"\\nInstalling LightGBM and scikit-learn...\")\n",
        "!pip install lightgbm --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "\n",
        "\n",
        "# --- Step 5: Install the rest of the libraries (EMD-signal pinned) ---\n",
        "print(\"\\nInstalling remaining libraries (EMD-signal pinned)...\")\n",
        "!pip install ccxt optuna shap \"EMD-signal<1.4.0\" matplotlib pyyaml websocket-client dill==0.3.7 pandas_ta --quiet\n",
        "\n",
        "# --- Step 6: Mount Google Drive ---\n",
        "print(\"\\nMounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Step 7: Define Base Directory for the Project ---\n",
        "BOT_BASE_DIR = '/content/drive/MyDrive/trading_bot_project_v2/' # !!! YOUR PATH HERE !!!\n",
        "import os\n",
        "os.makedirs(BOT_BASE_DIR, exist_ok=True)\n",
        "os.environ['BOT_BASE_DIR'] = BOT_BASE_DIR\n",
        "print(f\"BOT_BASE_DIR set to: {BOT_BASE_DIR}\")\n",
        "import sys\n",
        "if BOT_BASE_DIR not in sys.path:\n",
        "    sys.path.append(BOT_BASE_DIR)\n",
        "    print(f\"Added {BOT_BASE_DIR} to sys.path for module imports.\")\n",
        "\n",
        "# --- Step 8: Check Python and Library Versions ---\n",
        "print(f\"\\n--- Library Versions ---\")\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "try:\n",
        "    import importlib.metadata\n",
        "\n",
        "    libs_to_check = {\n",
        "        \"ccxt\": \"CCXT\", \"lightgbm\": \"LightGBM\", \"pandas\": \"Pandas\",\n",
        "        \"numpy\": \"NumPy\", \"optuna\": \"Optuna\", \"shap\": \"SHAP\",\n",
        "        \"EMD-signal\": \"EMD-signal\", \"matplotlib\": \"Matplotlib\", \"scipy\": \"SciPy\",\n",
        "        \"PyYAML\": \"PyYAML\", \"websocket-client\": \"websocket-client\", \"dill\": \"Dill\",\n",
        "        \"scikit-learn\": \"scikit-learn\", \"pandas_ta\": \"pandas_ta\"\n",
        "    }\n",
        "    for lib_pkg_name, display_name in libs_to_check.items():\n",
        "        try:\n",
        "            version = importlib.metadata.version(lib_pkg_name)\n",
        "            print(f\"{display_name}: {version}\")\n",
        "        except importlib.metadata.PackageNotFoundError:\n",
        "            print(f\"{display_name}: Not installed or version not found.\")\n",
        "        except Exception as e_ver:\n",
        "            print(f\"Error getting version for {display_name}: {e_ver}\")\n",
        "except ImportError:\n",
        "    print(\"Could not import 'importlib.metadata'. Manual version checks might be needed.\")\n",
        "except Exception as e_outer:\n",
        "    print(f\"An error occurred during library version checking: {e_outer}\")\n",
        "print(\"------------------------\")\n",
        "\n",
        "print(\"\\nEnvironment setup complete (ULTRA CLEAN Install Attempt).\")\n",
        "print(\"IMPORTANT: Ensure all your custom .py files are in the BOT_BASE_DIR and contain plain Python code.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQFl0DYZsnvB",
        "outputId": "40a8172f-fdb8-4d6b-e96b-ca844749f7e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggressively uninstalling numpy, scipy, pandas, lightgbm, EMD-signal, pandas-ta, shap, scikit-learn...\n",
            "\u001b[33mWARNING: Skipping EMD-signal as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pandas-ta as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "Installing NumPy <2.0 (target 1.26.4)...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.28.0 requires pandas>=1.1.4, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "xarray 2025.1.2 requires pandas>=2.1, which is not installed.\n",
            "pytensor 2.27.1 requires scipy<2,>=1, which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "bqplot 0.12.44 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "cvxpy 1.6.2 requires scipy>=1.11.0, which is not installed.\n",
            "db-dtypes 1.4.1 requires pandas>=0.24.2, which is not installed.\n",
            "cuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "fastai 2.7.18 requires pandas, which is not installed.\n",
            "fastai 2.7.18 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.18 requires scipy, which is not installed.\n",
            "albumentations 2.0.5 requires scipy>=1.10.0, which is not installed.\n",
            "clarabel 0.10.0 requires scipy, which is not installed.\n",
            "scs 3.2.7.post2 requires scipy, which is not installed.\n",
            "imgaug 0.4.0 requires scipy, which is not installed.\n",
            "bigquery-magics 0.6.0 requires pandas>=1.1.0, which is not installed.\n",
            "cmdstanpy 1.2.5 requires pandas, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "jaxlib 0.4.33 requires scipy>=1.10, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "bokeh 3.6.3 requires pandas>=1.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "xarray-einstats 0.8.0 requires scipy>=1.9, which is not installed.\n",
            "pymc 5.20.1 requires pandas>=0.24.0, which is not installed.\n",
            "pymc 5.20.1 requires scipy>=1.4.1, which is not installed.\n",
            "osqp 0.6.7.post3 requires scipy>=0.13.2, which is not installed.\n",
            "panel 1.6.1 requires pandas>=1.2, which is not installed.\n",
            "arviz 0.20.0 requires pandas>=1.5.0, which is not installed.\n",
            "arviz 0.20.0 requires scipy>=1.9.0, which is not installed.\n",
            "qdldl 0.1.7.post5 requires scipy>=0.13.2, which is not installed.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, which is not installed.\n",
            "dask-cuda 25.2.0 requires pandas>=1.3, which is not installed.\n",
            "bigframes 1.38.0 requires pandas>=1.5.3, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "ibis-framework 9.2.0 requires pandas<3,>=1.5.3, which is not installed.\n",
            "jax 0.4.33 requires scipy>=1.10, which is not installed.\n",
            "yfinance 0.2.54 requires pandas>=1.3.0, which is not installed.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, which is not installed.\n",
            "mizani 0.13.1 requires scipy>=1.8.0, which is not installed.\n",
            "holoviews 1.20.1 requires pandas>=1.3, which is not installed.\n",
            "datascience 0.17.6 requires pandas, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "geopandas 1.0.1 requires pandas>=1.4.0, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "umap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\n",
            "librosa 0.10.2.post1 requires scikit-learn>=0.20.0, which is not installed.\n",
            "librosa 0.10.2.post1 requires scipy>=1.2.0, which is not installed.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "xgboost 2.1.4 requires scipy, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "prophet 1.1.6 requires pandas>=1.0.4, which is not installed.\n",
            "geemap 0.35.2 requires pandas, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Installing specific compatible versions of Pandas and SciPy...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "fastai 2.7.18 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.18 requires scipy, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "xarray-einstats 0.8.0 requires scipy>=1.9, which is not installed.\n",
            "pymc 5.20.1 requires scipy>=1.4.1, which is not installed.\n",
            "arviz 0.20.0 requires scipy>=1.9.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "mizani 0.13.1 requires scipy>=1.8.0, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "fastai 2.7.18 requires scikit-learn, which is not installed.\n",
            "sentence-transformers 3.4.1 requires scikit-learn, which is not installed.\n",
            "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "librosa 0.10.2.post1 requires scikit-learn>=0.20.0, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Installing LightGBM and scikit-learn...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "Installing remaining libraries (EMD-signal pinned)...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pandas_ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "BOT_BASE_DIR set to: /content/drive/MyDrive/trading_bot_project_v2/\n",
            "Added /content/drive/MyDrive/trading_bot_project_v2/ to sys.path for module imports.\n",
            "\n",
            "--- Library Versions ---\n",
            "Python: 3.11.11\n",
            "CCXT: 4.4.82\n",
            "LightGBM: 4.6.0\n",
            "Pandas: 2.0.3\n",
            "NumPy: 1.26.4\n",
            "Optuna: 4.3.0\n",
            "SHAP: 0.47.2\n",
            "EMD-signal: 1.3.0\n",
            "Matplotlib: 3.10.0\n",
            "SciPy: 1.11.4\n",
            "PyYAML: 6.0.2\n",
            "websocket-client: 1.8.0\n",
            "Dill: 0.3.7\n",
            "scikit-learn: 1.6.1\n",
            "pandas_ta: 0.3.14b0\n",
            "------------------------\n",
            "\n",
            "Environment setup complete (ULTRA CLEAN Install Attempt).\n",
            "IMPORTANT: Ensure all your custom .py files are in the BOT_BASE_DIR and contain plain Python code.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Configuration Settings\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "import numpy as np # For np.nan if you need to represent null for some specific logic\n",
        "\n",
        "# --- Ensure BOT_BASE_DIR is available (set in Cell 1) ---\n",
        "BOT_BASE_DIR = os.environ.get('BOT_BASE_DIR')\n",
        "if BOT_BASE_DIR is None:\n",
        "    print(\"CRITICAL ERROR (Cell 2): BOT_BASE_DIR is not set. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Run Cell 1 to define it.\")\n",
        "\n",
        "# --- Define Configuration Dictionary (Based on User Input) ---\n",
        "config = {\n",
        "    # -- Exchange & Symbol --\n",
        "    'exchange_name': 'bybit',  # Default, will be overridden if in user YAML\n",
        "    'exchange_testnet': True,\n",
        "    'symbol': 'BTC/USDT:USDT', # User provided\n",
        "    'market_type': 'linear',   # Default, assuming linear from symbol format\n",
        "    'timeframe': '1m', # User provided\n",
        "\n",
        "    # -- Data Fetching & Paths --\n",
        "    'base_dir': BOT_BASE_DIR,  # Uses the BOT_BASE_DIR from environment\n",
        "    'fetch_ohlcv_limit': 2500, # User provided\n",
        "    'load_existing_ohlcv': True,\n",
        "    'l2_data_folder': 'l2_data',\n",
        "    'l2_log_file': 'l2_data_collector.log',\n",
        "    'fetch_ohlcv_limit_for_scaling': 750,\n",
        "    'fetch_ohlcv_limit_wfo': 5000, # User provided\n",
        "\n",
        "    # -- L2 Data Collector Specific --\n",
        "    'collector_symbol': 'BTCUSDT', # User provided\n",
        "    'collector_duration': 1,       # User provided\n",
        "    'collector_unit': \"minutes\",   # User provided\n",
        "    'collector_depth': 50,         # User provided (for WebSocket subscription)\n",
        "    'collector_category': \"linear\",# User provided\n",
        "    'l2_max_file_size_mb': 20,\n",
        "    'l2_collection_duration_seconds': 60, # Derived from 1 minute for consistency\n",
        "\n",
        "    # -- Feature Engineering --\n",
        "    'feature_window': 24,      # User provided\n",
        "    'ohlcv_base_features': [\"z_close\", \"z_volume\", \"z_spread\"],\n",
        "    'ta_features': ['rsi', 'macd', 'bbands', 'atr', 'kama', 'supertrend', 'vwap'], # Added from user's ta_indicator_params\n",
        "\n",
        "    'ta_indicator_params': { # User provided\n",
        "        'rsi': {'length': 10, 'scalar': 100},\n",
        "        'macd': {'fast': 8, 'slow': 21, 'signal': 5},\n",
        "        'bbands': {'length': 20, 'std': 2.5},\n",
        "        'atr': {'length': 14}, # Default, can be overridden here\n",
        "        'kama': {'length': 10, 'fast': 2, 'slow': 30},\n",
        "        'supertrend': {'length': 7, 'multiplier': 3, 'atr_period': 10},\n",
        "        'vwap': {},\n",
        "    },\n",
        "\n",
        "    'use_hht_features': True,\n",
        "    'hht_features_imf_bases': ['hht_freq_imf', 'hht_amp_imf'],\n",
        "    'hht_imf_count': 3,\n",
        "    'hht_emd_noise_width': 0.05,\n",
        "\n",
        "    'use_l2_features': True,   # User provided\n",
        "    'use_l2_features_for_training': True,\n",
        "    'l2_depth_imbalance_levels': [3, 5, 10, 15, 25], # User provided\n",
        "    'l2_features': [\n",
        "        'price_impact_10',\n",
        "        'bid_curve', 'ask_curve'\n",
        "        # 'depth_imb_X' features are generated based on l2_depth_imbalance_levels\n",
        "    ],\n",
        "    'l2_price_impact_depth_idx': 4,\n",
        "    'l2_curve_fit_levels': 20,\n",
        "    'l2_depth': 25, # User provided (likely for REST L2 snapshot depth in DataHandler)\n",
        "\n",
        "    # -- Label Generation --\n",
        "    'labeling_method': 'triple_barrier', # User provided\n",
        "\n",
        "    'label_volatility_window': 20, # User provided (for vol_norm_return)\n",
        "    'label_clip_quantiles': [0.01, 0.99], # User provided (for vol_norm_return)\n",
        "    'label_shift': -1,         # User provided (for vol_norm_return)\n",
        "\n",
        "    'triple_barrier_profit_target_atr_mult': 2.5, # User provided\n",
        "    'triple_barrier_stop_loss_atr_mult': 1.0,   # User provided\n",
        "    'triple_barrier_time_horizon_bars': 12,     # User provided\n",
        "    'triple_barrier_atr_column': 'atr',         # User provided\n",
        "\n",
        "    # -- Model Training --\n",
        "    'random_state': 42,\n",
        "    'test_size': 0.2,          # User provided\n",
        "    'min_training_samples': 100,\n",
        "    'train_ensemble': False,\n",
        "    'lgbm_n_jobs': -1,\n",
        "\n",
        "    'optuna_trials': 100,       # User provided\n",
        "    'optuna_n_estimators_max': 1500,\n",
        "    'optuna_early_stopping_rounds': 25,\n",
        "    'optuna_study_name': f\"lgbm_opt_BTC-USDT_1m\", # Adjusted based on user's symbol/timeframe\n",
        "    'optuna_load_if_exists': True,\n",
        "    'optuna_n_jobs': 1,\n",
        "    'optuna_timeout_seconds': None,\n",
        "\n",
        "    'optuna_search_spaces': { # User provided\n",
        "        'n_estimators': {'type': 'int', 'low': 50, 'high': 800, 'step': 25},\n",
        "        'learning_rate': {'type': 'float', 'low': 0.005, 'high': 0.1, 'log': True},\n",
        "        'num_leaves': {'type': 'int', 'low': 15, 'high': 100},\n",
        "        'max_depth': {'type': 'int', 'low': 2, 'high': 8},\n",
        "        'lambda_l1': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'lambda_l2': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'feature_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_freq': {'type': 'int', 'low': 1, 'high': 10},\n",
        "        'min_child_samples': {'type': 'int', 'low': 3, 'high': 40}\n",
        "    },\n",
        "\n",
        "    'ensemble_long_thresh': 0.5,\n",
        "    'ensemble_short_thresh': -0.5,\n",
        "    'ensemble_clf_params': {\n",
        "        'objective': 'multiclass', 'metric': 'multi_logloss',\n",
        "        'num_class': 3, 'n_estimators': 200,\n",
        "    },\n",
        "    'ensemble_reg_params': {\n",
        "        'objective': 'regression_l1', 'metric': 'mae',\n",
        "        'n_estimators': 200,\n",
        "    },\n",
        "\n",
        "    'enable_feature_selection': False, # User provided\n",
        "    'feature_selection_method': 'shap', # User provided\n",
        "    'num_features_to_select': 30,       # User provided\n",
        "\n",
        "    # -- Model Prediction & Trading Logic --\n",
        "    # 'backtest_threshold' from user's list is now 'prediction_threshold' for clarity\n",
        "    'prediction_threshold': 0.2, # User provided as backtest_threshold\n",
        "    'use_ensemble_for_backtest': False,\n",
        "    'use_ensemble_for_simulation': False,\n",
        "    'use_ensemble_for_visualization': False,\n",
        "\n",
        "    # -- Risk Management --\n",
        "    'risk_management': {\n",
        "        'max_drawdown': 0.20,\n",
        "        'volatility_lookback': 14,\n",
        "        'position_sizing_mode': 'volatility_target',\n",
        "        'volatility_target_pct': 0.02,\n",
        "        'max_equity_risk_pct': 0.05,\n",
        "        'fixed_fraction_pct': 0.02,\n",
        "        # User provided stop_loss_pct: null, take_profit_pct: null.\n",
        "        # These are not directly used by current ATR-based AdvancedRiskManager.\n",
        "        # Keeping them if user has other plans, or they can be removed if only ATR-based is used.\n",
        "        'stop_loss_pct': None, # User provided\n",
        "        'take_profit_pct': None, # User provided\n",
        "        'sl_atr_multiplier': 1.5,\n",
        "        'tp_atr_multiplier': 2.5\n",
        "    },\n",
        "    'fallback_volatility_pct_for_sizing': 0.02,\n",
        "\n",
        "    # -- Order Execution --\n",
        "    'execution': {\n",
        "        'slippage_model_pct': 0.0005,\n",
        "        'max_order_book_levels': 20,\n",
        "        'default_entry_order_type': 'market', # User provided as simulation_entry_order_type\n",
        "        'default_exit_order_type': 'limit',  # User provided as simulation_exit_order_type\n",
        "    },\n",
        "\n",
        "    # -- Backtesting --\n",
        "    'initial_balance': 10000, # User provided\n",
        "    'commission_pct': 0.0006,  # User provided\n",
        "    'leverage': 3,             # User provided\n",
        "    'fallback_atr_pct_for_backtest': 0.02,\n",
        "\n",
        "    # -- Walk-Forward Optimization --\n",
        "    'run_walk_forward_optimization': False, # Default to False, can be overridden\n",
        "    'walk_forward_train_periods': 730,      # User provided\n",
        "    'walk_forward_test_periods': 180,       # User provided\n",
        "    'walk_forward_step_periods': 180,       # User provided\n",
        "    'walk_forward_initial_warmup': 100,     # User provided\n",
        "    'walk_forward_retrain_frequency_folds': 1, # User provided\n",
        "    # fetch_ohlcv_limit_wfo is already under Data Fetching\n",
        "\n",
        "    # -- Live Simulation --\n",
        "    'run_simulation_flag': True, # User provided\n",
        "    'simulation_threshold': 0.2, # User provided\n",
        "    'fetch_live_limit': 300,   # User provided\n",
        "    'min_simulation_interval_seconds': 15,\n",
        "    'simulation_duration_seconds': 300, # User provided\n",
        "\n",
        "    # -- Visualization --\n",
        "    'show_plots': True,\n",
        "    'plot_style': 'seaborn-v0_8-darkgrid',\n",
        "    'use_shap_for_importance': True, # User provided as use_shap_override\n",
        "    'shap_max_samples': 1000,\n",
        "    'plot_figsize_equity': (14, 7),\n",
        "    'plot_figsize_lgbm': (12, 10),\n",
        "    'plot_figsize_shap_bar': (12, 10),\n",
        "    'plot_figsize_shap_dot': (12, 10),\n",
        "    'plot_figsize_emd': (14, 12),\n",
        "    'plot_figsize_features': (14, 10),\n",
        "\n",
        "    # -- Orchestrator --\n",
        "    'allow_no_exchange_init': False\n",
        "}\n",
        "\n",
        "# --- Overwrite specific keys from user's list if they differ from my structured interpretation ---\n",
        "# This step ensures the user's exact values (from their snippet) are prioritized\n",
        "# for the keys they explicitly provided.\n",
        "user_provided_config_snippet = {\n",
        "    'base_dir': BOT_BASE_DIR, # This must come from the environment\n",
        "    'symbol': 'BTC/USDT',\n",
        "    'timeframe': '1m',\n",
        "    'feature_window': 24,\n",
        "    'use_l2_features': True,\n",
        "    # 'l2_depth_levels': 25, # Renamed to l2_depth_imbalance_levels, and l2_depth is separate\n",
        "    'l2_depth': 25, # For REST L2 snapshots if DataHandler uses it directly\n",
        "    'fetch_ohlcv_limit': 2500,\n",
        "    'optuna_trials': 100,\n",
        "    'test_size': 0.2,\n",
        "    'backtest_threshold': 0.2, # This is now 'prediction_threshold'\n",
        "    'initial_balance': 10000,\n",
        "    'commission_pct': 0.0006,\n",
        "    'leverage': 3,\n",
        "    'stop_loss_pct': None, # Kept as user provided\n",
        "    'take_profit_pct': None, # Kept as user provided\n",
        "    'run_simulation_flag': True,\n",
        "    'simulation_threshold': 0.2,\n",
        "    'fetch_live_limit': 300,\n",
        "    'simulation_duration_seconds': 300,\n",
        "    'use_shap_override': True, # This is now 'use_shap_for_importance'\n",
        "    'collector_symbol': 'BTCUSDT',\n",
        "    'collector_duration': 1,\n",
        "    'collector_unit': 'minutes',\n",
        "    'collector_depth': 50,\n",
        "    'collector_category': 'linear',\n",
        "    'labeling_method': 'triple_barrier',\n",
        "    'label_volatility_window': 20,\n",
        "    'label_clip_quantiles': [0.01, 0.99],\n",
        "    'label_shift': -1,\n",
        "    'triple_barrier_profit_target_atr_mult': 2.5,\n",
        "    'triple_barrier_stop_loss_atr_mult': 1.0,\n",
        "    'triple_barrier_time_horizon_bars': 12,\n",
        "    'triple_barrier_atr_column': 'atr',\n",
        "    'walk_forward_train_periods': 730,\n",
        "    'walk_forward_test_periods': 180,\n",
        "    'walk_forward_step_periods': 180,\n",
        "    'walk_forward_initial_warmup': 100,\n",
        "    'walk_forward_retrain_frequency_folds': 1,\n",
        "    'fetch_ohlcv_limit_wfo': 5000,\n",
        "    'enable_feature_selection': False,\n",
        "    'feature_selection_method': 'shap',\n",
        "    'num_features_to_select': 30,\n",
        "    'optuna_search_spaces': {\n",
        "        'n_estimators': {'type': 'int', 'low': 50, 'high': 800, 'step': 25},\n",
        "        'learning_rate': {'type': 'float', 'low': 0.005, 'high': 0.1, 'log': True},\n",
        "        'num_leaves': {'type': 'int', 'low': 15, 'high': 100},\n",
        "        'max_depth': {'type': 'int', 'low': 2, 'high': 8},\n",
        "        'lambda_l1': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'lambda_l2': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'feature_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_freq': {'type': 'int', 'low': 1, 'high': 10},\n",
        "        'min_child_samples': {'type': 'int', 'low': 3, 'high': 40}\n",
        "    },\n",
        "    # Mapping user's 'simulation_entry/exit_order_type' to 'execution' dict\n",
        "    # 'simulation_entry_order_type': 'market', # Will be handled below\n",
        "    # 'simulation_exit_order_type': 'limit',   # Will be handled below\n",
        "    'l2_depth_imbalance_levels': [3, 5, 10, 15, 25],\n",
        "    'ta_indicator_params': {\n",
        "        'rsi': {'length': 10, 'scalar': 100},\n",
        "        'macd': {'fast': 8, 'slow': 21, 'signal': 5},\n",
        "        'bbands': {'length': 20, 'std': 2.5}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Update the main 'config' dictionary with the user's exact values for the keys they provided\n",
        "# and handle specific mappings.\n",
        "for key, value in user_provided_config_snippet.items():\n",
        "    if key == 'backtest_threshold':\n",
        "        config['prediction_threshold'] = value\n",
        "    elif key == 'use_shap_override':\n",
        "        config['use_shap_for_importance'] = value\n",
        "    # elif key == 'l2_depth_levels': # User had this, map to l2_depth_imbalance_levels for FeatureEngineer\n",
        "    #     config['l2_depth_imbalance_levels'] = value\n",
        "    #     # 'l2_depth' is a separate parameter for DataHandler's REST L2 snapshot\n",
        "    elif key == 'simulation_entry_order_type':\n",
        "        config['execution']['default_entry_order_type'] = value\n",
        "    elif key == 'simulation_exit_order_type':\n",
        "        config['execution']['default_exit_order_type'] = value\n",
        "    else:\n",
        "        config[key] = value\n",
        "\n",
        "# Ensure base_dir is always from the environment variable\n",
        "config['base_dir'] = BOT_BASE_DIR\n",
        "# Correctly derive l2_collection_duration_seconds from collector_duration and collector_unit\n",
        "if config.get('collector_unit') == 'minutes':\n",
        "    config['l2_collection_duration_seconds'] = config.get('collector_duration', 1) * 60\n",
        "elif config.get('collector_unit') == 'hours':\n",
        "    config['l2_collection_duration_seconds'] = config.get('collector_duration', 1) * 3600\n",
        "else: # Default to seconds if unit is unclear or missing\n",
        "    config['l2_collection_duration_seconds'] = config.get('collector_duration', 60)\n",
        "\n",
        "\n",
        "# --- Save Configuration to YAML File ---\n",
        "config_file_path = os.path.join(BOT_BASE_DIR, 'config.yaml')\n",
        "try:\n",
        "    with open(config_file_path, 'w') as f:\n",
        "        yaml.dump(config, f, sort_keys=False, indent=4, width=120, Dumper=yaml.SafeDumper) # Use SafeDumper\n",
        "    print(f\"\\nConfiguration (User Updated for Phase 1) saved to: {config_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError saving configuration: {e}\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# --- Display a snippet of the config for verification ---\n",
        "print(\"\\n--- Configuration Snippet (User Update for Phase 1) ---\")\n",
        "keys_to_display = [\n",
        "    'exchange_name', 'symbol', 'timeframe', 'base_dir',\n",
        "    'labeling_method', 'train_ensemble',\n",
        "    'run_walk_forward_optimization', 'run_simulation_flag',\n",
        "    'enable_feature_selection', 'prediction_threshold',\n",
        "    'l2_depth_imbalance_levels'\n",
        "]\n",
        "for key in keys_to_display:\n",
        "    if key in config:\n",
        "        print(f\"{key}: {config[key]}\")\n",
        "if 'optuna_search_spaces' in config and config['optuna_search_spaces']:\n",
        "    print(f\"optuna_search_spaces (first item key): {list(config['optuna_search_spaces'].keys())[0] if config['optuna_search_spaces'] else 'Not set'}\")\n",
        "if 'ta_indicator_params' in config and config['ta_indicator_params']:\n",
        "     print(f\"ta_indicator_params (first item key): {list(config['ta_indicator_params'].keys())[0] if config['ta_indicator_params'] else 'Not set'}\")\n",
        "print(\"-----------------------------------------\")\n",
        "\n"
      ],
      "metadata": {
        "id": "q3U7G-5TzRr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f86989e-4c40-409a-ac72-432aea38e4c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuration (User Updated for Phase 1) saved to: /content/drive/MyDrive/trading_bot_project_v2/config.yaml\n",
            "\n",
            "--- Configuration Snippet (User Update for Phase 1) ---\n",
            "exchange_name: bybit\n",
            "symbol: BTC/USDT\n",
            "timeframe: 1m\n",
            "base_dir: /content/drive/MyDrive/trading_bot_project_v2/\n",
            "labeling_method: triple_barrier\n",
            "train_ensemble: False\n",
            "run_walk_forward_optimization: False\n",
            "run_simulation_flag: True\n",
            "enable_feature_selection: False\n",
            "prediction_threshold: 0.2\n",
            "l2_depth_imbalance_levels: [3, 5, 10, 15, 25]\n",
            "optuna_search_spaces (first item key): n_estimators\n",
            "ta_indicator_params (first item key): rsi\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: L2 Data Collector (Refactored Usage)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import time # For a brief pause if needed\n",
        "from datetime import datetime, timezone # For logging\n",
        "import traceback # For error printing\n",
        "\n",
        "# --- Ensure BOT_BASE_DIR is available (set in Cell 1) ---\n",
        "BOT_BASE_DIR = os.environ.get('BOT_BASE_DIR')\n",
        "if BOT_BASE_DIR is None:\n",
        "    print(\"CRITICAL ERROR (Cell 3): BOT_BASE_DIR is not set. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Run Cell 1 to define it.\")\n",
        "\n",
        "# --- Add BOT_BASE_DIR to Python path to allow importing l2_data_collector ---\n",
        "# This should have been done in Cell 1 already, but good to ensure for standalone cell execution if possible\n",
        "if BOT_BASE_DIR not in sys.path:\n",
        "    sys.path.append(BOT_BASE_DIR)\n",
        "    print(f\"Info (Cell 3): Added {BOT_BASE_DIR} to sys.path for L2DataCollector import.\")\n",
        "\n",
        "# --- Import the refactored class ---\n",
        "L2DataCollector = None # Initialize to None\n",
        "try:\n",
        "    from l2_data_collector import L2DataCollector # Assumes l2_data_collector.py is in BOT_BASE_DIR\n",
        "    print(\"L2DataCollector class imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR (Cell 3): Could not import L2DataCollector: {e}\")\n",
        "    print(f\"Please ensure 'l2_data_collector.py' is in the directory: {BOT_BASE_DIR} and that Cell 1 was run.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during L2DataCollector import: {e}\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# --- Load Main Configuration to Extract Collector Settings ---\n",
        "# This assumes config.yaml is in BOT_BASE_DIR and was created by Cell 2.\n",
        "config_file_path_main = os.path.join(BOT_BASE_DIR, 'config.yaml')\n",
        "main_config_for_l2 = {} # Use a distinct variable name\n",
        "collector_specific_config = {}\n",
        "\n",
        "if os.path.exists(config_file_path_main):\n",
        "    try:\n",
        "        with open(config_file_path_main, 'r') as f:\n",
        "            main_config_for_l2 = yaml.safe_load(f)\n",
        "        if main_config_for_l2:\n",
        "            print(f\"Successfully loaded main configuration from {config_file_path_main} for L2 collector settings.\")\n",
        "\n",
        "            # Extract collector-specific settings from the main config.\n",
        "            # The L2DataCollector class itself has defaults, but we use main_config to override them.\n",
        "            # The L2DataCollector's __init__ expects a 'config' dict.\n",
        "            collector_specific_config = {\n",
        "                'symbol': main_config_for_l2.get('collector_symbol', 'BTCUSDT'), # L2Collector uses 'symbol' for its target\n",
        "                'market_type': main_config_for_l2.get('collector_category', main_config_for_l2.get('market_type', 'linear')),\n",
        "                'exchange_name': main_config_for_l2.get('exchange_name', 'bybit'),\n",
        "                'l2_data_folder': main_config_for_l2.get('l2_data_folder', 'l2_data'),\n",
        "                'l2_log_file': main_config_for_l2.get('l2_log_file', 'l2_data_collector.log'), # Log file name for L2 collector\n",
        "                'l2_max_file_size_mb': main_config_for_l2.get('l2_max_file_size_mb', 20),\n",
        "                'l2_collection_duration_seconds': main_config_for_l2.get('l2_collection_duration_seconds', 300),\n",
        "                'l2_websocket_depth': main_config_for_l2.get('collector_depth', 50)\n",
        "            }\n",
        "            print(\"L2 Collector parameters extracted from main config:\")\n",
        "            for k, v in collector_specific_config.items():\n",
        "                print(f\"  {k}: {v}\")\n",
        "        else:\n",
        "            print(f\"Warning (Cell 3): Main config.yaml at {config_file_path_main} was empty. L2 Collector might use class defaults.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning (Cell 3): Could not load or parse main config.yaml ({e}). L2 Collector might use class defaults or fail if critical params missing.\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\"Warning (Cell 3): Main config.yaml not found at {config_file_path_main}. L2 Collector will use its internal defaults if not provided in collector_specific_config.\")\n",
        "\n",
        "\n",
        "# --- Instantiate and Run the Collector ---\n",
        "# Check if the L2DataCollector class was successfully imported AND if we have some config for it\n",
        "if L2DataCollector is not None:\n",
        "    if not collector_specific_config and not main_config_for_l2: # If no config could be loaded at all\n",
        "        print(\"ERROR (Cell 3): No configuration available for L2DataCollector. Cannot proceed.\")\n",
        "    else:\n",
        "        print(\"\\n--- Initializing and Starting L2 Data Collector ---\")\n",
        "\n",
        "        # The L2DataCollector's __init__ expects 'config' (which are the collector_specific_config here)\n",
        "        # and 'bot_base_dir'.\n",
        "        l2_collector_instance = None\n",
        "        try:\n",
        "            l2_collector_instance = L2DataCollector(config=collector_specific_config, bot_base_dir=BOT_BASE_DIR)\n",
        "\n",
        "            # The start_collection_websocket method contains its own loop based on l2_collection_duration_seconds.\n",
        "            # This cell will effectively block until that duration is over or an interrupt occurs.\n",
        "            duration_to_run = collector_specific_config.get('l2_collection_duration_seconds', 300) # Get from extracted params\n",
        "            print(f\"L2 Collector is configured to run for approximately {duration_to_run / 60:.1f} minutes.\")\n",
        "            print(\"You can interrupt the kernel (Runtime -> Interrupt execution or Ctrl+M I) to stop it sooner.\")\n",
        "            print(f\"L2 data will be saved in: {os.path.join(BOT_BASE_DIR, collector_specific_config.get('l2_data_folder', 'l2_data'))}\")\n",
        "            print(f\"L2 collector log file: {os.path.join(BOT_BASE_DIR, collector_specific_config.get('l2_log_file', 'l2_data_collector.log'))}\")\n",
        "\n",
        "            l2_collector_instance.start_collection_websocket()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nL2 Data Collection interrupted by user in notebook cell.\")\n",
        "            if l2_collector_instance:\n",
        "                l2_collector_instance.stop_collection_websocket() # Ensure graceful shutdown\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while running the L2 Data Collector: {e}\")\n",
        "            traceback.print_exc()\n",
        "            if l2_collector_instance and getattr(l2_collector_instance, 'ws', None) is not None : # Check if ws object exists\n",
        "                l2_collector_instance.stop_collection_websocket()\n",
        "        finally:\n",
        "            print(\"--- L2 Data Collector Cell Execution Finished ---\")\n",
        "            # Note: If start_collection_websocket runs in a blocking way for its duration,\n",
        "            # this \"finished\" message will appear after the collection period.\n",
        "else:\n",
        "    print(\"\\nL2DataCollector class not available (import failed). Cannot start L2 data collection.\")\n",
        "    print(\"Ensure 'l2_data_collector.py' is in your BOT_BASE_DIR and Cell 1 has been run.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2DataCollector class imported successfully.\n",
            "Successfully loaded main configuration from /content/drive/MyDrive/trading_bot_project_v2/config.yaml for L2 collector settings.\n",
            "L2 Collector parameters extracted from main config:\n",
            "  symbol: BTCUSDT\n",
            "  market_type: linear\n",
            "  exchange_name: bybit\n",
            "  l2_data_folder: l2_data\n",
            "  l2_log_file: l2_data_collector.log\n",
            "  l2_max_file_size_mb: 20\n",
            "  l2_collection_duration_seconds: 60\n",
            "  l2_websocket_depth: 50\n",
            "\n",
            "--- Initializing and Starting L2 Data Collector ---\n",
            "[2025-05-21 12:52:26 UTC] (L2Collector) L2DataCollector initialized for symbol: BTCUSDT, exchange: bybit\n",
            "L2 Collector is configured to run for approximately 1.0 minutes.\n",
            "You can interrupt the kernel (Runtime -> Interrupt execution or Ctrl+M I) to stop it sooner.\n",
            "L2 data will be saved in: /content/drive/MyDrive/trading_bot_project_v2/l2_data\n",
            "L2 collector log file: /content/drive/MyDrive/trading_bot_project_v2/l2_data_collector.log\n",
            "[2025-05-21 12:52:27 UTC] (L2Collector) Using WSS URL for Bybit (linear): wss://stream.bybit.com/v5/public/linear\n",
            "[2025-05-21 12:52:27 UTC] (L2Collector) Starting WebSocket L2 data collection...\n",
            "[2025-05-21 12:52:27 UTC] (L2Collector) Opened new data file: /content/drive/MyDrive/trading_bot_project_v2/l2_data/l2_data_BTCUSDT_20250521_125227.jsonl.gz\n",
            "[2025-05-21 12:52:27 UTC] (L2Collector) WebSocket collection thread started. Target duration: 60 seconds.\n",
            "[2025-05-21 12:52:28 UTC] (L2Collector) WebSocket Connection Opened for BTCUSDT\n",
            "[2025-05-21 12:52:28 UTC] (L2Collector) Subscribed to orderbook.50.BTCUSDT\n",
            "[2025-05-21 12:53:27 UTC] (L2Collector) Collection duration reached or termination signaled.\n",
            "[2025-05-21 12:53:27 UTC] (L2Collector) Attempting to stop WebSocket L2 data collection...\n",
            "[2025-05-21 12:53:27 UTC] (L2Collector) WebSocket Closed: Status=None, Msg='None'\n",
            "[2025-05-21 12:53:27 UTC] (L2Collector) WebSocket close request sent.\n",
            "[2025-05-21 12:53:27 UTC] (L2Collector) Waiting for WebSocket thread to join...\n",
            "[2025-05-21 12:53:27 UTC] (L2Collector) WebSocket thread joined.\n",
            "[2025-05-21 12:53:27 UTC] (L2Collector) Closed final data file: /content/drive/MyDrive/trading_bot_project_v2/l2_data/l2_data_BTCUSDT_20250521_125227.jsonl.gz\n",
            "[2025-05-21 12:53:27 UTC] (L2Collector) L2 data collection fully stopped.\n",
            "--- L2 Data Collector Cell Execution Finished ---\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "id": "bPiZvwMtxx-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8486c4af-e903-4834-c5c8-306beea8d402"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Imports & Global Setup (Main Bot)\n",
        "\n",
        "# --- Core Libraries ---\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "import warnings\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import sys\n",
        "import yaml # For loading config\n",
        "import pickle # For saving/loading ensemble models\n",
        "\n",
        "# --- Add BOT_BASE_DIR to Python path ---\n",
        "# This allows importing custom modules from this directory\n",
        "# Ensure BOT_BASE_DIR is set, typically in Cell 1.\n",
        "bot_base_dir = os.environ.get('BOT_BASE_DIR')\n",
        "if bot_base_dir is None:\n",
        "    print(\"CRITICAL ERROR (Cell 4): BOT_BASE_DIR is not set in environment. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Run Cell 1 to define it.\")\n",
        "\n",
        "if BOT_BASE_DIR not in sys.path: # Should have been added in Cell 1, but double-check\n",
        "    sys.path.append(BOT_BASE_DIR)\n",
        "    print(f\"Info (Cell 4): Added {BOT_BASE_DIR} to sys.path\")\n",
        "\n",
        "# --- Data Handling & Numerics ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Machine Learning ---\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# --- Load Configuration ---\n",
        "print(\"Loading configuration...\")\n",
        "config_file_path = os.path.join(bot_base_dir, 'config.yaml')\n",
        "config = {}\n",
        "try:\n",
        "    with open(config_file_path, 'r') as f:\n",
        "        main_config_loader = yaml.safe_load(f)\n",
        "        if main_config_loader:\n",
        "            config = main_config_loader\n",
        "    print(f\"Configuration loaded successfully from {config_file_path}\")\n",
        "    if not config:\n",
        "        print(\"Warning (Cell 4): Configuration file was empty. Using empty config dict.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR (Cell 4): Configuration file not found at {config_file_path}. Please run Cell 2 to create it.\")\n",
        "    print(\"Using empty config dict. Bot may not function correctly.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error (Cell 4) loading configuration: {e}\")\n",
        "    traceback.print_exc()\n",
        "    print(\"Using empty config dict. Bot may not function correctly.\")\n",
        "\n",
        "# --- Optional Libraries Setup ---\n",
        "print(\"\\nSetting up optional libraries and global flags...\")\n",
        "try:\n",
        "    import optuna\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "    HAS_OPTUNA = True\n",
        "    print(\"Optuna loaded.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): Optuna not found. Model training requiring it may fail.\")\n",
        "    HAS_OPTUNA = False\n",
        "    optuna = None\n",
        "\n",
        "try:\n",
        "    from PyEMD import EMD\n",
        "    HAS_PYEMD = True\n",
        "    print(\"PyEMD (EMD-signal) library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): PyEMD (EMD-signal) not found. HHT features will be disabled.\")\n",
        "    HAS_PYEMD = False\n",
        "    class EMD:\n",
        "        def __init__(self, *args, **kwargs): pass\n",
        "        def __call__(self, signal, *args, **kwargs): return np.array([])\n",
        "        def get_imfs_and_residue(self): return np.array([]), np.array([])\n",
        "    print(\"Using dummy EMD class.\")\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    HAS_SHAP = True\n",
        "    print(\"SHAP library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): SHAP library not found. SHAP plots will be disabled.\")\n",
        "    HAS_SHAP = False\n",
        "    shap = None\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    if config and 'plot_style' in config:\n",
        "        try:\n",
        "            plt.style.use(config.get('plot_style'))\n",
        "            print(f\"Applied plot style: {config.get('plot_style')}\")\n",
        "        except Exception as e_style:\n",
        "            print(f\"Warning (Cell 4): Could not apply plot style '{config.get('plot_style')}': {e_style}\")\n",
        "    HAS_MATPLOTLIB = True\n",
        "    print(\"Matplotlib loaded.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): Matplotlib not found. Plotting will be disabled.\")\n",
        "    HAS_MATPLOTLIB = False\n",
        "    plt = None\n",
        "\n",
        "try:\n",
        "    from scipy.signal import hilbert\n",
        "    HAS_SCIPY_HILBERT = True\n",
        "    print(\"Scipy (signal.hilbert) loaded.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): Scipy.signal.hilbert not found. HHT features requiring it may fail.\")\n",
        "    HAS_SCIPY_HILBERT = False\n",
        "    def hilbert(signal, N=None, axis=-1):\n",
        "        print(\"Error (Cell 4): hilbert function called but scipy.signal.hilbert not available.\")\n",
        "        return np.zeros_like(signal) + 1j * np.zeros_like(signal)\n",
        "    print(\"Using dummy hilbert function.\")\n",
        "\n",
        "try:\n",
        "    import pandas_ta as ta\n",
        "    HAS_PANDAS_TA = True\n",
        "    print(\"Pandas TA library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): pandas_ta library not found. Advanced TA features relying on it will be disabled.\")\n",
        "    HAS_PANDAS_TA = False\n",
        "    ta = None\n",
        "\n",
        "# --- Exchange Interaction ---\n",
        "try:\n",
        "    import ccxt\n",
        "    print(\"CCXT loaded.\")\n",
        "except ImportError:\n",
        "    print(\"ERROR (Cell 4): CCXT not found. Exchange interaction will fail.\")\n",
        "    ccxt = None\n",
        "\n",
        "# --- Custom Helper Class Imports ---\n",
        "print(\"\\nImporting custom bot classes...\")\n",
        "custom_classes_to_import = [\n",
        "    \"AdvancedRiskManager\", \"SmartOrderExecutor\", \"DataHandler\",\n",
        "    \"FeatureEngineer\", \"LabelGenerator\", \"ModelTrainer\",\n",
        "    \"ModelPredictor\", \"StrategyBacktester\", \"LiveSimulator\",\n",
        "    \"Visualizer\", \"TradingBotOrchestrator\"\n",
        "]\n",
        "\n",
        "for class_name_str in custom_classes_to_import:\n",
        "    try:\n",
        "        module_name = class_name_str.lower()\n",
        "        module = __import__(module_name)\n",
        "        globals()[class_name_str] = getattr(module, class_name_str)\n",
        "        print(f\"{class_name_str} class loaded successfully from {module_name}.py\")\n",
        "    except ImportError as e:\n",
        "        print(f\"ERROR (Cell 4) importing {class_name_str} from {class_name_str.lower()}.py: {e}\")\n",
        "        print(f\"Please ensure '{class_name_str.lower()}.py' is in '{bot_base_dir}' or Python path and contains plain Python code.\")\n",
        "        globals()[class_name_str] = None\n",
        "    except AttributeError as e:\n",
        "        print(f\"ERROR (Cell 4): Attribute {class_name_str} not found in module {class_name_str.lower()}.py. Check class name. Error: {e}\")\n",
        "        globals()[class_name_str] = None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while importing {class_name_str}: {e}\")\n",
        "        traceback.print_exc()\n",
        "        globals()[class_name_str] = None\n",
        "\n",
        "# --- API Key Loading (Using Colab Secrets or Environment Variables) ---\n",
        "print(\"\\nLoading API Keys...\")\n",
        "BYBIT_API_KEY = None\n",
        "BYBIT_API_SECRET = None\n",
        "\n",
        "# Try Colab userdata first\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    # --- MODIFIED TO USE TESTNET KEY NAMES ---\n",
        "    BYBIT_API_KEY = userdata.get(\"BYBIT_API_KEY_MAIN_TEST\")\n",
        "    BYBIT_API_SECRET = userdata.get(\"BYBIT_API_SECRET_MAIN_TEST\")\n",
        "\n",
        "    if BYBIT_API_KEY and BYBIT_API_SECRET:\n",
        "        print(\"TESTNET API Keys loaded successfully from Colab secrets.\")\n",
        "    else:\n",
        "        print(\"*** WARNING: TESTNET API Keys not found or empty in Colab secrets ('BYBIT_API_KEY_MAIN_TEST', 'BYBIT_API_SECRET_MAIN_TEST'). ***\")\n",
        "        BYBIT_API_KEY = None\n",
        "        BYBIT_API_SECRET = None\n",
        "except ImportError:\n",
        "    print(\"Not in Colab environment. Will check environment variables for API keys.\")\n",
        "    # If not found in Colab secrets, try environment variables\n",
        "    if not (BYBIT_API_KEY and BYBIT_API_SECRET): # Check again in case Colab import failed but env vars might exist\n",
        "        BYBIT_API_KEY = os.environ.get(\"BYBIT_API_KEY_MAIN_TEST\") # Check for TESTNET env vars\n",
        "        BYBIT_API_SECRET = os.environ.get(\"BYBIT_API_SECRET_MAIN_TEST\")\n",
        "        if BYBIT_API_KEY and BYBIT_API_SECRET:\n",
        "            print(\"TESTNET API Keys loaded successfully from environment variables.\")\n",
        "        else:\n",
        "            print(\"WARNING (Cell 4): TESTNET API Keys not found in Colab secrets or environment variables ('BYBIT_API_KEY_MAIN_TEST', 'BYBIT_API_SECRET_MAIN_TEST'). Live trading requiring authentication will fail if testnet keys are intended.\")\n",
        "except Exception as e: # Catch other potential errors during userdata.get()\n",
        "    print(f\"An unexpected error occurred loading secrets: {e}\")\n",
        "    traceback.print_exc()\n",
        "    BYBIT_API_KEY = None\n",
        "    BYBIT_API_SECRET = None\n",
        "\n",
        "# --- Warnings Configuration ---\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "print(\"\\nImports and global setup complete.\")\n",
        "if config:\n",
        "    print(f\"Config dictionary loaded with {len(config)} top-level keys.\")\n",
        "    print(f\"Config 'exchange_testnet' is set to: {config.get('exchange_testnet')}\")\n",
        "else:\n",
        "    print(\"CRITICAL WARNING (Cell 4): Config dictionary is empty or failed to load. Bot will likely not function correctly.\")\n"
      ],
      "metadata": {
        "id": "-tyw0dYhoiNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4053b4df-a18e-4070-f488-86777c6d197c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading configuration...\n",
            "Configuration loaded successfully from /content/drive/MyDrive/trading_bot_project_v2/config.yaml\n",
            "\n",
            "Setting up optional libraries and global flags...\n",
            "Optuna loaded.\n",
            "PyEMD (EMD-signal) library loaded successfully.\n",
            "SHAP library loaded successfully.\n",
            "Applied plot style: seaborn-v0_8-darkgrid\n",
            "Matplotlib loaded.\n",
            "Scipy (signal.hilbert) loaded.\n",
            "Pandas TA library loaded successfully.\n",
            "CCXT loaded.\n",
            "\n",
            "Importing custom bot classes...\n",
            "AdvancedRiskManager class loaded successfully from advancedriskmanager.py\n",
            "SmartOrderExecutor class loaded successfully from smartorderexecutor.py\n",
            "DataHandler class loaded successfully from datahandler.py\n",
            "FeatureEngineer class loaded successfully from featureengineer.py\n",
            "LabelGenerator class loaded successfully from labelgenerator.py\n",
            "ModelTrainer class loaded successfully from modeltrainer.py\n",
            "ModelPredictor class loaded successfully from modelpredictor.py\n",
            "StrategyBacktester class loaded successfully from strategybacktester.py\n",
            "LiveSimulator class loaded successfully from livesimulator.py\n",
            "Visualizer class loaded successfully from visualizer.py\n",
            "TradingBotOrchestrator class loaded successfully from tradingbotorchestrator.py\n",
            "\n",
            "Loading API Keys...\n",
            "TESTNET API Keys loaded successfully from Colab secrets.\n",
            "\n",
            "Imports and global setup complete.\n",
            "Config dictionary loaded with 97 top-level keys.\n",
            "Config 'exchange_testnet' is set to: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Cell 5 · Trading Bot Orchestrator Initialization   (patched)\n",
        "# ================================================================\n",
        "import os, traceback, inspect\n",
        "\n",
        "# ---------- 0 · Pre-flight checks --------------------------------\n",
        "if \"BOT_BASE_DIR\" not in os.environ:\n",
        "    raise EnvironmentError(\"Cell 5: BOT_BASE_DIR not set – run Cell 1 first.\")\n",
        "BOT_BASE_DIR = os.environ[\"BOT_BASE_DIR\"]\n",
        "print(f\"Info:  BOT_BASE_DIR → {BOT_BASE_DIR}\")\n",
        "\n",
        "if \"config\" not in globals() or not isinstance(config, dict) or not config:\n",
        "    raise ValueError(\"Cell 5: global 'config' missing or empty – did you run Cells 2 & 4?\")\n",
        "print(f\"Info:  base config has {len(config)} keys.\")\n",
        "\n",
        "# ---------- 1 · Build Bybit-Testnet exchange_kwargs --------------\n",
        "exchange_kwargs = {\n",
        "    \"enableRateLimit\": True,\n",
        "    \"options\": {\n",
        "        \"defaultType\": \"linear\",          # USDT-perps\n",
        "        \"fetchCurrencies\": False,         # skip private /asset/ call\n",
        "        \"urls\": {                         # point both routes at test-net\n",
        "            \"api\": {\n",
        "                \"public\":  \"https://api-testnet.bybit.com\",\n",
        "                \"private\": \"https://api-testnet.bybit.com\",\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "    # no apiKey / secret → public-only access\n",
        "}\n",
        "\n",
        "# ---------- 2 · Merge into notebook-level config -----------------\n",
        "config.update({\n",
        "    \"exchange_name\":        \"bybit\",\n",
        "    \"exchange_kwargs\":      exchange_kwargs,\n",
        "    \"allow_no_exchange_init\": False,      # fail hard if even public init breaks\n",
        "})\n",
        "\n",
        "# ---------- 3 · Collect global library flags / modules -----------\n",
        "global_library_flags = {\n",
        "    name: globals().get(name, False) for name in [\n",
        "        \"HAS_OPTUNA\", \"HAS_PYEMD\", \"HAS_SCIPY_HILBERT\",\n",
        "        \"HAS_SHAP\", \"HAS_MATPLOTLIB\", \"HAS_PANDAS_TA\"\n",
        "    ]\n",
        "}\n",
        "global_library_modules = {\n",
        "    name: globals().get(name) for name in [\n",
        "        \"optuna\", \"EMD\", \"hilbert\", \"shap\", \"plt\", \"ta\", \"ccxt\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ---------- 4 · Instantiate TradingBotOrchestrator ---------------\n",
        "print(\"\\n--- Initialising TradingBotOrchestrator ---\")\n",
        "try:\n",
        "    sig = inspect.signature(TradingBotOrchestrator)\n",
        "    if \"bot_base_dir\" in sig.parameters:                   # old signature\n",
        "        bot_orchestrator = TradingBotOrchestrator(\n",
        "            config=config,\n",
        "            bot_base_dir=BOT_BASE_DIR,\n",
        "            global_library_flags=global_library_flags,\n",
        "            global_library_modules=global_library_modules,\n",
        "            api_key=None, api_secret=None,                # public-only\n",
        "        )\n",
        "    else:                                                  # new signature – dir in config\n",
        "        config[\"bot_base_dir\"] = BOT_BASE_DIR\n",
        "        bot_orchestrator = TradingBotOrchestrator(\n",
        "            config=config,\n",
        "            global_library_flags=global_library_flags,\n",
        "            global_library_modules=global_library_modules,\n",
        "            api_key=None, api_secret=None,\n",
        "        )\n",
        "\n",
        "    if bot_orchestrator.exchange:\n",
        "        print(f\"✅  Orchestrator ready – {len(bot_orchestrator.exchange.symbols)} symbols loaded from Bybit test-net\")\n",
        "    else:\n",
        "        print(\"⚠️  Orchestrator initialised but exchange is None (check allow_no_exchange_init flag)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌  Failed to initialise TradingBotOrchestrator: {e}\")\n",
        "    traceback.print_exc()\n",
        "    bot_orchestrator = None\n",
        "\n",
        "print(\"\\n--- Cell 5 done ---\")"
      ],
      "metadata": {
        "id": "lEZoYVZ948Is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d209e2-3b82-4185-bdc8-6a6bddd5173b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Info:  BOT_BASE_DIR → /content/drive/MyDrive/trading_bot_project_v2/\n",
            "Info:  base config has 97 keys.\n",
            "\n",
            "--- Initialising TradingBotOrchestrator ---\n",
            "Exchange 'bybit' initialized in SANDBOX/TESTNET mode.\n",
            "Markets loaded for 'bybit'.\n",
            "AdvancedRiskManager initialized.\n",
            "SmartOrderExecutor initialized.\n",
            "DataHandler initialized. OHLCV path: /content/drive/MyDrive/trading_bot_project_v2/ohlcv_data_BTC_USDT_1m.csv, L2 Raw path: /content/drive/MyDrive/trading_bot_project_v2/l2_data/btcusdt_l2_data_1m.jsonl\n",
            "FeatureEngineer initialized (Phase 1 Update).\n",
            "LabelGenerator initialized (Phase 1 Update). Using method: triple_barrier\n",
            "ModelTrainer initialized (Phase 1 Update).\n",
            "ModelPredictor initialized.\n",
            "StrategyBacktester initialized.\n",
            "LiveSimulator initialized.\n",
            "Visualizer initialized.\n",
            "TradingBotOrchestrator initialized (Phase 1 Update).\n",
            "✅  Orchestrator ready – 2421 symbols loaded from Bybit test-net\n",
            "\n",
            "--- Cell 5 done ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Trading Bot Orchestrator Initialization\n",
        "\n",
        "# This cell instantiates the main TradingBotOrchestrator class.\n",
        "# The complex logic of the old CombinedTradingBot has been moved to specialized classes\n",
        "# which were imported in Cell 4 and are instantiated by the TradingBotOrchestrator.\n",
        "\n",
        "import os # For BOT_BASE_DIR check\n",
        "import traceback # For printing stack trace on error\n",
        "\n",
        "# --- Ensure prerequisite variables from Cell 1 and Cell 4 are available ---\n",
        "# These checks are important for notebook execution flow.\n",
        "if 'BOT_BASE_DIR' not in os.environ or os.environ.get('BOT_BASE_DIR') is None:\n",
        "    print(\"CRITICAL ERROR (Cell 5): BOT_BASE_DIR is not set. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Ensure Cell 1 has been executed.\")\n",
        "else:\n",
        "    bot_base_dir_check = os.environ.get('BOT_BASE_DIR') # To use in this cell if needed, already global from Cell 4\n",
        "    print(f\"Info (Cell 5): Using BOT_BASE_DIR: {bot_base_dir_check}\")\n",
        "\n",
        "\n",
        "if 'config' not in globals() or not isinstance(config, dict) or not config:\n",
        "    print(\"CRITICAL ERROR (Cell 5): 'config' dictionary not found, not a dict, or is empty.\")\n",
        "    print(\"Please ensure Cell 2 (Config Settings) and Cell 4 (Imports & Global Setup) have been run successfully.\")\n",
        "    raise ValueError(\"'config' dictionary not available or invalid. Ensure Cell 2 and Cell 4 have been executed.\")\n",
        "else:\n",
        "    print(f\"Info (Cell 5): 'config' dictionary loaded with {len(config)} keys.\")\n",
        "\n",
        "print(\"\\n--- Initializing Trading Bot Orchestrator ---\")\n",
        "\n",
        "# Gather global library flags and modules (expected to be set in Cell 4)\n",
        "# These are passed to the orchestrator to inform component initializations.\n",
        "# Using globals().get() for safety, providing False/None as defaults.\n",
        "global_library_flags = {\n",
        "    'HAS_OPTUNA': globals().get('HAS_OPTUNA', False),\n",
        "    'HAS_PYEMD': globals().get('HAS_PYEMD', False),\n",
        "    'HAS_SCIPY_HILBERT': globals().get('HAS_SCIPY_HILBERT', False),\n",
        "    'HAS_SHAP': globals().get('HAS_SHAP', False),\n",
        "    'HAS_MATPLOTLIB': globals().get('HAS_MATPLOTLIB', False),\n",
        "    'HAS_PANDAS_TA': globals().get('HAS_PANDAS_TA', False)\n",
        "}\n",
        "print(f\"Info (Cell 5): Global library flags collected: {global_library_flags}\")\n",
        "\n",
        "global_library_modules = {\n",
        "    'optuna': globals().get('optuna'),\n",
        "    'EMD': globals().get('EMD'),\n",
        "    'hilbert': globals().get('hilbert'),\n",
        "    'shap': globals().get('shap'),\n",
        "    'plt': globals().get('plt'),\n",
        "    'ta': globals().get('ta'),\n",
        "    'ccxt': globals().get('ccxt') # Pass the ccxt module itself\n",
        "}\n",
        "# Simple check for modules\n",
        "# for mod_name, mod_obj in global_library_modules.items():\n",
        "#     print(f\"Info (Cell 5): Module '{mod_name}' is {'available' if mod_obj else 'NOT available (or dummy)'}\")\n",
        "\n",
        "\n",
        "# Check if TradingBotOrchestrator class was imported successfully in Cell 4\n",
        "bot_orchestrator = None # Initialize to None\n",
        "if 'TradingBotOrchestrator' in globals() and TradingBotOrchestrator is not None:\n",
        "    try:\n",
        "        print(\"Instantiating TradingBotOrchestrator...\")\n",
        "        bot_orchestrator = TradingBotOrchestrator(\n",
        "            config=config,\n",
        "            api_key=globals().get('BYBIT_API_KEY'),\n",
        "            api_secret=globals().get('BYBIT_API_SECRET'),\n",
        "            global_library_flags=global_library_flags,\n",
        "            global_library_modules=global_library_modules\n",
        "        )\n",
        "\n",
        "        # Check if the exchange component within the orchestrator was initialized\n",
        "        if bot_orchestrator.exchange:\n",
        "            print(\"TradingBotOrchestrator instance created and exchange initialized successfully.\")\n",
        "        elif config.get('allow_no_exchange_init', False): # If no exchange is okay (e.g. offline analysis)\n",
        "            print(\"Info (Cell 5): TradingBotOrchestrator instance created, but without a live exchange connection (as per 'allow_no_exchange_init' config). Some functionalities like live trading/simulation will be unavailable.\")\n",
        "        else:\n",
        "            # This case means exchange init failed AND it was required\n",
        "            print(\"CRITICAL (Cell 5): TradingBotOrchestrator instance created, but its exchange component FAILED to initialize, and 'allow_no_exchange_init' is False. Live trading/simulation and data fetching will not work.\")\n",
        "            # Depending on the desired behavior, you might want to nullify bot_orchestrator here or raise an error\n",
        "            # to prevent Cell 6 from running with a non-functional orchestrator.\n",
        "            # For now, it will proceed but Cell 6 should check bot_orchestrator.exchange.\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR (Cell 5): Failed to instantiate TradingBotOrchestrator: {e}\")\n",
        "        traceback.print_exc()\n",
        "        bot_orchestrator = None # Ensure it's None on failure\n",
        "else:\n",
        "    print(\"ERROR (Cell 5): TradingBotOrchestrator class not imported or not available.\")\n",
        "    print(\"Please ensure Cell 4 ran successfully and 'trading_bot_orchestrator.py' is correct and in the BOT_BASE_DIR.\")\n",
        "\n",
        "# The old CombinedTradingBot class definition should be REMOVED from this cell.\n",
        "# All its logic is now distributed across the imported .py files and managed by TradingBotOrchestrator.\n",
        "\n",
        "print(\"\\n--- Orchestrator Initialization Attempt Finished ---\")\n",
        "if bot_orchestrator:\n",
        "    print(f\"Orchestrator object: {bot_orchestrator}\")\n",
        "    if bot_orchestrator.exchange:\n",
        "        print(f\"Orchestrator exchange object: {bot_orchestrator.exchange.id if bot_orchestrator.exchange else 'None'}\")\n",
        "else:\n",
        "    print(\"Orchestrator object is None (failed to initialize).\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Info (Cell 5): Using BOT_BASE_DIR: /content/drive/MyDrive/trading_bot_project_v2/\n",
            "Info (Cell 5): 'config' dictionary loaded with 99 keys.\n",
            "\n",
            "--- Initializing Trading Bot Orchestrator ---\n",
            "Info (Cell 5): Global library flags collected: {'HAS_OPTUNA': True, 'HAS_PYEMD': True, 'HAS_SCIPY_HILBERT': True, 'HAS_SHAP': True, 'HAS_MATPLOTLIB': True, 'HAS_PANDAS_TA': True}\n",
            "Instantiating TradingBotOrchestrator...\n",
            "Exchange 'bybit' initialized in SANDBOX/TESTNET mode.\n",
            "Markets loaded for 'bybit'.\n",
            "AdvancedRiskManager initialized.\n",
            "SmartOrderExecutor initialized.\n",
            "DataHandler initialized. OHLCV path: /content/drive/MyDrive/trading_bot_project_v2/ohlcv_data_BTC_USDT_1m.csv, L2 Raw path: /content/drive/MyDrive/trading_bot_project_v2/l2_data/btcusdt_l2_data_1m.jsonl\n",
            "FeatureEngineer initialized (Phase 1 Update).\n",
            "LabelGenerator initialized (Phase 1 Update). Using method: triple_barrier\n",
            "ModelTrainer initialized (Phase 1 Update).\n",
            "ModelPredictor initialized.\n",
            "StrategyBacktester initialized.\n",
            "LiveSimulator initialized.\n",
            "Visualizer initialized.\n",
            "TradingBotOrchestrator initialized (Phase 1 Update).\n",
            "TradingBotOrchestrator instance created and exchange initialized successfully.\n",
            "\n",
            "--- Orchestrator Initialization Attempt Finished ---\n",
            "Orchestrator object: <tradingbotorchestrator.TradingBotOrchestrator object at 0x7df0d047bfd0>\n",
            "Orchestrator exchange object: bybit\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "id": "RSEBAz9Zxpur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a486a594-786a-497b-a869-0d713534ec27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Main Execution Workflow\n",
        "\n",
        "# This cell assumes that 'bot_orchestrator' has been successfully initialized in Cell 5,\n",
        "# and 'config' (the configuration dictionary) is also available from Cell 4.\n",
        "# It also assumes that all necessary library flags (e.g., HAS_MATPLOTLIB) and\n",
        "# modules (e.g., plt) are globally available from Cell 4.\n",
        "\n",
        "import pandas as pd # For pd.option_context\n",
        "import traceback # For printing stack trace on error\n",
        "import sys # For checking interactive mode\n",
        "\n",
        "if 'bot_orchestrator' in globals() and bot_orchestrator is not None and \\\n",
        "   (bot_orchestrator.exchange or config.get('allow_no_exchange_init', False)):\n",
        "    print(\"\\n--- Starting Main Execution Workflow via Orchestrator ---\")\n",
        "\n",
        "    # --- CHOOSE YOUR WORKFLOW ---\n",
        "    # Set 'RUN_WALK_FORWARD' to True to execute Walk-Forward Optimization.\n",
        "    # Otherwise, the standard single train/backtest/simulation workflow will run.\n",
        "    RUN_WALK_FORWARD = config.get('run_walk_forward_optimization', False) # Get from config\n",
        "\n",
        "    if RUN_WALK_FORWARD:\n",
        "        print(\"\\n*** EXECUTING WALK-FORWARD OPTIMIZATION WORKFLOW ***\")\n",
        "        try:\n",
        "            bot_orchestrator.run_full_workflow(run_wfo=True)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the Walk-Forward Optimization workflow: {e}\")\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"\\n*** EXECUTING STANDARD WORKFLOW (Single Train/Backtest/Sim) ***\")\n",
        "        try:\n",
        "            bot_orchestrator.run_full_workflow(run_wfo=False)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the standard workflow: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # --- Granular Workflow Control (Optional - Uncomment sections to run specific parts) ---\n",
        "    # This is useful for debugging or re-running specific stages after making config changes.\n",
        "    # Ensure that the necessary preceding stages have been successfully completed or data is available.\n",
        "\n",
        "    # print(\"\\n--- Granular Workflow Control (Example) ---\")\n",
        "\n",
        "    # --- Stage 1: Prepare Data for Training ---\n",
        "    # data_prepared_successfully = False\n",
        "    # print(\"\\n--- Stage 1: Preparing Data ---\")\n",
        "    # try:\n",
        "    #     # For a standard run, df_input can be None to load full history.\n",
        "    #     # For re-running a specific WFO fold's data prep, you'd need to load that slice.\n",
        "    #     if bot_orchestrator.prepare_data_for_training(df_input=None, save_features=True, save_ohlcv=True):\n",
        "    #         print(\"Data preparation for training was successful.\")\n",
        "    #         data_prepared_successfully = True\n",
        "    #         # You can inspect the prepared dataframes:\n",
        "    #         # print(\"Historical Data Head:\\n\", bot_orchestrator.df_historical_data.head())\n",
        "    #         # print(\"Features DataFrame Head:\\n\", bot_orchestrator.df_features.head())\n",
        "    #         # print(\"Labeled Features DataFrame Head:\\n\", bot_orchestrator.df_labeled_features.head())\n",
        "    #     else:\n",
        "    #         print(\"Data preparation for training failed. Check logs.\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error during data preparation: {e}\")\n",
        "    #     traceback.print_exc()\n",
        "\n",
        "    # --- Stage 2: Train Model ---\n",
        "    # model_trained_successfully = False\n",
        "    # if data_prepared_successfully:\n",
        "    #     print(\"\\n--- Stage 2: Training Model ---\")\n",
        "    #     try:\n",
        "    #         # Trains on self.df_labeled_features by default if df_training_data is None\n",
        "    #         if bot_orchestrator.train_model(save_model=True):\n",
        "    #             print(\"Model training was successful.\")\n",
        "    #             model_trained_successfully = True\n",
        "    #             # print(\"Trained features list:\", bot_orchestrator.trained_features_list)\n",
        "    #         else:\n",
        "    #             print(\"Model training failed. Check logs.\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error during model training: {e}\")\n",
        "    #         traceback.print_exc()\n",
        "    # else:\n",
        "    #     print(\"Skipping model training (data not prepared).\")\n",
        "\n",
        "    # --- Stage 3: Run Backtest ---\n",
        "    # backtest_run_successfully = False\n",
        "    # # A model needs to be available either from current session training or loaded.\n",
        "    # model_is_ready_for_backtest = (bot_orchestrator.trained_model_booster is not None or\n",
        "    #                                bot_orchestrator.trained_ensemble_models is not None or\n",
        "    #                                (bot_orchestrator.model_predictor and bot_orchestrator.model_predictor.model_object is not None))\n",
        "    # if model_is_ready_for_backtest or config.get('allow_backtest_load_model_directly', True): # Add flag to allow direct load\n",
        "    #     print(\"\\n--- Stage 3: Running Backtest ---\")\n",
        "    #     try:\n",
        "    #         # df_backtest_data=None uses self.df_features. load_latest_model=True reloads from disk.\n",
        "    #         # If model was just trained, set load_latest_model=False to use in-memory model.\n",
        "    #         backtest_results_df, trades_log_df = bot_orchestrator.run_backtest(df_backtest_data=None, load_latest_model=not model_trained_successfully)\n",
        "    #         if backtest_results_df is not None and not backtest_results_df.empty:\n",
        "    #             print(\"Backtest finished.\")\n",
        "    #             if trades_log_df is not None and not trades_log_df.empty:\n",
        "    #                 print(\"\\nBacktest Trades Log Sample:\")\n",
        "    #                 with pd.option_context('display.max_rows', 10, 'display.max_columns', None, 'display.width', 1000):\n",
        "    #                     print(trades_log_df.head())\n",
        "    #             elif trades_log_df is not None:\n",
        "    #                 print(\"\\nNo trades executed during backtest.\")\n",
        "    #             backtest_run_successfully = True\n",
        "    #         else:\n",
        "    #             print(\"Backtest failed to produce results. Check logs.\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error during backtest: {e}\")\n",
        "    #         traceback.print_exc()\n",
        "    # else:\n",
        "    #     print(\"Skipping backtest (model not trained or loaded).\")\n",
        "\n",
        "    # --- Stage 4: Run Live Simulation (Optional) ---\n",
        "    # model_is_ready_for_simulation = (bot_orchestrator.trained_model_booster is not None or\n",
        "    #                                  bot_orchestrator.trained_ensemble_models is not None or\n",
        "    #                                  (bot_orchestrator.model_predictor and bot_orchestrator.model_predictor.model_object is not None))\n",
        "    # if model_is_ready_for_simulation or config.get('allow_simulation_load_model_directly', True):\n",
        "    #     if config.get('run_simulation_flag', False): # Check the master flag\n",
        "    #         print(\"\\n--- Stage 4: Running Live Simulation ---\")\n",
        "    #         try:\n",
        "    #             if bot_orchestrator.live_simulator:\n",
        "    #                 bot_orchestrator.run_live_simulation()\n",
        "    #             else:\n",
        "    #                 print(\"Live simulator component not available. Skipping simulation.\")\n",
        "    #         except Exception as e:\n",
        "    #             print(f\"Error during live simulation: {e}\")\n",
        "    #             traceback.print_exc()\n",
        "    #             if bot_orchestrator.live_simulator and bot_orchestrator.live_simulator.simulation_running:\n",
        "    #                 bot_orchestrator.live_simulator.stop_live_simulation()\n",
        "    #     else:\n",
        "    #         print(\"\\nSkipping Live Simulation (run_simulation_flag is False in config).\")\n",
        "    # else:\n",
        "    #     print(\"Skipping live simulation (model not trained or loaded).\")\n",
        "\n",
        "    # --- Stage 5: Visualize Other Results (Feature Importance, EMD, etc.) ---\n",
        "    # print(\"\\n--- Stage 5: Visualizing Other Results ---\")\n",
        "    # try:\n",
        "    #     if bot_orchestrator.visualizer:\n",
        "    #         bot_orchestrator.visualize_results()\n",
        "    #     else:\n",
        "    #         print(\"Visualizer not available, skipping additional visualizations.\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error during visualization: {e}\")\n",
        "    #     traceback.print_exc()\n",
        "\n",
        "    print(\"\\n--- Orchestrator Workflow Attempt Finished ---\")\n",
        "else:\n",
        "    print(\"\\nSkipping Main Execution Workflow: Orchestrator not initialized, exchange failed, or 'allow_no_exchange_init' is False with no exchange.\")\n",
        "    print(\"Please ensure Cell 1 (Setup), Cell 2 (Config), Cell 4 (Imports), and Cell 5 (Orchestrator Init) have run successfully.\")\n",
        "\n",
        "print(\"\\n=\"*50)\n",
        "print(\"Notebook Execution Finished\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Optional: Keep plots open if not in interactive mode (e.g., running as script)\n",
        "# This part is from your original notebook.\n",
        "# Ensure 'plt' and 'HAS_MATPLOTLIB' are accessible here (defined in Cell 4)\n",
        "# if 'plt' in globals() and plt is not None and \\\n",
        "#    'HAS_MATPLOTLIB' in globals() and HAS_MATPLOTLIB and \\\n",
        "#    'sys' in globals() : # Check if sys was imported\n",
        "#      INTERACTIVE_MODE = 'ipykernel' in sys.modules\n",
        "#      if not INTERACTIVE_MODE:\n",
        "#          print(\"Displaying plots. Close plot windows to exit if any were generated and `show_plots` is True in config.\")\n",
        "#          plt.show() # This will show all figures generated if plt.show() wasn't called in Visualizer"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Main Execution Workflow via Orchestrator ---\n",
            "\n",
            "*** EXECUTING STANDARD WORKFLOW (Single Train/Backtest/Sim) ***\n",
            "\n",
            "--- Starting Data Preparation ---\n",
            "No input DataFrame provided, loading full historical data.\n",
            "Loading existing OHLCV data from /content/drive/MyDrive/trading_bot_project_v2/ohlcv_data_BTC_USDT_1m.csv\n",
            "Loaded 999 records from CSV.\n",
            "Cleaned OHLCV data. Shape: (999, 6)\n",
            "Loading and aligning historical L2 data...\n",
            "Loaded 2621 L2 records from /content/drive/MyDrive/trading_bot_project_v2/l2_data/btcusdt_l2_data_1m.jsonl\n",
            "Warning (DataHandler): 999 OHLCV candles could not be aligned with preceding L2 data.\n",
            "Aligned L2 data with OHLCV. Resulting shape: (999, 8)\n",
            "Data loading and preparation complete. Final DataFrame shape: (999, 8)\n",
            "Starting feature generation for 999 rows...\n",
            "Warning (FeatureEngineer): Error calculating TA feature 'macd': Cannot set a DataFrame with multiple columns to the single column macd\n",
            "Warning (FeatureEngineer): Error calculating TA feature 'bbands': Cannot set a DataFrame with multiple columns to the single column bbands\n",
            "[!] VWAP volume series is not datetime ordered. Results may not be as expected.\n",
            "[!] VWAP price series is not datetime ordered. Results may not be as expected.\n",
            "Warning (FeatureEngineer): Error calculating TA feature 'vwap': 'RangeIndex' object has no attribute 'to_period'\n",
            "Dropped 999 rows due to NaN in essential non-L2 features.\n",
            "Error (FeatureEngineer): DataFrame empty after NaN drop in essential features.\n",
            "Error (Orchestrator): Failed to engineer features.\n",
            "\n",
            "--- Orchestrator Workflow Attempt Finished ---\n",
            "\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "Notebook Execution Finished\n",
            "==================================================\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "id": "pMrqQdIUxelt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c933f7a-9315-446f-b1a8-5a3389efd6f5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}