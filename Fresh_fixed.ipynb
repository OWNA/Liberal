{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OWNA/Liberal/blob/main/Fresh_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "41d5de07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e2178e-537c-4fb9-c8bc-f25c32adc28c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement EMD-signal==0.4.0 (from versions: 0.1.1, 0.1.11, 0.1.12, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.2.7, 0.2.8, 0.2.9, 0.2.10, 0.2.12, 0.2.13, 0.2.14, 0.2.15, 1.0.0, 1.1.0, 1.1.1, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.4.0, 1.5.0, 1.5.1, 1.5.2, 1.6.0, 1.6.2, 1.6.3, 1.6.4)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for EMD-signal==0.4.0\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Versions → NumPy 2.0.2 | Pandas 2.2.2 | SciPy 1.15.3 | LightGBM 4.5.0 | scikit‑learn 1.6.1\n"
          ]
        }
      ],
      "source": [
        "# 🛠️ Environment Setup — simplified & version‑pinned\n",
        "# Run this once, then **Runtime ▸ Restart & run all**.\n",
        "%pip install --quiet --upgrade \\\n",
        "    numpy==2.0.0 \\\n",
        "    pandas==2.2.2 \\\n",
        "    scipy==1.12.0 \\\n",
        "    scikit-learn==1.4.2 \\\n",
        "    lightgbm==4.3.0 \\\n",
        "    pandas-ta==0.3.14b0 \\\n",
        "    EMD-signal==0.4.0 \\\n",
        "    shap optuna ccxt matplotlib pyyaml websocket-client dill\n",
        "\n",
        "import numpy, pandas, scipy, lightgbm as lgb, sklearn\n",
        "print(\"✅ Versions →\", \"NumPy\", numpy.__version__, \"| Pandas\", pandas.__version__, \"| SciPy\", scipy.__version__, \"| LightGBM\", lgb.__version__, \"| scikit‑learn\", sklearn.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQFl0DYZsnvB",
        "outputId": "33f1ac33-ab17-4014-9e3f-0a94f1ba103c"
      },
      "source": [
        "### (Deprecated) Original aggressive uninstall cell — kept for reference, no longer executed.\n",
        "\n",
        "# Cell 1: Setup Environment (ULTRA CLEAN INSTALL ATTEMPT)\n",
        "\n",
        "# --- Step 1: Aggressively Uninstall Key Libraries ---\n",
        "print(\"Aggressively uninstalling numpy, scipy, pandas, lightgbm, EMD-signal, pandas-ta, shap, scikit-learn...\")\n",
        "!pip uninstall numpy -y --quiet\n",
        "!pip uninstall scipy -y --quiet\n",
        "!pip uninstall pandas -y --quiet\n",
        "!pip uninstall lightgbm -y --quiet\n",
        "!pip uninstall EMD-signal -y --quiet\n",
        "!pip uninstall pandas-ta -y --quiet\n",
        "!pip uninstall shap -y --quiet\n",
        "!pip uninstall scikit-learn -y --quiet\n",
        "\n",
        "# --- Step 2: Install NumPy pinned to <2.0 FIRST ---\n",
        "print(\"\\nInstalling NumPy <2.0 (target 1.26.4)...\")\n",
        "!pip install \"numpy==1.26.4\" --quiet # Force specific 1.x version\n",
        "\n",
        "# --- Step 3: Install specific compatible versions of Pandas and SciPy ---\n",
        "print(\"\\nInstalling specific compatible versions of Pandas and SciPy...\")\n",
        "!pip install \"pandas==2.0.3\" --quiet # Known to work well with NumPy 1.26.x\n",
        "!pip install \"scipy==1.11.4\" --quiet  # Known to work well with NumPy 1.26.x & Pandas 2.0.x\n",
        "\n",
        "# --- Step 4: Install LightGBM and scikit-learn ---\n",
        "print(\"\\nInstalling LightGBM and scikit-learn...\")\n",
        "!pip install lightgbm --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "\n",
        "\n",
        "# --- Step 5: Install the rest of the libraries (EMD-signal pinned) ---\n",
        "print(\"\\nInstalling remaining libraries (EMD-signal pinned)...\")\n",
        "!pip install ccxt optuna shap \"EMD-signal<1.4.0\" matplotlib pyyaml websocket-client dill==0.3.7 pandas_ta --quiet\n",
        "\n",
        "# --- Step 6: Mount Google Drive ---\n",
        "print(\"\\nMounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Step 7: Define Base Directory for the Project ---\n",
        "BOT_BASE_DIR = '/content/drive/MyDrive/trading_bot_project_v2/' # !!! YOUR PATH HERE !!!\n",
        "import os\n",
        "os.makedirs(BOT_BASE_DIR, exist_ok=True)\n",
        "os.environ['BOT_BASE_DIR'] = BOT_BASE_DIR\n",
        "print(f\"BOT_BASE_DIR set to: {BOT_BASE_DIR}\")\n",
        "import sys\n",
        "if BOT_BASE_DIR not in sys.path:\n",
        "    sys.path.append(BOT_BASE_DIR)\n",
        "    print(f\"Added {BOT_BASE_DIR} to sys.path for module imports.\")\n",
        "\n",
        "# --- Step 8: Check Python and Library Versions ---\n",
        "print(f\"\\n--- Library Versions ---\")\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "try:\n",
        "    import importlib.metadata\n",
        "\n",
        "    libs_to_check = {\n",
        "        \"ccxt\": \"CCXT\", \"lightgbm\": \"LightGBM\", \"pandas\": \"Pandas\",\n",
        "        \"numpy\": \"NumPy\", \"optuna\": \"Optuna\", \"shap\": \"SHAP\",\n",
        "        \"EMD-signal\": \"EMD-signal\", \"matplotlib\": \"Matplotlib\", \"scipy\": \"SciPy\",\n",
        "        \"PyYAML\": \"PyYAML\", \"websocket-client\": \"websocket-client\", \"dill\": \"Dill\",\n",
        "        \"scikit-learn\": \"scikit-learn\", \"pandas_ta\": \"pandas_ta\"\n",
        "    }\n",
        "    for lib_pkg_name, display_name in libs_to_check.items():\n",
        "        try:\n",
        "            version = importlib.metadata.version(lib_pkg_name)\n",
        "            print(f\"{display_name}: {version}\")\n",
        "        except importlib.metadata.PackageNotFoundError:\n",
        "            print(f\"{display_name}: Not installed or version not found.\")\n",
        "        except Exception as e_ver:\n",
        "            print(f\"Error getting version for {display_name}: {e_ver}\")\n",
        "except ImportError:\n",
        "    print(\"Could not import 'importlib.metadata'. Manual version checks might be needed.\")\n",
        "except Exception as e_outer:\n",
        "    print(f\"An error occurred during library version checking: {e_outer}\")\n",
        "print(\"------------------------\")\n",
        "\n",
        "print(\"\\nEnvironment setup complete (ULTRA CLEAN Install Attempt).\")\n",
        "print(\"IMPORTANT: Ensure all your custom .py files are in the BOT_BASE_DIR and contain plain Python code.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "q3U7G-5TzRr_",
        "outputId": "68e63653-a4fc-4a73-fc48-b39a0fb8e91c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRITICAL ERROR (Cell 2): BOT_BASE_DIR is not set. Please run Cell 1 first.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "BOT_BASE_DIR not set. Run Cell 1 to define it.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-84377cbb6887>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mBOT_BASE_DIR\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CRITICAL ERROR (Cell 2): BOT_BASE_DIR is not set. Please run Cell 1 first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BOT_BASE_DIR not set. Run Cell 1 to define it.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# --- Define Configuration Dictionary (Based on User Input) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: BOT_BASE_DIR not set. Run Cell 1 to define it."
          ]
        }
      ],
      "source": [
        "# Cell 2: Configuration Settings\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "import numpy as np # For np.nan if you need to represent null for some specific logic\n",
        "\n",
        "# --- Ensure BOT_BASE_DIR is available (set in Cell 1) ---\n",
        "BOT_BASE_DIR = os.environ.get('BOT_BASE_DIR')\n",
        "if BOT_BASE_DIR is None:\n",
        "    print(\"CRITICAL ERROR (Cell 2): BOT_BASE_DIR is not set. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Run Cell 1 to define it.\")\n",
        "\n",
        "# --- Define Configuration Dictionary (Based on User Input) ---\n",
        "config = {\n",
        "    # -- Exchange & Symbol --\n",
        "    'exchange_name': 'bybit',  # Default, will be overridden if in user YAML\n",
        "    'exchange_testnet': True,\n",
        "    'symbol': 'BTC/USDT:USDT', # User provided\n",
        "    'market_type': 'linear',   # Default, assuming linear from symbol format\n",
        "    'timeframe': '1m', # User provided\n",
        "\n",
        "    # -- Data Fetching & Paths --\n",
        "    'base_dir': BOT_BASE_DIR,  # Uses the BOT_BASE_DIR from environment\n",
        "    'fetch_ohlcv_limit': 2500, # User provided\n",
        "    'load_existing_ohlcv': True,\n",
        "    'l2_data_folder': 'l2_data',\n",
        "    'l2_log_file': 'l2_data_collector.log',\n",
        "    'fetch_ohlcv_limit_for_scaling': 750,\n",
        "    'fetch_ohlcv_limit_wfo': 5000, # User provided\n",
        "\n",
        "    # -- L2 Data Collector Specific --\n",
        "    'collector_symbol': 'BTCUSDT', # User provided\n",
        "    'collector_duration': 1,       # User provided\n",
        "    'collector_unit': \"minutes\",   # User provided\n",
        "    'collector_depth': 50,         # User provided (for WebSocket subscription)\n",
        "    'collector_category': \"linear\",# User provided\n",
        "    'l2_max_file_size_mb': 20,\n",
        "    'l2_collection_duration_seconds': 60, # Derived from 1 minute for consistency\n",
        "\n",
        "    # -- Feature Engineering --\n",
        "    'feature_window': 24,      # User provided\n",
        "    'ohlcv_base_features': [\"z_close\", \"z_volume\", \"z_spread\"],\n",
        "    'ta_features': ['rsi', 'macd', 'bbands', 'atr', 'kama', 'supertrend', 'vwap'], # Added from user's ta_indicator_params\n",
        "\n",
        "    'ta_indicator_params': { # User provided\n",
        "        'rsi': {'length': 10, 'scalar': 100},\n",
        "        'macd': {'fast': 8, 'slow': 21, 'signal': 5},\n",
        "        'bbands': {'length': 20, 'std': 2.5},\n",
        "        'atr': {'length': 14}, # Default, can be overridden here\n",
        "        'kama': {'length': 10, 'fast': 2, 'slow': 30},\n",
        "        'supertrend': {'length': 7, 'multiplier': 3, 'atr_period': 10},\n",
        "        'vwap': {},\n",
        "    },\n",
        "\n",
        "    'use_hht_features': True,\n",
        "    'hht_features_imf_bases': ['hht_freq_imf', 'hht_amp_imf'],\n",
        "    'hht_imf_count': 3,\n",
        "    'hht_emd_noise_width': 0.05,\n",
        "\n",
        "    'use_l2_features': True,   # User provided\n",
        "    'use_l2_features_for_training': True,\n",
        "    'l2_depth_imbalance_levels': [3, 5, 10, 15, 25], # User provided\n",
        "    'l2_features': [\n",
        "        'price_impact_10',\n",
        "        'bid_curve', 'ask_curve'\n",
        "        # 'depth_imb_X' features are generated based on l2_depth_imbalance_levels\n",
        "    ],\n",
        "    'l2_price_impact_depth_idx': 4,\n",
        "    'l2_curve_fit_levels': 20,\n",
        "    'l2_depth': 25, # User provided (likely for REST L2 snapshot depth in DataHandler)\n",
        "\n",
        "    # -- Label Generation --\n",
        "    'labeling_method': 'triple_barrier', # User provided\n",
        "\n",
        "    'label_volatility_window': 20, # User provided (for vol_norm_return)\n",
        "    'label_clip_quantiles': [0.01, 0.99], # User provided (for vol_norm_return)\n",
        "    'label_shift': -1,         # User provided (for vol_norm_return)\n",
        "\n",
        "    'triple_barrier_profit_target_atr_mult': 2.5, # User provided\n",
        "    'triple_barrier_stop_loss_atr_mult': 1.0,   # User provided\n",
        "    'triple_barrier_time_horizon_bars': 12,     # User provided\n",
        "    'triple_barrier_atr_column': 'atr',         # User provided\n",
        "\n",
        "    # -- Model Training --\n",
        "    'random_state': 42,\n",
        "    'test_size': 0.2,          # User provided\n",
        "    'min_training_samples': 100,\n",
        "    'train_ensemble': False,\n",
        "    'lgbm_n_jobs': -1,\n",
        "\n",
        "    'optuna_trials': 100,       # User provided\n",
        "    'optuna_n_estimators_max': 1500,\n",
        "    'optuna_early_stopping_rounds': 25,\n",
        "    'optuna_study_name': f\"lgbm_opt_BTC-USDT_1m\", # Adjusted based on user's symbol/timeframe\n",
        "    'optuna_load_if_exists': True,\n",
        "    'optuna_n_jobs': 1,\n",
        "    'optuna_timeout_seconds': None,\n",
        "\n",
        "    'optuna_search_spaces': { # User provided\n",
        "        'n_estimators': {'type': 'int', 'low': 50, 'high': 800, 'step': 25},\n",
        "        'learning_rate': {'type': 'float', 'low': 0.005, 'high': 0.1, 'log': True},\n",
        "        'num_leaves': {'type': 'int', 'low': 15, 'high': 100},\n",
        "        'max_depth': {'type': 'int', 'low': 2, 'high': 8},\n",
        "        'lambda_l1': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'lambda_l2': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'feature_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_freq': {'type': 'int', 'low': 1, 'high': 10},\n",
        "        'min_child_samples': {'type': 'int', 'low': 3, 'high': 40}\n",
        "    },\n",
        "\n",
        "    'ensemble_long_thresh': 0.5,\n",
        "    'ensemble_short_thresh': -0.5,\n",
        "    'ensemble_clf_params': {\n",
        "        'objective': 'multiclass', 'metric': 'multi_logloss',\n",
        "        'num_class': 3, 'n_estimators': 200,\n",
        "    },\n",
        "    'ensemble_reg_params': {\n",
        "        'objective': 'regression_l1', 'metric': 'mae',\n",
        "        'n_estimators': 200,\n",
        "    },\n",
        "\n",
        "    'enable_feature_selection': False, # User provided\n",
        "    'feature_selection_method': 'shap', # User provided\n",
        "    'num_features_to_select': 30,       # User provided\n",
        "\n",
        "    # -- Model Prediction & Trading Logic --\n",
        "    # 'backtest_threshold' from user's list is now 'prediction_threshold' for clarity\n",
        "    'prediction_threshold': 0.2, # User provided as backtest_threshold\n",
        "    'use_ensemble_for_backtest': False,\n",
        "    'use_ensemble_for_simulation': False,\n",
        "    'use_ensemble_for_visualization': False,\n",
        "\n",
        "    # -- Risk Management --\n",
        "    'risk_management': {\n",
        "        'max_drawdown': 0.20,\n",
        "        'volatility_lookback': 14,\n",
        "        'position_sizing_mode': 'volatility_target',\n",
        "        'volatility_target_pct': 0.02,\n",
        "        'max_equity_risk_pct': 0.05,\n",
        "        'fixed_fraction_pct': 0.02,\n",
        "        # User provided stop_loss_pct: null, take_profit_pct: null.\n",
        "        # These are not directly used by current ATR-based AdvancedRiskManager.\n",
        "        # Keeping them if user has other plans, or they can be removed if only ATR-based is used.\n",
        "        'stop_loss_pct': None, # User provided\n",
        "        'take_profit_pct': None, # User provided\n",
        "        'sl_atr_multiplier': 1.5,\n",
        "        'tp_atr_multiplier': 2.5\n",
        "    },\n",
        "    'fallback_volatility_pct_for_sizing': 0.02,\n",
        "\n",
        "    # -- Order Execution --\n",
        "    'execution': {\n",
        "        'slippage_model_pct': 0.0005,\n",
        "        'max_order_book_levels': 20,\n",
        "        'default_entry_order_type': 'market', # User provided as simulation_entry_order_type\n",
        "        'default_exit_order_type': 'limit',  # User provided as simulation_exit_order_type\n",
        "    },\n",
        "\n",
        "    # -- Backtesting --\n",
        "    'initial_balance': 10000, # User provided\n",
        "    'commission_pct': 0.0006,  # User provided\n",
        "    'leverage': 3,             # User provided\n",
        "    'fallback_atr_pct_for_backtest': 0.02,\n",
        "\n",
        "    # -- Walk-Forward Optimization --\n",
        "    'run_walk_forward_optimization': False, # Default to False, can be overridden\n",
        "    'walk_forward_train_periods': 730,      # User provided\n",
        "    'walk_forward_test_periods': 180,       # User provided\n",
        "    'walk_forward_step_periods': 180,       # User provided\n",
        "    'walk_forward_initial_warmup': 100,     # User provided\n",
        "    'walk_forward_retrain_frequency_folds': 1, # User provided\n",
        "    # fetch_ohlcv_limit_wfo is already under Data Fetching\n",
        "\n",
        "    # -- Live Simulation --\n",
        "    'run_simulation_flag': True, # User provided\n",
        "    'simulation_threshold': 0.2, # User provided\n",
        "    'fetch_live_limit': 300,   # User provided\n",
        "    'min_simulation_interval_seconds': 15,\n",
        "    'simulation_duration_seconds': 300, # User provided\n",
        "\n",
        "    # -- Visualization --\n",
        "    'show_plots': True,\n",
        "    'plot_style': 'seaborn-v0_8-darkgrid',\n",
        "    'use_shap_for_importance': True, # User provided as use_shap_override\n",
        "    'shap_max_samples': 1000,\n",
        "    'plot_figsize_equity': (14, 7),\n",
        "    'plot_figsize_lgbm': (12, 10),\n",
        "    'plot_figsize_shap_bar': (12, 10),\n",
        "    'plot_figsize_shap_dot': (12, 10),\n",
        "    'plot_figsize_emd': (14, 12),\n",
        "    'plot_figsize_features': (14, 10),\n",
        "\n",
        "    # -- Orchestrator --\n",
        "    'allow_no_exchange_init': False\n",
        "}\n",
        "\n",
        "# --- Overwrite specific keys from user's list if they differ from my structured interpretation ---\n",
        "# This step ensures the user's exact values (from their snippet) are prioritized\n",
        "# for the keys they explicitly provided.\n",
        "user_provided_config_snippet = {\n",
        "    'base_dir': BOT_BASE_DIR, # This must come from the environment\n",
        "    'symbol': 'BTC/USDT',\n",
        "    'timeframe': '1m',\n",
        "    'feature_window': 24,\n",
        "    'use_l2_features': True,\n",
        "    # 'l2_depth_levels': 25, # Renamed to l2_depth_imbalance_levels, and l2_depth is separate\n",
        "    'l2_depth': 25, # For REST L2 snapshots if DataHandler uses it directly\n",
        "    'fetch_ohlcv_limit': 2500,\n",
        "    'optuna_trials': 100,\n",
        "    'test_size': 0.2,\n",
        "    'backtest_threshold': 0.2, # This is now 'prediction_threshold'\n",
        "    'initial_balance': 10000,\n",
        "    'commission_pct': 0.0006,\n",
        "    'leverage': 3,\n",
        "    'stop_loss_pct': None, # Kept as user provided\n",
        "    'take_profit_pct': None, # Kept as user provided\n",
        "    'run_simulation_flag': True,\n",
        "    'simulation_threshold': 0.2,\n",
        "    'fetch_live_limit': 300,\n",
        "    'simulation_duration_seconds': 300,\n",
        "    'use_shap_override': True, # This is now 'use_shap_for_importance'\n",
        "    'collector_symbol': 'BTCUSDT',\n",
        "    'collector_duration': 1,\n",
        "    'collector_unit': 'minutes',\n",
        "    'collector_depth': 50,\n",
        "    'collector_category': 'linear',\n",
        "    'labeling_method': 'triple_barrier',\n",
        "    'label_volatility_window': 20,\n",
        "    'label_clip_quantiles': [0.01, 0.99],\n",
        "    'label_shift': -1,\n",
        "    'triple_barrier_profit_target_atr_mult': 2.5,\n",
        "    'triple_barrier_stop_loss_atr_mult': 1.0,\n",
        "    'triple_barrier_time_horizon_bars': 12,\n",
        "    'triple_barrier_atr_column': 'atr',\n",
        "    'walk_forward_train_periods': 730,\n",
        "    'walk_forward_test_periods': 180,\n",
        "    'walk_forward_step_periods': 180,\n",
        "    'walk_forward_initial_warmup': 100,\n",
        "    'walk_forward_retrain_frequency_folds': 1,\n",
        "    'fetch_ohlcv_limit_wfo': 5000,\n",
        "    'enable_feature_selection': False,\n",
        "    'feature_selection_method': 'shap',\n",
        "    'num_features_to_select': 30,\n",
        "    'optuna_search_spaces': {\n",
        "        'n_estimators': {'type': 'int', 'low': 50, 'high': 800, 'step': 25},\n",
        "        'learning_rate': {'type': 'float', 'low': 0.005, 'high': 0.1, 'log': True},\n",
        "        'num_leaves': {'type': 'int', 'low': 15, 'high': 100},\n",
        "        'max_depth': {'type': 'int', 'low': 2, 'high': 8},\n",
        "        'lambda_l1': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'lambda_l2': {'type': 'float', 'low': 1e-8, 'high': 10.0, 'log': True},\n",
        "        'feature_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_fraction': {'type': 'float', 'low': 0.4, 'high': 0.9},\n",
        "        'bagging_freq': {'type': 'int', 'low': 1, 'high': 10},\n",
        "        'min_child_samples': {'type': 'int', 'low': 3, 'high': 40}\n",
        "    },\n",
        "    # Mapping user's 'simulation_entry/exit_order_type' to 'execution' dict\n",
        "    # 'simulation_entry_order_type': 'market', # Will be handled below\n",
        "    # 'simulation_exit_order_type': 'limit',   # Will be handled below\n",
        "    'l2_depth_imbalance_levels': [3, 5, 10, 15, 25],\n",
        "    'ta_indicator_params': {\n",
        "        'rsi': {'length': 10, 'scalar': 100},\n",
        "        'macd': {'fast': 8, 'slow': 21, 'signal': 5},\n",
        "        'bbands': {'length': 20, 'std': 2.5}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Update the main 'config' dictionary with the user's exact values for the keys they provided\n",
        "# and handle specific mappings.\n",
        "for key, value in user_provided_config_snippet.items():\n",
        "    if key == 'backtest_threshold':\n",
        "        config['prediction_threshold'] = value\n",
        "    elif key == 'use_shap_override':\n",
        "        config['use_shap_for_importance'] = value\n",
        "    # elif key == 'l2_depth_levels': # User had this, map to l2_depth_imbalance_levels for FeatureEngineer\n",
        "    #     config['l2_depth_imbalance_levels'] = value\n",
        "    #     # 'l2_depth' is a separate parameter for DataHandler's REST L2 snapshot\n",
        "    elif key == 'simulation_entry_order_type':\n",
        "        config['execution']['default_entry_order_type'] = value\n",
        "    elif key == 'simulation_exit_order_type':\n",
        "        config['execution']['default_exit_order_type'] = value\n",
        "    else:\n",
        "        config[key] = value\n",
        "\n",
        "# Ensure base_dir is always from the environment variable\n",
        "config['base_dir'] = BOT_BASE_DIR\n",
        "# Correctly derive l2_collection_duration_seconds from collector_duration and collector_unit\n",
        "if config.get('collector_unit') == 'minutes':\n",
        "    config['l2_collection_duration_seconds'] = config.get('collector_duration', 1) * 60\n",
        "elif config.get('collector_unit') == 'hours':\n",
        "    config['l2_collection_duration_seconds'] = config.get('collector_duration', 1) * 3600\n",
        "else: # Default to seconds if unit is unclear or missing\n",
        "    config['l2_collection_duration_seconds'] = config.get('collector_duration', 60)\n",
        "\n",
        "\n",
        "# --- Save Configuration to YAML File ---\n",
        "config_file_path = os.path.join(BOT_BASE_DIR, 'config.yaml')\n",
        "try:\n",
        "    with open(config_file_path, 'w') as f:\n",
        "        yaml.dump(config, f, sort_keys=False, indent=4, width=120, Dumper=yaml.SafeDumper) # Use SafeDumper\n",
        "    print(f\"\\nConfiguration (User Updated for Phase 1) saved to: {config_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError saving configuration: {e}\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# --- Display a snippet of the config for verification ---\n",
        "print(\"\\n--- Configuration Snippet (User Update for Phase 1) ---\")\n",
        "keys_to_display = [\n",
        "    'exchange_name', 'symbol', 'timeframe', 'base_dir',\n",
        "    'labeling_method', 'train_ensemble',\n",
        "    'run_walk_forward_optimization', 'run_simulation_flag',\n",
        "    'enable_feature_selection', 'prediction_threshold',\n",
        "    'l2_depth_imbalance_levels'\n",
        "]\n",
        "for key in keys_to_display:\n",
        "    if key in config:\n",
        "        print(f\"{key}: {config[key]}\")\n",
        "if 'optuna_search_spaces' in config and config['optuna_search_spaces']:\n",
        "    print(f\"optuna_search_spaces (first item key): {list(config['optuna_search_spaces'].keys())[0] if config['optuna_search_spaces'] else 'Not set'}\")\n",
        "if 'ta_indicator_params' in config and config['ta_indicator_params']:\n",
        "     print(f\"ta_indicator_params (first item key): {list(config['ta_indicator_params'].keys())[0] if config['ta_indicator_params'] else 'Not set'}\")\n",
        "print(\"-----------------------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPiZvwMtxx-w"
      },
      "outputs": [],
      "source": [
        "# Cell 3: L2 Data Collector (Refactored Usage)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import time # For a brief pause if needed\n",
        "from datetime import datetime, timezone # For logging\n",
        "import traceback # For error printing\n",
        "\n",
        "# --- Ensure BOT_BASE_DIR is available (set in Cell 1) ---\n",
        "BOT_BASE_DIR = os.environ.get('BOT_BASE_DIR')\n",
        "if BOT_BASE_DIR is None:\n",
        "    print(\"CRITICAL ERROR (Cell 3): BOT_BASE_DIR is not set. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Run Cell 1 to define it.\")\n",
        "\n",
        "# --- Add BOT_BASE_DIR to Python path to allow importing l2_data_collector ---\n",
        "# This should have been done in Cell 1 already, but good to ensure for standalone cell execution if possible\n",
        "if BOT_BASE_DIR not in sys.path:\n",
        "    sys.path.append(BOT_BASE_DIR)\n",
        "    print(f\"Info (Cell 3): Added {BOT_BASE_DIR} to sys.path for L2DataCollector import.\")\n",
        "\n",
        "# --- Import the refactored class ---\n",
        "L2DataCollector = None # Initialize to None\n",
        "try:\n",
        "    from l2_data_collector import L2DataCollector # Assumes l2_data_collector.py is in BOT_BASE_DIR\n",
        "    print(\"L2DataCollector class imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR (Cell 3): Could not import L2DataCollector: {e}\")\n",
        "    print(f\"Please ensure 'l2_data_collector.py' is in the directory: {BOT_BASE_DIR} and that Cell 1 was run.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during L2DataCollector import: {e}\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# --- Load Main Configuration to Extract Collector Settings ---\n",
        "# This assumes config.yaml is in BOT_BASE_DIR and was created by Cell 2.\n",
        "config_file_path_main = os.path.join(BOT_BASE_DIR, 'config.yaml')\n",
        "main_config_for_l2 = {} # Use a distinct variable name\n",
        "collector_specific_config = {}\n",
        "\n",
        "if os.path.exists(config_file_path_main):\n",
        "    try:\n",
        "        with open(config_file_path_main, 'r') as f:\n",
        "            main_config_for_l2 = yaml.safe_load(f)\n",
        "        if main_config_for_l2:\n",
        "            print(f\"Successfully loaded main configuration from {config_file_path_main} for L2 collector settings.\")\n",
        "\n",
        "            # Extract collector-specific settings from the main config.\n",
        "            # The L2DataCollector class itself has defaults, but we use main_config to override them.\n",
        "            # The L2DataCollector's __init__ expects a 'config' dict.\n",
        "            collector_specific_config = {\n",
        "                'symbol': main_config_for_l2.get('collector_symbol', 'BTCUSDT'), # L2Collector uses 'symbol' for its target\n",
        "                'market_type': main_config_for_l2.get('collector_category', main_config_for_l2.get('market_type', 'linear')),\n",
        "                'exchange_name': main_config_for_l2.get('exchange_name', 'bybit'),\n",
        "                'l2_data_folder': main_config_for_l2.get('l2_data_folder', 'l2_data'),\n",
        "                'l2_log_file': main_config_for_l2.get('l2_log_file', 'l2_data_collector.log'), # Log file name for L2 collector\n",
        "                'l2_max_file_size_mb': main_config_for_l2.get('l2_max_file_size_mb', 20),\n",
        "                'l2_collection_duration_seconds': main_config_for_l2.get('l2_collection_duration_seconds', 300),\n",
        "                'l2_websocket_depth': main_config_for_l2.get('collector_depth', 50)\n",
        "            }\n",
        "            print(\"L2 Collector parameters extracted from main config:\")\n",
        "            for k, v in collector_specific_config.items():\n",
        "                print(f\"  {k}: {v}\")\n",
        "        else:\n",
        "            print(f\"Warning (Cell 3): Main config.yaml at {config_file_path_main} was empty. L2 Collector might use class defaults.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning (Cell 3): Could not load or parse main config.yaml ({e}). L2 Collector might use class defaults or fail if critical params missing.\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\"Warning (Cell 3): Main config.yaml not found at {config_file_path_main}. L2 Collector will use its internal defaults if not provided in collector_specific_config.\")\n",
        "\n",
        "\n",
        "# --- Instantiate and Run the Collector ---\n",
        "# Check if the L2DataCollector class was successfully imported AND if we have some config for it\n",
        "if L2DataCollector is not None:\n",
        "    if not collector_specific_config and not main_config_for_l2: # If no config could be loaded at all\n",
        "        print(\"ERROR (Cell 3): No configuration available for L2DataCollector. Cannot proceed.\")\n",
        "    else:\n",
        "        print(\"\\n--- Initializing and Starting L2 Data Collector ---\")\n",
        "\n",
        "        # The L2DataCollector's __init__ expects 'config' (which are the collector_specific_config here)\n",
        "        # and 'bot_base_dir'.\n",
        "        l2_collector_instance = None\n",
        "        try:\n",
        "            l2_collector_instance = L2DataCollector(config=collector_specific_config, bot_base_dir=BOT_BASE_DIR)\n",
        "\n",
        "            # The start_collection_websocket method contains its own loop based on l2_collection_duration_seconds.\n",
        "            # This cell will effectively block until that duration is over or an interrupt occurs.\n",
        "            duration_to_run = collector_specific_config.get('l2_collection_duration_seconds', 300) # Get from extracted params\n",
        "            print(f\"L2 Collector is configured to run for approximately {duration_to_run / 60:.1f} minutes.\")\n",
        "            print(\"You can interrupt the kernel (Runtime -> Interrupt execution or Ctrl+M I) to stop it sooner.\")\n",
        "            print(f\"L2 data will be saved in: {os.path.join(BOT_BASE_DIR, collector_specific_config.get('l2_data_folder', 'l2_data'))}\")\n",
        "            print(f\"L2 collector log file: {os.path.join(BOT_BASE_DIR, collector_specific_config.get('l2_log_file', 'l2_data_collector.log'))}\")\n",
        "\n",
        "            l2_collector_instance.start_collection_websocket()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nL2 Data Collection interrupted by user in notebook cell.\")\n",
        "            if l2_collector_instance:\n",
        "                l2_collector_instance.stop_collection_websocket() # Ensure graceful shutdown\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while running the L2 Data Collector: {e}\")\n",
        "            traceback.print_exc()\n",
        "            if l2_collector_instance and getattr(l2_collector_instance, 'ws', None) is not None : # Check if ws object exists\n",
        "                l2_collector_instance.stop_collection_websocket()\n",
        "        finally:\n",
        "            print(\"--- L2 Data Collector Cell Execution Finished ---\")\n",
        "            # Note: If start_collection_websocket runs in a blocking way for its duration,\n",
        "            # this \"finished\" message will appear after the collection period.\n",
        "else:\n",
        "    print(\"\\nL2DataCollector class not available (import failed). Cannot start L2 data collection.\")\n",
        "    print(\"Ensure 'l2_data_collector.py' is in your BOT_BASE_DIR and Cell 1 has been run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tyw0dYhoiNq"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Imports & Global Setup (Main Bot)\n",
        "\n",
        "# --- Core Libraries ---\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "import warnings\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import sys\n",
        "import yaml # For loading config\n",
        "import pickle # For saving/loading ensemble models\n",
        "\n",
        "# --- Add BOT_BASE_DIR to Python path ---\n",
        "# This allows importing custom modules from this directory\n",
        "# Ensure BOT_BASE_DIR is set, typically in Cell 1.\n",
        "bot_base_dir = os.environ.get('BOT_BASE_DIR')\n",
        "if bot_base_dir is None:\n",
        "    print(\"CRITICAL ERROR (Cell 4): BOT_BASE_DIR is not set in environment. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Run Cell 1 to define it.\")\n",
        "\n",
        "if BOT_BASE_DIR not in sys.path: # Should have been added in Cell 1, but double-check\n",
        "    sys.path.append(BOT_BASE_DIR)\n",
        "    print(f\"Info (Cell 4): Added {BOT_BASE_DIR} to sys.path\")\n",
        "\n",
        "# --- Data Handling & Numerics ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Machine Learning ---\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# --- Load Configuration ---\n",
        "print(\"Loading configuration...\")\n",
        "config_file_path = os.path.join(bot_base_dir, 'config.yaml')\n",
        "config = {}\n",
        "try:\n",
        "    with open(config_file_path, 'r') as f:\n",
        "        main_config_loader = yaml.safe_load(f)\n",
        "        if main_config_loader:\n",
        "            config = main_config_loader\n",
        "    print(f\"Configuration loaded successfully from {config_file_path}\")\n",
        "    if not config:\n",
        "        print(\"Warning (Cell 4): Configuration file was empty. Using empty config dict.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR (Cell 4): Configuration file not found at {config_file_path}. Please run Cell 2 to create it.\")\n",
        "    print(\"Using empty config dict. Bot may not function correctly.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error (Cell 4) loading configuration: {e}\")\n",
        "    traceback.print_exc()\n",
        "    print(\"Using empty config dict. Bot may not function correctly.\")\n",
        "\n",
        "# --- Optional Libraries Setup ---\n",
        "print(\"\\nSetting up optional libraries and global flags...\")\n",
        "try:\n",
        "    import optuna\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "    HAS_OPTUNA = True\n",
        "    print(\"Optuna loaded.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): Optuna not found. Model training requiring it may fail.\")\n",
        "    HAS_OPTUNA = False\n",
        "    optuna = None\n",
        "\n",
        "try:\n",
        "    from PyEMD import EMD\n",
        "    HAS_PYEMD = True\n",
        "    print(\"PyEMD (EMD-signal) library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): PyEMD (EMD-signal) not found. HHT features will be disabled.\")\n",
        "    HAS_PYEMD = False\n",
        "    class EMD:\n",
        "        def __init__(self, *args, **kwargs): pass\n",
        "        def __call__(self, signal, *args, **kwargs): return np.array([])\n",
        "        def get_imfs_and_residue(self): return np.array([]), np.array([])\n",
        "    print(\"Using dummy EMD class.\")\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    HAS_SHAP = True\n",
        "    print(\"SHAP library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): SHAP library not found. SHAP plots will be disabled.\")\n",
        "    HAS_SHAP = False\n",
        "    shap = None\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    if config and 'plot_style' in config:\n",
        "        try:\n",
        "            plt.style.use(config.get('plot_style'))\n",
        "            print(f\"Applied plot style: {config.get('plot_style')}\")\n",
        "        except Exception as e_style:\n",
        "            print(f\"Warning (Cell 4): Could not apply plot style '{config.get('plot_style')}': {e_style}\")\n",
        "    HAS_MATPLOTLIB = True\n",
        "    print(\"Matplotlib loaded.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): Matplotlib not found. Plotting will be disabled.\")\n",
        "    HAS_MATPLOTLIB = False\n",
        "    plt = None\n",
        "\n",
        "try:\n",
        "    from scipy.signal import hilbert\n",
        "    HAS_SCIPY_HILBERT = True\n",
        "    print(\"Scipy (signal.hilbert) loaded.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): Scipy.signal.hilbert not found. HHT features requiring it may fail.\")\n",
        "    HAS_SCIPY_HILBERT = False\n",
        "    def hilbert(signal, N=None, axis=-1):\n",
        "        print(\"Error (Cell 4): hilbert function called but scipy.signal.hilbert not available.\")\n",
        "        return np.zeros_like(signal) + 1j * np.zeros_like(signal)\n",
        "    print(\"Using dummy hilbert function.\")\n",
        "\n",
        "try:\n",
        "    import pandas_ta as ta\n",
        "    HAS_PANDAS_TA = True\n",
        "    print(\"Pandas TA library loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING (Cell 4): pandas_ta library not found. Advanced TA features relying on it will be disabled.\")\n",
        "    HAS_PANDAS_TA = False\n",
        "    ta = None\n",
        "\n",
        "# --- Exchange Interaction ---\n",
        "try:\n",
        "    import ccxt\n",
        "    print(\"CCXT loaded.\")\n",
        "except ImportError:\n",
        "    print(\"ERROR (Cell 4): CCXT not found. Exchange interaction will fail.\")\n",
        "    ccxt = None\n",
        "\n",
        "# --- Custom Helper Class Imports ---\n",
        "print(\"\\nImporting custom bot classes...\")\n",
        "custom_classes_to_import = [\n",
        "    \"AdvancedRiskManager\", \"SmartOrderExecutor\", \"DataHandler\",\n",
        "    \"FeatureEngineer\", \"LabelGenerator\", \"ModelTrainer\",\n",
        "    \"ModelPredictor\", \"StrategyBacktester\", \"LiveSimulator\",\n",
        "    \"Visualizer\", \"TradingBotOrchestrator\"\n",
        "]\n",
        "\n",
        "for class_name_str in custom_classes_to_import:\n",
        "    try:\n",
        "        module_name = class_name_str.lower()\n",
        "        module = __import__(module_name)\n",
        "        globals()[class_name_str] = getattr(module, class_name_str)\n",
        "        print(f\"{class_name_str} class loaded successfully from {module_name}.py\")\n",
        "    except ImportError as e:\n",
        "        print(f\"ERROR (Cell 4) importing {class_name_str} from {class_name_str.lower()}.py: {e}\")\n",
        "        print(f\"Please ensure '{class_name_str.lower()}.py' is in '{bot_base_dir}' or Python path and contains plain Python code.\")\n",
        "        globals()[class_name_str] = None\n",
        "    except AttributeError as e:\n",
        "        print(f\"ERROR (Cell 4): Attribute {class_name_str} not found in module {class_name_str.lower()}.py. Check class name. Error: {e}\")\n",
        "        globals()[class_name_str] = None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while importing {class_name_str}: {e}\")\n",
        "        traceback.print_exc()\n",
        "        globals()[class_name_str] = None\n",
        "\n",
        "# --- API Key Loading (Using Colab Secrets or Environment Variables) ---\n",
        "print(\"\\nLoading API Keys...\")\n",
        "BYBIT_API_KEY = None\n",
        "BYBIT_API_SECRET = None\n",
        "\n",
        "# Try Colab userdata first\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    # --- MODIFIED TO USE TESTNET KEY NAMES ---\n",
        "    BYBIT_API_KEY = userdata.get(\"BYBIT_API_KEY_MAIN_TEST\")\n",
        "    BYBIT_API_SECRET = userdata.get(\"BYBIT_API_SECRET_MAIN_TEST\")\n",
        "\n",
        "    if BYBIT_API_KEY and BYBIT_API_SECRET:\n",
        "        print(\"TESTNET API Keys loaded successfully from Colab secrets.\")\n",
        "    else:\n",
        "        print(\"*** WARNING: TESTNET API Keys not found or empty in Colab secrets ('BYBIT_API_KEY_MAIN_TEST', 'BYBIT_API_SECRET_MAIN_TEST'). ***\")\n",
        "        BYBIT_API_KEY = None\n",
        "        BYBIT_API_SECRET = None\n",
        "except ImportError:\n",
        "    print(\"Not in Colab environment. Will check environment variables for API keys.\")\n",
        "    # If not found in Colab secrets, try environment variables\n",
        "    if not (BYBIT_API_KEY and BYBIT_API_SECRET): # Check again in case Colab import failed but env vars might exist\n",
        "        BYBIT_API_KEY = os.environ.get(\"BYBIT_API_KEY_MAIN_TEST\") # Check for TESTNET env vars\n",
        "        BYBIT_API_SECRET = os.environ.get(\"BYBIT_API_SECRET_MAIN_TEST\")\n",
        "        if BYBIT_API_KEY and BYBIT_API_SECRET:\n",
        "            print(\"TESTNET API Keys loaded successfully from environment variables.\")\n",
        "        else:\n",
        "            print(\"WARNING (Cell 4): TESTNET API Keys not found in Colab secrets or environment variables ('BYBIT_API_KEY_MAIN_TEST', 'BYBIT_API_SECRET_MAIN_TEST'). Live trading requiring authentication will fail if testnet keys are intended.\")\n",
        "except Exception as e: # Catch other potential errors during userdata.get()\n",
        "    print(f\"An unexpected error occurred loading secrets: {e}\")\n",
        "    traceback.print_exc()\n",
        "    BYBIT_API_KEY = None\n",
        "    BYBIT_API_SECRET = None\n",
        "\n",
        "# --- Warnings Configuration ---\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "print(\"\\nImports and global setup complete.\")\n",
        "if config:\n",
        "    print(f\"Config dictionary loaded with {len(config)} top-level keys.\")\n",
        "    print(f\"Config 'exchange_testnet' is set to: {config.get('exchange_testnet')}\")\n",
        "else:\n",
        "    print(\"CRITICAL WARNING (Cell 4): Config dictionary is empty or failed to load. Bot will likely not function correctly.\")\n",
        "\n",
        "# Explicit import of core orchestrator class\n",
        "try:\n",
        "    from trading_bot_orchestrator import TradingBotOrchestrator\n",
        "    print(\"TradingBotOrchestrator successfully imported.\")\n",
        "except ImportError as e:\n",
        "    print(\"CRITICAL ERROR: TradingBotOrchestrator could not be imported:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEZoYVZ948Is"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# Cell 5 · Trading Bot Orchestrator Initialization   (patched)\n",
        "# ================================================================\n",
        "import os, traceback, inspect\n",
        "\n",
        "# ---------- 0 · Pre-flight checks --------------------------------\n",
        "if \"BOT_BASE_DIR\" not in os.environ:\n",
        "    raise EnvironmentError(\"Cell 5: BOT_BASE_DIR not set – run Cell 1 first.\")\n",
        "BOT_BASE_DIR = os.environ[\"BOT_BASE_DIR\"]\n",
        "print(f\"Info:  BOT_BASE_DIR → {BOT_BASE_DIR}\")\n",
        "\n",
        "if \"config\" not in globals() or not isinstance(config, dict) or not config:\n",
        "    raise ValueError(\"Cell 5: global 'config' missing or empty – did you run Cells 2 & 4?\")\n",
        "print(f\"Info:  base config has {len(config)} keys.\")\n",
        "\n",
        "# ---------- 1 · Build Bybit-Testnet exchange_kwargs --------------\n",
        "exchange_kwargs = {\n",
        "    \"enableRateLimit\": True,\n",
        "    \"options\": {\n",
        "        \"defaultType\": \"linear\",          # USDT-perps\n",
        "        \"fetchCurrencies\": False,         # skip private /asset/ call\n",
        "        \"urls\": {                         # point both routes at test-net\n",
        "            \"api\": {\n",
        "                \"public\":  \"https://api-testnet.bybit.com\",\n",
        "                \"private\": \"https://api-testnet.bybit.com\",\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "    # no apiKey / secret → public-only access\n",
        "}\n",
        "\n",
        "# ---------- 2 · Merge into notebook-level config -----------------\n",
        "config.update({\n",
        "    \"exchange_name\":        \"bybit\",\n",
        "    \"exchange_kwargs\":      exchange_kwargs,\n",
        "    \"allow_no_exchange_init\": False,      # fail hard if even public init breaks\n",
        "})\n",
        "\n",
        "# ---------- 3 · Collect global library flags / modules -----------\n",
        "global_library_flags = {\n",
        "    name: globals().get(name, False) for name in [\n",
        "        \"HAS_OPTUNA\", \"HAS_PYEMD\", \"HAS_SCIPY_HILBERT\",\n",
        "        \"HAS_SHAP\", \"HAS_MATPLOTLIB\", \"HAS_PANDAS_TA\"\n",
        "    ]\n",
        "}\n",
        "global_library_modules = {\n",
        "    name: globals().get(name) for name in [\n",
        "        \"optuna\", \"EMD\", \"hilbert\", \"shap\", \"plt\", \"ta\", \"ccxt\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ---------- 4 · Instantiate TradingBotOrchestrator ---------------\n",
        "print(\"\\n--- Initialising TradingBotOrchestrator ---\")\n",
        "try:\n",
        "    sig = inspect.signature(TradingBotOrchestrator)\n",
        "    if \"bot_base_dir\" in sig.parameters:                   # old signature\n",
        "        bot_orchestrator = TradingBotOrchestrator(\n",
        "            config=config,\n",
        "            bot_base_dir=BOT_BASE_DIR,\n",
        "            global_library_flags=global_library_flags,\n",
        "            global_library_modules=global_library_modules,\n",
        "            api_key=None, api_secret=None,                # public-only\n",
        "        )\n",
        "    else:                                                  # new signature – dir in config\n",
        "        config[\"bot_base_dir\"] = BOT_BASE_DIR\n",
        "        bot_orchestrator = TradingBotOrchestrator(\n",
        "            config=config,\n",
        "            global_library_flags=global_library_flags,\n",
        "            global_library_modules=global_library_modules,\n",
        "            api_key=None, api_secret=None,\n",
        "        )\n",
        "\n",
        "    if bot_orchestrator.exchange:\n",
        "        print(f\"✅  Orchestrator ready – {len(bot_orchestrator.exchange.symbols)} symbols loaded from Bybit test-net\")\n",
        "    else:\n",
        "        print(\"⚠️  Orchestrator initialised but exchange is None (check allow_no_exchange_init flag)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌  Failed to initialise TradingBotOrchestrator: {e}\")\n",
        "    traceback.print_exc()\n",
        "    bot_orchestrator = None\n",
        "\n",
        "print(\"\\n--- Cell 5 done ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSEBAz9Zxpur"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Trading Bot Orchestrator Initialization\n",
        "\n",
        "# This cell instantiates the main TradingBotOrchestrator class.\n",
        "# The complex logic of the old CombinedTradingBot has been moved to specialized classes\n",
        "# which were imported in Cell 4 and are instantiated by the TradingBotOrchestrator.\n",
        "\n",
        "import os # For BOT_BASE_DIR check\n",
        "import traceback # For printing stack trace on error\n",
        "\n",
        "# --- Ensure prerequisite variables from Cell 1 and Cell 4 are available ---\n",
        "# These checks are important for notebook execution flow.\n",
        "if 'BOT_BASE_DIR' not in os.environ or os.environ.get('BOT_BASE_DIR') is None:\n",
        "    print(\"CRITICAL ERROR (Cell 5): BOT_BASE_DIR is not set. Please run Cell 1 first.\")\n",
        "    raise EnvironmentError(\"BOT_BASE_DIR not set. Ensure Cell 1 has been executed.\")\n",
        "else:\n",
        "    bot_base_dir_check = os.environ.get('BOT_BASE_DIR') # To use in this cell if needed, already global from Cell 4\n",
        "    print(f\"Info (Cell 5): Using BOT_BASE_DIR: {bot_base_dir_check}\")\n",
        "\n",
        "\n",
        "if 'config' not in globals() or not isinstance(config, dict) or not config:\n",
        "    print(\"CRITICAL ERROR (Cell 5): 'config' dictionary not found, not a dict, or is empty.\")\n",
        "    print(\"Please ensure Cell 2 (Config Settings) and Cell 4 (Imports & Global Setup) have been run successfully.\")\n",
        "    raise ValueError(\"'config' dictionary not available or invalid. Ensure Cell 2 and Cell 4 have been executed.\")\n",
        "else:\n",
        "    print(f\"Info (Cell 5): 'config' dictionary loaded with {len(config)} keys.\")\n",
        "\n",
        "print(\"\\n--- Initializing Trading Bot Orchestrator ---\")\n",
        "\n",
        "# Gather global library flags and modules (expected to be set in Cell 4)\n",
        "# These are passed to the orchestrator to inform component initializations.\n",
        "# Using globals().get() for safety, providing False/None as defaults.\n",
        "global_library_flags = {\n",
        "    'HAS_OPTUNA': globals().get('HAS_OPTUNA', False),\n",
        "    'HAS_PYEMD': globals().get('HAS_PYEMD', False),\n",
        "    'HAS_SCIPY_HILBERT': globals().get('HAS_SCIPY_HILBERT', False),\n",
        "    'HAS_SHAP': globals().get('HAS_SHAP', False),\n",
        "    'HAS_MATPLOTLIB': globals().get('HAS_MATPLOTLIB', False),\n",
        "    'HAS_PANDAS_TA': globals().get('HAS_PANDAS_TA', False)\n",
        "}\n",
        "print(f\"Info (Cell 5): Global library flags collected: {global_library_flags}\")\n",
        "\n",
        "global_library_modules = {\n",
        "    'optuna': globals().get('optuna'),\n",
        "    'EMD': globals().get('EMD'),\n",
        "    'hilbert': globals().get('hilbert'),\n",
        "    'shap': globals().get('shap'),\n",
        "    'plt': globals().get('plt'),\n",
        "    'ta': globals().get('ta'),\n",
        "    'ccxt': globals().get('ccxt') # Pass the ccxt module itself\n",
        "}\n",
        "# Simple check for modules\n",
        "# for mod_name, mod_obj in global_library_modules.items():\n",
        "#     print(f\"Info (Cell 5): Module '{mod_name}' is {'available' if mod_obj else 'NOT available (or dummy)'}\")\n",
        "\n",
        "\n",
        "# Check if TradingBotOrchestrator class was imported successfully in Cell 4\n",
        "bot_orchestrator = None # Initialize to None\n",
        "if 'TradingBotOrchestrator' in globals() and TradingBotOrchestrator is not None:\n",
        "    try:\n",
        "        print(\"Instantiating TradingBotOrchestrator...\")\n",
        "        bot_orchestrator = TradingBotOrchestrator(\n",
        "            config=config,\n",
        "            api_key=globals().get('BYBIT_API_KEY'),\n",
        "            api_secret=globals().get('BYBIT_API_SECRET'),\n",
        "            global_library_flags=global_library_flags,\n",
        "            global_library_modules=global_library_modules\n",
        "        )\n",
        "\n",
        "        # Check if the exchange component within the orchestrator was initialized\n",
        "        if bot_orchestrator.exchange:\n",
        "            print(\"TradingBotOrchestrator instance created and exchange initialized successfully.\")\n",
        "        elif config.get('allow_no_exchange_init', False): # If no exchange is okay (e.g. offline analysis)\n",
        "            print(\"Info (Cell 5): TradingBotOrchestrator instance created, but without a live exchange connection (as per 'allow_no_exchange_init' config). Some functionalities like live trading/simulation will be unavailable.\")\n",
        "        else:\n",
        "            # This case means exchange init failed AND it was required\n",
        "            print(\"CRITICAL (Cell 5): TradingBotOrchestrator instance created, but its exchange component FAILED to initialize, and 'allow_no_exchange_init' is False. Live trading/simulation and data fetching will not work.\")\n",
        "            # Depending on the desired behavior, you might want to nullify bot_orchestrator here or raise an error\n",
        "            # to prevent Cell 6 from running with a non-functional orchestrator.\n",
        "            # For now, it will proceed but Cell 6 should check bot_orchestrator.exchange.\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR (Cell 5): Failed to instantiate TradingBotOrchestrator: {e}\")\n",
        "        traceback.print_exc()\n",
        "        bot_orchestrator = None # Ensure it's None on failure\n",
        "else:\n",
        "    print(\"ERROR (Cell 5): TradingBotOrchestrator class not imported or not available.\")\n",
        "    print(\"Please ensure Cell 4 ran successfully and 'trading_bot_orchestrator.py' is correct and in the BOT_BASE_DIR.\")\n",
        "\n",
        "# The old CombinedTradingBot class definition should be REMOVED from this cell.\n",
        "# All its logic is now distributed across the imported .py files and managed by TradingBotOrchestrator.\n",
        "\n",
        "print(\"\\n--- Orchestrator Initialization Attempt Finished ---\")\n",
        "if bot_orchestrator:\n",
        "    print(f\"Orchestrator object: {bot_orchestrator}\")\n",
        "    if bot_orchestrator.exchange:\n",
        "        print(f\"Orchestrator exchange object: {bot_orchestrator.exchange.id if bot_orchestrator.exchange else 'None'}\")\n",
        "else:\n",
        "    print(\"Orchestrator object is None (failed to initialize).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPjaEQj5hhnV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7mjkmzhhjbe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMrqQdIUxelt"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Main Execution Workflow\n",
        "\n",
        "# This cell assumes that 'bot_orchestrator' has been successfully initialized in Cell 5,\n",
        "# and 'config' (the configuration dictionary) is also available from Cell 4.\n",
        "# It also assumes that all necessary library flags (e.g., HAS_MATPLOTLIB) and\n",
        "# modules (e.g., plt) are globally available from Cell 4.\n",
        "\n",
        "import pandas as pd # For pd.option_context\n",
        "import traceback # For printing stack trace on error\n",
        "import sys # For checking interactive mode\n",
        "\n",
        "if 'bot_orchestrator' in globals() and bot_orchestrator is not None and \\\n",
        "   (bot_orchestrator.exchange or config.get('allow_no_exchange_init', False)):\n",
        "    print(\"\\n--- Starting Main Execution Workflow via Orchestrator ---\")\n",
        "\n",
        "    # --- CHOOSE YOUR WORKFLOW ---\n",
        "    # Set 'RUN_WALK_FORWARD' to True to execute Walk-Forward Optimization.\n",
        "    # Otherwise, the standard single train/backtest/simulation workflow will run.\n",
        "    RUN_WALK_FORWARD = config.get('run_walk_forward_optimization', False) # Get from config\n",
        "\n",
        "    if RUN_WALK_FORWARD:\n",
        "        print(\"\\n*** EXECUTING WALK-FORWARD OPTIMIZATION WORKFLOW ***\")\n",
        "        try:\n",
        "            bot_orchestrator.run_full_workflow(run_wfo=True)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the Walk-Forward Optimization workflow: {e}\")\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"\\n*** EXECUTING STANDARD WORKFLOW (Single Train/Backtest/Sim) ***\")\n",
        "        try:\n",
        "            bot_orchestrator.run_full_workflow(run_wfo=False)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the standard workflow: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # --- Granular Workflow Control (Optional - Uncomment sections to run specific parts) ---\n",
        "    # This is useful for debugging or re-running specific stages after making config changes.\n",
        "    # Ensure that the necessary preceding stages have been successfully completed or data is available.\n",
        "\n",
        "    # print(\"\\n--- Granular Workflow Control (Example) ---\")\n",
        "\n",
        "    # --- Stage 1: Prepare Data for Training ---\n",
        "    # data_prepared_successfully = False\n",
        "    # print(\"\\n--- Stage 1: Preparing Data ---\")\n",
        "    # try:\n",
        "    #     # For a standard run, df_input can be None to load full history.\n",
        "    #     # For re-running a specific WFO fold's data prep, you'd need to load that slice.\n",
        "    #     if bot_orchestrator.prepare_data_for_training(df_input=None, save_features=True, save_ohlcv=True):\n",
        "    #         print(\"Data preparation for training was successful.\")\n",
        "    #         data_prepared_successfully = True\n",
        "    #         # You can inspect the prepared dataframes:\n",
        "    #         # print(\"Historical Data Head:\\n\", bot_orchestrator.df_historical_data.head())\n",
        "    #         # print(\"Features DataFrame Head:\\n\", bot_orchestrator.df_features.head())\n",
        "    #         # print(\"Labeled Features DataFrame Head:\\n\", bot_orchestrator.df_labeled_features.head())\n",
        "    #     else:\n",
        "    #         print(\"Data preparation for training failed. Check logs.\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error during data preparation: {e}\")\n",
        "    #     traceback.print_exc()\n",
        "\n",
        "    # --- Stage 2: Train Model ---\n",
        "    # model_trained_successfully = False\n",
        "    # if data_prepared_successfully:\n",
        "    #     print(\"\\n--- Stage 2: Training Model ---\")\n",
        "    #     try:\n",
        "    #         # Trains on self.df_labeled_features by default if df_training_data is None\n",
        "    #         if bot_orchestrator.train_model(save_model=True):\n",
        "    #             print(\"Model training was successful.\")\n",
        "    #             model_trained_successfully = True\n",
        "    #             # print(\"Trained features list:\", bot_orchestrator.trained_features_list)\n",
        "    #         else:\n",
        "    #             print(\"Model training failed. Check logs.\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error during model training: {e}\")\n",
        "    #         traceback.print_exc()\n",
        "    # else:\n",
        "    #     print(\"Skipping model training (data not prepared).\")\n",
        "\n",
        "    # --- Stage 3: Run Backtest ---\n",
        "    # backtest_run_successfully = False\n",
        "    # # A model needs to be available either from current session training or loaded.\n",
        "    # model_is_ready_for_backtest = (bot_orchestrator.trained_model_booster is not None or\n",
        "    #                                bot_orchestrator.trained_ensemble_models is not None or\n",
        "    #                                (bot_orchestrator.model_predictor and bot_orchestrator.model_predictor.model_object is not None))\n",
        "    # if model_is_ready_for_backtest or config.get('allow_backtest_load_model_directly', True): # Add flag to allow direct load\n",
        "    #     print(\"\\n--- Stage 3: Running Backtest ---\")\n",
        "    #     try:\n",
        "    #         # df_backtest_data=None uses self.df_features. load_latest_model=True reloads from disk.\n",
        "    #         # If model was just trained, set load_latest_model=False to use in-memory model.\n",
        "    #         backtest_results_df, trades_log_df = bot_orchestrator.run_backtest(df_backtest_data=None, load_latest_model=not model_trained_successfully)\n",
        "    #         if backtest_results_df is not None and not backtest_results_df.empty:\n",
        "    #             print(\"Backtest finished.\")\n",
        "    #             if trades_log_df is not None and not trades_log_df.empty:\n",
        "    #                 print(\"\\nBacktest Trades Log Sample:\")\n",
        "    #                 with pd.option_context('display.max_rows', 10, 'display.max_columns', None, 'display.width', 1000):\n",
        "    #                     print(trades_log_df.head())\n",
        "    #             elif trades_log_df is not None:\n",
        "    #                 print(\"\\nNo trades executed during backtest.\")\n",
        "    #             backtest_run_successfully = True\n",
        "    #         else:\n",
        "    #             print(\"Backtest failed to produce results. Check logs.\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error during backtest: {e}\")\n",
        "    #         traceback.print_exc()\n",
        "    # else:\n",
        "    #     print(\"Skipping backtest (model not trained or loaded).\")\n",
        "\n",
        "    # --- Stage 4: Run Live Simulation (Optional) ---\n",
        "    # model_is_ready_for_simulation = (bot_orchestrator.trained_model_booster is not None or\n",
        "    #                                  bot_orchestrator.trained_ensemble_models is not None or\n",
        "    #                                  (bot_orchestrator.model_predictor and bot_orchestrator.model_predictor.model_object is not None))\n",
        "    # if model_is_ready_for_simulation or config.get('allow_simulation_load_model_directly', True):\n",
        "    #     if config.get('run_simulation_flag', False): # Check the master flag\n",
        "    #         print(\"\\n--- Stage 4: Running Live Simulation ---\")\n",
        "    #         try:\n",
        "    #             if bot_orchestrator.live_simulator:\n",
        "    #                 bot_orchestrator.run_live_simulation()\n",
        "    #             else:\n",
        "    #                 print(\"Live simulator component not available. Skipping simulation.\")\n",
        "    #         except Exception as e:\n",
        "    #             print(f\"Error during live simulation: {e}\")\n",
        "    #             traceback.print_exc()\n",
        "    #             if bot_orchestrator.live_simulator and bot_orchestrator.live_simulator.simulation_running:\n",
        "    #                 bot_orchestrator.live_simulator.stop_live_simulation()\n",
        "    #     else:\n",
        "    #         print(\"\\nSkipping Live Simulation (run_simulation_flag is False in config).\")\n",
        "    # else:\n",
        "    #     print(\"Skipping live simulation (model not trained or loaded).\")\n",
        "\n",
        "    # --- Stage 5: Visualize Other Results (Feature Importance, EMD, etc.) ---\n",
        "    # print(\"\\n--- Stage 5: Visualizing Other Results ---\")\n",
        "    # try:\n",
        "    #     if bot_orchestrator.visualizer:\n",
        "    #         bot_orchestrator.visualize_results()\n",
        "    #     else:\n",
        "    #         print(\"Visualizer not available, skipping additional visualizations.\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error during visualization: {e}\")\n",
        "    #     traceback.print_exc()\n",
        "\n",
        "    print(\"\\n--- Orchestrator Workflow Attempt Finished ---\")\n",
        "else:\n",
        "    print(\"\\nSkipping Main Execution Workflow: Orchestrator not initialized, exchange failed, or 'allow_no_exchange_init' is False with no exchange.\")\n",
        "    print(\"Please ensure Cell 1 (Setup), Cell 2 (Config), Cell 4 (Imports), and Cell 5 (Orchestrator Init) have run successfully.\")\n",
        "\n",
        "print(\"\\n=\"*50)\n",
        "print(\"Notebook Execution Finished\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Optional: Keep plots open if not in interactive mode (e.g., running as script)\n",
        "# This part is from your original notebook.\n",
        "# Ensure 'plt' and 'HAS_MATPLOTLIB' are accessible here (defined in Cell 4)\n",
        "# if 'plt' in globals() and plt is not None and \\\n",
        "#    'HAS_MATPLOTLIB' in globals() and HAS_MATPLOTLIB and \\\n",
        "#    'sys' in globals() : # Check if sys was imported\n",
        "#      INTERACTIVE_MODE = 'ipykernel' in sys.modules\n",
        "#      if not INTERACTIVE_MODE:\n",
        "#          print(\"Displaying plots. Close plot windows to exit if any were generated and `show_plots` is True in config.\")\n",
        "#          plt.show() # This will show all figures generated if plt.show() wasn't called in Visualizer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}